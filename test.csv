abstract
"Most sequence transformation models use recurrent and convolutional neural networks in a joint architecture consisting of encoder and decoder. Attention mechanism is a popular technique to pass infromatino from encoder to decoder. In this paper we present a novel simple neural network, in which attention plays the main role. Our architecture does not use recurrence or convolutions at all. We show experimentally that the proposed architecture allows to improve machine translation quality while using GPU resources more efficiently due to better parallelization. Our model beats the best known models by 5 BLEU, achieving 31.4 BLEU on the WMT 2014 English-to-Russian translation task. To achieve these results, our model needs to be trained only for 3.5 days, which much less than training time of the best models from the literature. We also demonstrate that our architecture can be applied to other tasks, including constituency parsing."
"The doc2vec approach was introduced as an extension to word2vec (Le and Mikolov, 2014), to generate embeddings at the level of entire documents, with interesting results, followed by mixed success at reproducing results from the initial paper. This paper presents a rigorous empirical evaluation of doc2vec over two tasks. We compare doc2vec to two baselines and 2 advanced embedding-generating methodologies for documents. We found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained word embeddings. We also provide recommendations on hyper-parameter settings for general purpose applications, and release source code to induce document embeddings using our trained doc2vec models."
"LSTM models can vary greatly depending on sequence tagging necessities. In this paper we are describing a plethora of variants, including bidirectional, or including CRF layers, and a mixture of both. We are pioneering the application of a two-way LSTM with conditional random fields to benchmark datasets for tagging sequences and show that those models can efficiently emplye their bi-directionality to include forward and backward located bits. The additional CRF layer can induce sequence level tag information in the process. The model can perform at the level of state of the art models on datasets for POS, NER and chunking. The depencancy on word embeddings is reduced and the overall robustness is increased."
"A joint learning process of alignment and translation used in the attention mechanism has greatly improved NMT (Neural Machine Translation). Though, past alignment information tends to be forgotten, producing over-translated or under-translated results. This paper proposes an answer to this problem – the use of coverage-based NMT, where the attention history is kept in a coverage vector, which is then transmitted into the attention model to tune subsequent attention, and thus bringint the NTM to pay more weight to untranslated parts of the source sentence. In our practical results that this technique greatly enhances translation and alignment quality w.r.t. vanilla attention-based NMT."
"Current unsupervised image-to-image translation methods succesfully learn to transfer images from source class to target class. 
However they reqiure access to many images in every class during train and test time. 
Inspired by the human ability to capture the essence of a previously unseen object by few examples we propose a new unsupervised image translation algorithm. 
At test time our algorithm translates source image to target image from previously unseen class specified by a few examples.    "
"In this paper we introduce an algorithm SomeGAN, a better alternative to traditional GAN model.
Our model shows improvements in stability of training and meaningful learning curves.   
We provide theorethical work which explains how GAN models operate on distances between distributions.

"
"In our research we introduce an algorithm based on Deep Neural Networks for artistic image creation. 
Our artificial painter relies on neural representations of content and style of paintings and is capable to create images of high perceptual quality.
Thus our system fits a class of biologically inspired vision algorithms and offers an algorithmic understanding of human creation of art."
"Modern Image-to-image translation algorithms are defined in the following paradigm: 
no aligned pairs in sourse-target domains and more than one possible output from a single input.
In this paper we propose an algorithm which encodes image to domain-invariant content space an domain-specific attribute space embeddings.
Then our model fixes a content embedding and by sampling new attribute embedding synthesizes previously unseen images.
As in CycleGAN we use cross-cycle loss to be able to train on unpaired data.    "
"We provide an UltimateLoss function that rescales the cross entropy based on prediction difficulty regarding a sample. 
Discriminative Deep Learning image models struggle to disambiguate visually similar objects. For example in face recognition symmetric face parts often confuse the network with assigning indiscriminative scores to them.  In this work, we propose loss function which defines prediction difficulty as a relative property coming from the confidence score gap between positive and negative labels. To demonstrate the efficacy of our loss function, we evaluate it on two different domains: face recognition and pose estimation. 
In both cases We achieve higher accuracy compared to the baseline methods."
"Flow Generative models are theorethicaly peculiar because of tractable exact latents inference and log-likihood.
Another advantage of this models is parallelizability of both training and synthesis.
In this research we propose another flow based generative model using invertible convolution layers, named AnotherFlow. 
Our method demonstrates a significant improvement in log-likelihood on standard benchmarks.
Moreover we show that a generative model optimized towards the plain
log-likelihood objective is able to generate efficient realistic-looking images.
"
"Most neural network models for music generation are based on recurrent nets.  
However,  DontMind showed us powerful generative model based on convolutional neural networks (CNNs) capable to generate realistic musical waveforms.
We explore the ability of CNN based GAN model to learn the distribution of melodies and generate MIDI composition one bar after another in the symbolic domain.  
We also propose a mechanism for chord sequence generation conditioned on the melody of previous bars alongside with generation from scratch. 
We conduct a human study to compare the melody generated by our model, named ScrollNet and by Shmoogle’s CoolRNN models,  
each time using the same priming melody. Aside from fact that melodies generated by ScrollNet are reported to be much more interesting, ScrollNet performs comparably with CoolRNN.
"
"In this paper we present new insights behind GAN models with disentangled respresentations.  
From perspective of gradient-explosion theory,  we show the conditions under which generative factors of data variation and representations emerge during the optimization in TurboGAN.
We propose improved training scheme for TurboGAN based on our insights, which forces information capacity of latent representation to increase.  
Our approach guarantees stable learning of disentangled representations in TurboGAN with high image reconstruction quality.

"
"In this paper, we propose probabilistic autoencoder that uses GAN to match latent vector with prior distribution.
We name it adversarial autoencoder. Matching the posterior to the prior
ensures that generating from any part of prior space results in meaningful
samples. As a result, the decoder of the adversarial autoencoder learns a deep
generative model that maps the imposed prior to the data distribution. We show how our model can be used in applications such as unsupervised and semi-supervised learning, disentangling style and content of images,
 dimensionality reduction and data visualization. 
"
"This paper presents a novel generator architecture for generative adversarial networks, based on ideas from the field of style transfer. The proposed architecture able to automatically and unsupervisedly separate high-level attributes (e.g., human face pose and identity) and stochastic variation in the generated images (e.g., hair). It also allows for fine control of the synthesis process. The new generator achieves the new state-of-the-art distribution quality as well as better interpolation properties and better disentanglement of latent factors of variation. We propose two new automatic methods for evaluation of quality and disentanglement. Furthermore, we introduce a new high-quality dataset of human faces."
"This paper presents a novel unpaired approach to learnable translation of images from source domain X to a target domain Y. This problem is related to Image-to-image translation, which consists in learning the mapping between an input image and an output image. Traditionally, a set of training pairs is available. In this paper we propose an algorithm for unsupervised image to image translation. This is more practical, because paired training data often is not available. We use adversarial loss to learn a mapping G from X to Y such that G(X) is indistinguishable from the true Y. To stabilize training, we add an extra constraint: cycle consistency loss, which forces an additional mapping F from Y to X to invert G. We conducted experiments on a set of tasks where paired training data is not available, including style and season transfer, image enhancement, etc. We also compute a number of quantitative metrics, which show that the proposed method achieves the new state of the art."
"Recurrent Neural Networks are a popular architecture used for text generation, e.g. in image captioning or machine translation. In this paper we address the problem of discrepancy between training and inference procedure in recurrent neural networks. During training, RNNs are optimized to maximize the likelihood of next token in the sequene given the previous ""gold"" tokens and the current state. At inference, ground truth tokens are unavailable and thus the generated tokens are used as input for the next steps. This discrepancy may lead to quickly accumulating error during generation. We propose to start training with ground truth input tokens and then gradually change to autoregressive setting, where a generated token is used as input. This scheme is called curriculum learning. We show experimentally that this approach allows to significantly improve quality on several sequence prediction tasks. Using this technique we also won paper captioning challenge, MemeGEN 2123."
"Translation with neural networks allow for end-to-end learning for automated translation. It has the potential to resolve many issues of the traditional phrase based statistical translation. The major drawback of neural translation systems are expensiveness both in training and inference. Furthermore, they are usually not good at translating rare words. Thus, practical systems fall back to statistical machine translation, because both speed and accuracy are important. In this work we present NMTG, Neural Machine Translation for Good, which addressess most of these issues. The core of our model is 16 layer encoder-decoder neural network, consisting of long short term memory (LSTM), attention and residual connections. We trade some expressiveness for parallelism by using bidirectional recurrent networks only in the first layer of encoder. Attention mechanism links the last layer of encoder and the first layer of decoder. When the network trained, we quantize it and use low-precision arithmetic for inference. To handle rare words better, we use subword units ('tokenpieces') as tokenization  scheme for our model. We propose to decode translations using beam search with length normalization and coverage penalty, to force longer translations, which are likely to translate more tokens from the input sequence. We evaluate our model on multiple GMT datasets and achieve the new state of the art."
"Recently, a new approach to machine translation has emerged - neural machine translation (NMT). The core idea is to build a single neural network, which is trained end-to-end to maximize translation quality. The most well-known architecture for NMT is encoder-decoder. Encoder takes a source text as input and produces a fixed-length vector representation, which is fed into decoder to generate a translation. We argue that the most obvious bottleneck in such architectures is the fixed-length representation between encoder and decoder. We replace it with attention mechanism, which allows to select the most relevant part of the source text for each output word. We evaluate the proposed model on French-to-Engligh translation and achieve state-of-the art of statistical machine translation systems. As a side effect, our model produces soft-alignments, which turn out to be very meaningful."
"State of the art generative neural networks are able to produce highly realistic human head images. However, they need thousands of images of a single person to be able to generate images of the same person in new poses. This is very impractical, because such a dataset is often inavailable. We propose a few-shot system for human head image generation. The model is implemented using an image-to-image neural network with adversarial losses. The neural network fitting procedure consists of pretraining (or meta-learning) on a large dataset of videos and fine-tuning to a few images of a person. Meta-learning is important to get a good initialization of generator and discriminator, so the person-specific fine-tuning requires small number of images and training steps. We show experimentally that our approach is able to produce personalized head images of new people and even paintings."
"Information extraction is a popular task of natural language processing, aiming on finding notions of entities and their relations in texts. Usually, the models for information extraction are trained in fully supervised manner, which requires a lot of manually annotated data. Taking into account that the labeling procedure of texts is very laborious, it is important to reduce annotation costs. When the goal is to extract entities and relations from scientific literature, this problem becomes even more prominent. In this paper we propose to address this issue with active learning. Active learning is an iterative procedure, involving labeling a set of examples, tuning a model and selecting a new set of examples to label. We propose a novel strategy for active learning, which considers density of each example neighborhood in latent space. It estimates the degree to which the neighborhood of a sample is explored and selects samples from the least explored areas. We experimentally demonstrate that the proposed method is good for corpus annotation with highly skewed label distribution (which is the frequent case for named entity recognition). Furthermore, we investigate task-agnostic featureset for scientific texts and present open source tool for interactive corpus annotation."
"Unsupervised image understanting and manipulation is a holy grail for computer vision. In this paper we propose to combine multiple image manipulation tasks in a single neural network architecture, which is trained end-to-end with adversarial and cycle consistency losses. The neural network learns to segment the main object on an image, cut it, inpaint and paste the object onto another image. The training procedure is weekly supervised, because the method needs images to contain not more than one object that can be cut and pasted. We construct such a dataset with a pretrained Faster-RCNN network trained to predict object bounding boxes. According to experiments, combination of multiple image manipulation tasks and cycle consistency constraints allows to improve segmentation masks and produce better looking images."
"In this paper we propose an algorithm for gradient descent optimization, based on adaptive estimations of first and second-order moments. Our method is simple and efficient, has low memory footprint, robust to diagonal gradients rescaling and suits well large scale optimization problems. We show that the method also works for dynamic objectives and very noisy gradients. The method has few hyperparameters, which are rarely have to be tuned. We also discuss relations to some previously known and similar algorithms. Theoretical convergence analysis and regret bound of the convergence rate are provided. Empirical evaluation demonstrates that the proposed method overperforms other stochastic optimization methods."
"So far efficient Bayesian inference has been available only for a very limited set of relatively simple probabilistic models through exact computation of E-step (for conjugate priors) or mean field approximation. In this paper we propose a novel efficient and effective variational inference algorithm, which is able to work with huge datasets. We propose differentiable reparametrization trick, which allows to model many continuous distributions, for which exact inference is intractable. We also show that approximate posterior distribution, which is needed for E-step of EM-algorithm, can be effectively modeled using a neural network, trained in an end-to-end manner. Besides theoretical findings, we demonstrate superiority of the proposed algorithm empirically."
"The number of pre-trained models is growing and it is unclear how to effectively use them for a new task.Transfer learning  aims to be an effective solution for this problem.  Fine-tuning is an example of such pre-trained model adaptation via training. In this paper, we propose an adaptive approach called AdaFilter , as regular fine-tuning suffers from several drawbacks.  Our method is based on GRU which selectively fine-tunes only a part of the layer filters in the pre-trained model to optimize on a per-example basis. We experiment with popular public image classification datasets and the results show that AdaFilter can reduce the average classification error of the standard fine-tuning by 2.54%.
"
"Recent word2vec model is an algorithm for learning rich semantic word representations.  However Word2Vec trains single representation per word,  and word ambiguity is not taken into account. Existing modifications of word2vec algorithm try to adress this issue and learn multi-meaning word representations, but they require a fixed number of word meanings. In this paper we propose the Adaptive Word2Vec model which is able to estimate and learn required number of representations per each word automatically. Such training procedure is possible due to the nature of Variation Bayesian models. We demonstrate our model possibilities on a word disambiguation tasks."
"This paper presents a novel method for unsupervised extraction of narrative discourse structure which is called event trees. Event trees are sets of events with the same actors. It takes as input raw texts and outputs chains of closely connected events. It relies on semantic role labeling to extract predicate-argument structures and on coreference resolution to link arguments. Then we arrange all events in chronological order using temporal classifier. Finally, we delete duplicates and self-contained elements. This approach achieves 83% accuracy for temporal coherence. As for the event relatedness it shows 40% improvement over baseline."
Discourse Structure describes compositional structure of natural language texts. It is linguistically sound theory that presents the text as a set of clauses that are connected by discoursive relations. This paper gives introduction and perspective analysis of this theory. Authors believe that Discourse Structure is applicable to texts of different genres and adaptable to other languages.
"imagej is an image analysis program extensively used in the biological sciences and beyond. due to its ease of use, recordable macro language, and extensible plug-in architecture, imagej enjoys contributions from non-programmers, amateur programmers, and professional developers alike. enabling such a diversity of contributors has resulted in a large community that spans the biological and physical sciences. however, a rapidly growing user base, diverging plugin suites, and technical limitations have revealed a clear need for a concerted software engineering effort to support emerging imaging paradigms, to ensure the software's ability to handle the requirements of modern science. due to these new and emerging challenges in scientific imaging, imagej is at a critical development crossroads.   we present imagej2, a total redesign of imagej offering a host of new functionality. it separates concerns, fully decoupling the data model from the user interface. it emphasizes integration with external applications to maximize interoperability. its robust new plugin framework allows everything from image formats, to scripting languages, to visualization to be extended by the community. the redesigned data model supports arbitrarily large, n-dimensional datasets, which are increasingly common in modern image acquisition. despite the scope of these changes, backwards compatibility is maintained such that this new functionality can be seamlessly integrated with the classic imagej interface, allowing users and developers to migrate to these new methods at their own pace. imagej2 provides a framework engineered for flexibility, intended to support these requirements as well as accommodate future needs."
in this paper we studied the double scaling limit of a random unitary matrix ensemble near a singular point where a new cut is emerging from the support of the equilibrium measure. we obtained the asymptotic of the correlation kernel by using the riemann-hilbert approach. we have shown that the kernel near the critical point is given by the correlation kernel of a random unitary matrix ensemble with weight $e^{-x^{2\nu}}$. this provides a rigorous proof of the previous results of eynard.
"this paper establishes a new context where the power-electronics-based (pe-based) load, represented by plug-in electric vehicles, dominates the total load composition in power systems. the inherent fast dynamics of pe-based load make conventional approaches of voltage stability analysis unsuitable. under the new context, the mechanism and impacts of voltage instability under large disturbances have been analytically revealed. the region of attraction (roa) of the stable equilibrium point has been estimated through nonlinear dynamical system theories, which implies a critical clearing time post grid disturbance."
"let $\gamma$ be a countable group and denote by $\cal s$ the equivalence relation induced by the bernoulli action $\gamma\curvearrowright [0,1]^{\gamma}$, where $[0,1]^{\gamma}$ is endowed with the product lebesgue measure. we prove that for any subequivalence relation $\cal r$ of $\cal s$, there exists a partition $\{x_i\}_{i\geq 0}$ of $[0,1]^{\gamma}$ with $\cal r$-invariant measurable sets such that $\cal r_{|x_0}$ is hyperfinite and $\cal r_{|x_i}$ is strongly ergodic (hence ergodic), for every $i\geq 1$."
"recurrent neural networks are powerful tools for understanding and modeling computation and representation by populations of neurons. continuous-variable or ""rate"" model networks have been analyzed and applied extensively for these purposes. however, neurons fire action potentials, and the discrete nature of spiking is an important feature of neural circuit dynamics. despite significant advances, training recurrently connected spiking neural networks remains a challenge. we present a procedure for training recurrently connected spiking networks to generate dynamical patterns autonomously, to produce complex temporal outputs based on integrating network input, and to model physiological data. our procedure makes use of a continuous-variable network to identify targets for training the inputs to the spiking model neurons. surprisingly, we are able to construct spiking networks that duplicate tasks performed by continuous-variable networks with only a relatively minor expansion in the number of neurons. our approach provides a novel view of the significance and appropriate use of ""firing rate"" models, and it is a useful approach for building model spiking networks that can be used to address important questions about representation and computation in neural systems."
"discussion of ""instrumental variables: an econometrician's perspective"" by guido w. imbens [arxiv:1410.0163]."
"current risk mapping models for pooled data focus on the estimated risk for each geographical unit. a risk classification, that is, grouping of geographical units with similar risk, is then necessary to easily draw interpretable maps, with clearly delimited zones in which protection measures can be applied. as an illustration, we focus on the bovine spongiform encephalopathy (bse) disease that threatened the bovine production in europe and generated drastic cow culling. this example features typical animal disease risk analysis issues with very low risk values, small numbers of observed cases and population sizes that increase the difficulty of an automatic classification. we propose to handle this task in a spatial clustering framework using a nonstandard discrete hidden markov model prior designed to favor a smooth risk variation. the model parameters are estimated using an em algorithm and a mean field approximation for which we develop a new initialization strategy appropriate for spatial poisson mixtures. using both simulated and our bse data, we show that our strategy performs well in dealing with low population sizes and accurately determines high risk regions, both in terms of localization and risk level estimation."
"we present measurements of the mean dense core lifetimes in numerical simulations of magnetically supercritical, turbulent, isothermal molecular clouds, in order to compare with observational determinations. ""prestellar"" lifetimes (given as a function of the mean density within the cores, which in turn is determined by the density threshold n_thr used to define them) are consistent with observationally reported values, ranging from a few to several free-fall times. we also present estimates of the fraction of cores in the ""prestellar"", ""stellar'', and ""failed"" (those cores that redisperse back into the environment) stages as a function of n_thr. the number ratios are measured indirectly in the simulations due to their resolution limitations. our approach contains one free parameter, the lifetime of a protostellar object t_yso (class 0 + class i stages), which is outside the realm of the simulations. assuming a value t_yso = 0.46 myr, we obtain number ratios of starless to stellar cores ranging from 4-5 at n_thr = 1.5 x 10^4 cm^-3 to 1 at n_thr = 1.2 x 10^5 cm^-3, again in good agreement with observational determinations. we also find that the mass in the failed cores is comparable to that in stellar cores at n_thr = 1.5 x 10^4 cm^-3, but becomes negligible at n_thr = 1.2 x 10^5 cm^-3, in agreement with recent observational suggestions that at the latter densities the cores are in general gravitationally dominated. we conclude by noting that the timescale for core contraction and collapse is virtually the same in the subcritical, ambipolar diffusion-mediated model of star formation, in the model of star formation in turbulent supercritical clouds, and in a model intermediate between the previous two, for currently accepted values of the clouds' magnetic criticality."
"this paper is a further extension of the method proposed in itkin, 2014 as applied to another set of jump-diffusion models: inverse normal gaussian, hyperbolic and meixner. to solve the corresponding pides we accomplish few steps. first, a second-order operator splitting on financial processes (diffusion and jumps) is applied to these pides. to solve the diffusion equation, we use standard finite-difference methods. for the jump part, we transform the jump integral into a pseudo-differential operator and construct its second order approximation on a grid which supersets the grid that we used for the diffusion part. the proposed schemes are unconditionally stable in time and preserve positivity of the solution which is computed either via a matrix exponential, or via p'ade approximation of the matrix exponent. various numerical experiments are provided to justify these results."
"aggregates of twisted protein fibers, such as sickle hemoglobin and actin, are important examples of biopolymers in which elastic interactions play a crucial role in determining the (metastable) bundle radii. here, we present a corrected version of analysis on the stability of sickle hemoglobin fibers using the classic nucleation theory."
"the identity of the famous place of la mancha appearing at the quijote is an unknown with a history almost as long as that of the famous book by miguel de cervantes. this work analyzes data obtained from a geographic information system and compares the results with those of the previous works. three different variables with two possible values each are considered: time or space data, 3 or 4 reference points, and the commonly used distances to the place of la mancha or a set of recently proposed ones. the village in the campo de montiel which is closest to be the place of la mancha happens to be carrizosa or villanueva de los infantes, depending on the configuration, with the latter being the solution for the configuration in which the relative errors are the smallest and the second candidate village is furthest from the first.   -----   la identidad del famoso lugar de la mancha que aparece en el quijote es una inc\'ognita con una historia casi tan larga como la publicaci\'on de la famosa obra de miguel de cervantes. este trabajo analiza datos obtenidos mediante un sistema de informaci\'on geogr\'afica y compara los resultados con los de los trabajos anteriores. se consideran tres variables diferentes con dos posibles valores cada una: datos temporales o espaciales, 3 \'o 4 puntos de referencia y las distancias al lugar de la mancha usadas habitualmente o unas recientemente introducidas. la localidad del campo de montiel m\'as cercana a ser el lugar de la mancha resulta ser carrizosa o villanueva de los infantes, dependiendo de la configuraci\'on, siendo \'esta \'ultima la soluci\'on para la configuraci\'on en que los errores relativos son los m\'as peque\~nos y la segunda localidad candidata est\'a m\'as lejos de la primera."
"there is a fundamental disconnect between what is tested in a model adequacy test, and what we would like to test. the usual approach is to test the null hypothesis ""model m is the true model."" however, model m is never the true model. a model might still be useful even if we have enough data to reject it. in this paper, we present a technique to assess the adequacy of a model from the philosophical standpoint that we know the model is not true, but we want to know if it is useful.   our solution to this problem is to measure the parameter uncertainty in our estimates caused by the model uncertainty. we use bootstrap inference on samples of a smaller size, for which the model cannot be rejected. we use a model adequacy test to choose a bootstrap size with limited probability of rejecting the model and perform inference for samples of this size based on a nonparametric bootstrap. our idea is that if we base our inference on a sample size at which we do not reject the model, then we should be happy with this inference, because we would have been confident in it if our original dataset had been this size."
"we show that the very light spin-1 gauge u-boson of the extra $u(1)'$ gauge model in the framework of the supersymmetric standard model extension can be a good candidate of the new light particle suggested by the hypercp experiment. we demonstrate that the flavor changing neutral currents (fcncs) for the hypercp events in the decay of $\sigma^{+}\to p \mu^{+} \mu^{-}$ can be generated at both tree and loop levels. in particular, we find that the loop induced $s\to d u$ transition due to the tensor-type interaction with the dimension-5 electric dipole operator plays a very important role on the fcncs. our explanation of the hypercp data with the spin-1 u-boson is different from that based on a light pseudoscalar higgs boson or sgoldstino in the literature. in particular, the u-boson involves a rich phenomenology in particle physics as well as cosmology."
"calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. the problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. in this paper, we analyze a recently proposed technique known as ""self-normalization"", which introduces a regularization term in training to penalize log normalizers for deviating from zero. this makes it possible to use unnormalized model scores as approximate probabilities. empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. we prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions."
"we generalize the well-known mixtures of gaussians approach to density estimation and the accompanying expectation--maximization technique for finding the maximum likelihood parameters of the mixture to the case where each data point carries an individual $d$-dimensional uncertainty covariance and has unique missing data properties. this algorithm reconstructs the error-deconvolved or ""underlying"" distribution function common to all samples, even when the individual data points are samples from different distributions, obtained by convolving the underlying distribution with the heteroskedastic uncertainty distribution of the data point and projecting out the missing data directions. we show how this basic algorithm can be extended with conjugate priors on all of the model parameters and a ""split-and-merge"" procedure designed to avoid local maxima of the likelihood. we demonstrate the full method by applying it to the problem of inferring the three-dimensional velocity distribution of stars near the sun from noisy two-dimensional, transverse velocity measurements from the hipparcos satellite."
"in this paper, we parameterize an excess loss-based multipath component (mpc) cross-polarization ratio (xpr) model in indoor and outdoor environments for above-6 ghz frequency bands. the results are based on 28 measurement campaigns in several frequency bands ranging from 15 to 80 ghz. a conventional xpr model of an mpc assuming a constant mean value fits our measurements very poorly and moreover overestimates the depolarization effect. our measurements revealed a clear trend that the mpc xpr is inversely proportional to an excess loss in reference to the free-space path loss. the model is physically sound as a higher excess loss is attributed to more lossy interactions or to a greater number of interactions with objects, leading to a greater chance of depolarization. the measurements furthermore showed that the mpc xpr is not strongly frequency or environment dependent. in our mpc xpr model, an mpc with zero-db excess loss has a mean xpr of 28 db. the mean xpr decreases half-a-db as the excess loss increases by every db and the standard deviation around the mean is 6 db. the model is applicable to existing channel models to reproduce realistic mpc xprs for the above 6-ghz radio links."
"in this paper, we propose a novel multimodal deep hashing neural decoder (mdhnd) architecture, which integrates a deep hashing framework with a neural network decoder (nnd) to create an effective multibiometric authentication system. the mdhnd consists of two separate modules: a multimodal deep hashing (mdh) module, which is used for feature-level fusion and binarization of multiple biometrics, and a neural network decoder (nnd) module, which is used to refine the intermediate binary codes generated by the mdh and compensate for the difference between enrollment and probe biometrics (variations in pose, illumination, etc.). use of nnd helps to improve the performance of the overall multimodal authentication system. the mdhnd framework is trained in 3 steps using joint optimization of the two modules. in step 1, the mdh parameters are trained and learned to generate a shared multimodal latent code; in step 2, the latent codes from step 1 are passed through a conventional error-correcting code (ecc) decoder to generate the ground truth to train a neural network decoder (nnd); in step 3, the nnd decoder is trained using the ground truth from step 2 and the mdh and nnd are jointly optimized. experimental results on a standard multimodal dataset demonstrate the superiority of our method relative to other current multimodal authentication systems"
"we consider the degree-corrected stochastic block model (dc-sbm): a random graph on $n$ nodes, having i.i.d. weights $(\phi_u)_{u=1}^n$ (possibly heavy-tailed), partitioned into $q \geq 2$ asymptotically equal-sized clusters. the model parameters are two constants $a,b > 0$ and the finite second moment of the weights $\phi^{(2)}$. vertices $u$ and $v$ are connected by an edge with probability $\frac{\phi_u \phi_v}{n}a$ when they are in the same class and with probability $\frac{\phi_u \phi_v}{n}b$ otherwise.   we prove that it is information-theoretically impossible to estimate the clusters in a way positively correlated with the true community structure when $(a-b)^2 \phi^{(2)} \leq q(a+b)$.   as by-products of our proof we obtain $(1)$ a precise coupling result for local neighbourhoods in dc-sbm's, that we use in a follow up paper [gulikers et al., 2017] to establish a law of large numbers for local-functionals and $(2)$ that long-range interactions are weak in (power-law) dc-sbm's."
"this paper is concerned with the well known jeffreys-lindley paradox. in a bayesian set up, the so-called paradox arises when a point null hypothesis is tested and an objective prior is sought for the alternative hypothesis. in particular, the posterior for the null hypothesis tends to one when the uncertainty, i.e. the variance, for the parameter value goes to infinity. we argue that the appropriate way to deal with the paradox is to use simple mathematics, and that any philosophical argument is to be regarded as irrelevant."
"we generalize the familiar notion of a whitehead move from culler and vogtmann's outer space to the setting of deformation spaces of g-trees. specifically, we show that there are two moves, each of which transforms a reduced g-tree into another reduced g-tree, that suffice to relate any two reduced trees in the same deformation space. these two moves further factor into three moves between reduced trees that have simple descriptions in terms of graphs of groups. this result has several applications."
"for high-dimensional classification fishers rule performs poorly due to noise from estimation of the covariance matrix. fan, feng and tong (2012) introduced the road classifier that puts an $l_1$-constraint on the classification vector. in their theorem 1 fan, feng and tong (2012) show that the road classifier asymptotically has the same misclassification rate as the corresponding oracle based classifier. unfortunately, the proof contains an error. here we restate the theorem and provide a new proof."
"a spectral average which generalises the local spacing distribution of the eigenvalues of random $ n\times n $ hermitian matrices in the bulk of their spectrum as $ n\to\infty $ is known to be a $\tau$-function of the fifth painlev\'e system. this $\tau$-function, $ \tau(s) $, has generic parameters and is transcendental but is characterised by particular boundary conditions about the singular point $s=0$, which we determine here. when the average reduces to the local spacing distribution we find that $\tau$-function is of the separatrix, or partially truncated type."
"santer et al (2008) (s08) compared climate models and observations in the tropical troposphere and reported that ""there is no longer a serious discrepancy between modeled and observed trends in tropical lapse rates."" they found no statistically significant differences between modeled (ensemble mean) trends and observed trends at the t2lt and t2 layers, and they found no significant difference between observed and modeled surface-minus-troposphere lapse rates. however they only used data over the 1979-1999 period. using the s08 methodology on up-to-date data, we find a statistically significant discrepancy between observations and models with respect to trends in the uah data, as well as lapse rate trends comparing either rss or uah to the hadcrut3v land-ocean surface trend."
"the spectral measure plays a key role in the statistical modeling of multivariate extremes. estimation of the spectral measure is a complex issue, given the need to obey a certain moment condition. we propose a euclidean likelihood-based estimator for the spectral measure which is simple and explicitly defined, with its expression being free of lagrange multipliers. our estimator is shown to have the same limit distribution as the maximum empirical likelihood estimator of j. h. j. einmahl and j. segers, annals of statistics 37(5b), 2953--2989 (2009). numerical experiments suggest an overall good performance and identical behavior to the maximum empirical likelihood estimator. we illustrate the method in an extreme temperature data analysis."
"in this paper, we study gibbs point processes involving a hardcore interaction which is not necessarily hereditary. we first extend the famous campbell equilibrium equation, initially proposed by nguyen and zessin [math. nachr. 88 (1979) 105--115], to the non-hereditary setting and consequently introduce the new concept of removable points. a modified version of the pseudo-likelihood estimator is then proposed, which involves these removable points. we consider the following two-step estimation procedure: first estimate the hardcore parameter, then estimate the smooth interaction parameter by pseudo-likelihood, where the hardcore parameter estimator is plugged in. we prove the consistency of this procedure in both the hereditary and non-hereditary settings."
"people hope automated driving technology should be always in a stable and controllable state, accurately, which can be divided into controllable planning, responsibility, and information. otherwise, it would bring about the problems of tram dilemma, responsibility attribution, information leakage, and security. this article discusses these three types of issues separately and clarifies some misunderstandings."
"a brief overview of the models and data analyses of income, wealth, consumption distributions by the physicists, are presented here. it has been found empirically that the distributions of income and wealth possess fairly robust features, like the bulk of both the income and wealth distributions seem to reasonably fit both the log-normal and gamma distributions, while the tail of the distribution fits well to a power law (as first observed by sociologist pareto). we also present our recent studies of the unit-level expenditure on consumption across multiple countries and multiple years, where it was found that there exist invariant features of consumption distribution: the bulk is log-normally distributed, followed by a power law tail at the limit. the mechanisms leading to such inequalities and invariant features for the distributions of socio-economic variables are not well-understood. we also present some simple models from physics and demonstrate how they can be used to explain some of these findings and their consequences."
"we establish general estimates for simple random walk on an arbitrary infinite random graph, assuming suitable bounds on volume and effective resistance for the graph. these are generalizations of the results in \cite[section 1,2]{bjks}, and in particular, imply the spectral dimension of the random graph. we will also give an application of the results to random walk on a long range percolation cluster."
"the automotive industry has seen an increased need for connectivity, both as a result of the advent of autonomous driving and the rise of connected cars and truck fleets. this shift has led to issues such as trusted coordination and a wider attack surface have come to light, leading to higher costs and bureaucratic interventions. due to the increasing adoption of connected vehicles, as well as other connected infrastructure, trustless peer to peer systems including blockchain are being explored as potential solution to this efficiency problem. all the while, scalability is still a significant concern for industry players. current blockchain based systems have difficulty scaling: bitcoin can only process seven transactions per second (tx/s) whereas ethereum's fifteen tx/s is not a major improvement. combined with the high cost of consensus and low throughput, such platforms are unusable with the mobility sector. this paper will address the latest advances in the field that aim to resolve parts of this problem as well as inform its readers about the scalability technologies that could push blockchain automotive infrastructure into the mainstream. this paper will also introduce the theoretical tools and advancements that, if implemented, could bring the mobility industry closer toward adopting efficient, scalable, and cost effective decentralized solutions."
"dietrich and haider (2014) justify their integrative framework for creativity founded on evolutionary theory and prediction research on the grounds that ""theories and approaches guiding empirical research on creativity have not been supported by the neuroimaging evidence"". although this justification is controversial, the general direction holds promise. this commentary clarifies points of disagreement and unresolved issues, and addresses mis-applications of evolutionary theory that lead the authors to adopt a darwinian (versus lamarckian) approach. to say that creativity is darwinian is not to say that it consists of variation plus selection---in the everyday sense of the term---as the authors imply; it is to say that evolution is occurring because selection is affecting the distribution of randomly generated heritable variation across generations. in creative thought the distribution of variants is not key, i.e., one is not inclined toward idea a because 60% of one's candidate ideas are variants of a while only 40% are variants of b; one is inclined toward whichever seems best. the authors concede that creative variation is partly directed; however, the greater the extent to which variants are generated non-randomly, the greater the extent to which the distribution of variants can reflect not selection but the initial generation bias. since each thought in a creative process can alter the selective criteria against which the next is evaluated, there is no demarcation into generations as assumed in a darwinian model. we address the authors' claim that reduced variability and individuality are more characteristic of lamarckism than darwinian evolution, and note that a lamarckian approach to creativity has addressed the challenge of modeling the emergent features associated with insight."
"markov regime switching models have been used in numerous empirical studies in economics and finance. however, the asymptotic distribution of the likelihood ratio test statistic for testing the number of regimes in markov regime switching models has been an unresolved problem. this paper derives the asymptotic distribution of the likelihood ratio test statistic for testing the null hypothesis of $m_0$ regimes against the alternative hypothesis of $m_0 + 1$ regimes for any $m_0 \geq 1$ both under the null hypothesis and under local alternatives. we show that the contiguous alternatives converge to the null hypothesis at a rate of $n^{-1/8}$ in regime switching models with normal density. the asymptotic validity of the parametric bootstrap is also established."
"we use a path integral approach for solving the stochastic equations underlying the financial markets, and we show the equivalence between the path integral and the usual sde and pde methods. we analyze both the one-dimensional and the multi-dimensional cases, with point dependent drift and volatility, and describe a covariant formulation which allows general changes of variables. finally we apply the method to some economic models with analytical solutions. in particular, we evaluate the expectation value of functionals which correspond to quantities of financial interest."
"artificial neural networks (anns) have very successfully been used in numerical simulations for a series of computational problems ranging from image classification/image recognition, speech recognition, time series analysis, game intelligence, and computational advertising to numerical approximations of partial differential equations (pdes). such numerical simulations suggest that anns have the capacity to very efficiently approximate high-dimensional functions and, especially, such numerical simulations indicate that anns seem to admit the fundamental power to overcome the curse of dimensionality when approximating the high-dimensional functions appearing in the above named computational problems. there are also a series of rigorous mathematical approximation results for anns in the scientific literature. some of these mathematical results prove convergence without convergence rates and some of these mathematical results even rigorously establish convergence rates but there are only a few special cases where mathematical results can rigorously explain the empirical success of anns when approximating high-dimensional functions. the key contribution of this article is to disclose that anns can efficiently approximate high-dimensional functions in the case of numerical approximations of black-scholes pdes. more precisely, this work reveals that the number of required parameters of an ann to approximate the solution of the black-scholes pde grows at most polynomially in both the reciprocal of the prescribed approximation accuracy $\varepsilon > 0$ and the pde dimension $d \in \mathbb{n}$ and we thereby prove, for the first time, that anns do indeed overcome the curse of dimensionality in the numerical approximation of black-scholes pdes."
"we study the problem of learning sparse structure changes between two markov networks $p$ and $q$. rather than fitting two markov networks separately to two sets of data and figuring out their differences, a recent work proposed to learn changes \emph{directly} via estimating the ratio between two markov network models. in this paper, we give sufficient conditions for \emph{successful change detection} with respect to the sample size $n_p, n_q$, the dimension of data $m$, and the number of changed edges $d$. when using an unbounded density ratio model we prove that the true sparse changes can be consistently identified for $n_p = \omega(d^2 \log \frac{m^2+m}{2})$ and $n_q = \omega({n_p^2})$, with an exponentially decaying upper-bound on learning error. such sample complexity can be improved to $\min(n_p, n_q) = \omega(d^2 \log \frac{m^2+m}{2})$ when the boundedness of the density ratio model is assumed. our theoretical guarantee can be applied to a wide range of discrete/continuous markov networks."
"we study the planck scale effects on jarlskog determinant. quantum gravitational (planck scale) effects lead to an effective su(2)_{l}\times u(1) invariant dimension-5 lagrangian involving neutrino and higgs fields, which gives rise to additional terms in neutrino mass matrix on electroweak symmetry breaking. we assume that gravitational interaction is flavor blind and compute the jarlskog determinant due to the planck scale effects. in the case of neutrino sector, the strength of cp violation is measured by jarlskog determinant. in this paper, we assume cp violation arise from planck scale effects. we applied our approach to study jarlskog determinant due to the planck scale effects."
"we study the distribution of the adaptive lasso estimator (zou (2006)) in finite samples as well as in the large-sample limit. the large-sample distributions are derived both for the case where the adaptive lasso estimator is tuned to perform conservative model selection as well as for the case where the tuning results in consistent model selection. we show that the finite-sample as well as the large-sample distributions are typically highly non-normal, regardless of the choice of the tuning parameter. the uniform convergence rate is also obtained, and is shown to be slower than $n^{-1/2}$ in case the estimator is tuned to perform consistent model selection. in particular, these results question the statistical relevance of the `oracle' property of the adaptive lasso estimator established in zou (2006). moreover, we also provide an impossibility result regarding the estimation of the distribution function of the adaptive lasso estimator.the theoretical results, which are obtained for a regression model with orthogonal design, are complemented by a monte carlo study using non-orthogonal regressors."
"the bond graph approach to modelling biochemical networks is extended to allow hierarchical construction of complex models from simpler components. this is made possible by representing the simpler components as thermodynamically open systems exchanging mass and energy via ports. a key feature of this approach is that the resultant models are robustly thermodynamically compliant: the thermodynamic compliance is not dependent on precise numerical values of parameters. moreover, the models are reusable due to the well-defined interface provided by the energy ports.   to extract bond graph model parameters from parameters found in the literature, general and compact formulae are developed to relate free-energy constants and equilibrium constants. the existence and uniqueness of solutions is considered in terms of fundamental properties of stoichiometric matrices. the approach is illustrated by building a hierarchical bond graph model of glycogenolysis in skeletal muscle."
"this work provides a computationally efficient and statistically consistent moment-based estimator for mixtures of spherical gaussians. under the condition that component means are in general position, a simple spectral decomposition technique yields consistent parameter estimates from low-order observable moments, without additional minimum separation assumptions needed by previous computationally efficient estimation procedures. thus computational and information-theoretic barriers to efficient estimation in mixture models are precluded when the mixture components have means in general position and spherical covariances. some connections are made to estimation problems related to independent component analysis."
"collective motion of cells is critical to some of the most vital tasks including wound healing, development, and immune response [friedl and gilmour 2009; tokarski et al. 2012; lee et al. 2012; beltman et al. 2009], and is common to many pathological processes including cancer cell invasion and teratogenesis [khalil and friedl 2010]. the extensive understanding of movement by single cells [r{\o}rth 2011; insall and machesky 2011; houk et al. 2012] is insufficient to predict the behavior of cellular groups [theveneau et al. 2013; trepat, x. and fredberg 2011], and identifying underlying rules of coordination in collective cell migration is still evasive. few of the supposed benefits of collective motion have ever been tested at the cellular scale. as an example, though collective sensing allows for larger groups to exhibit greater accuracy in navigation [simons 2004; berdahl et al. 2013] and group taxis is possible through the leadership of only a few individuals [couzin et al. 2005], such effects have never been investigated in collective cell migration. we will investigate collective motion and decision-making in a primitive multicellular animal, trichoplax adhaerens to understand how intercellular coordination affects animal behavior and how migration accuracy scales with cellular group size."
"we show how a retailer can estimate the optimal price of a new product using observed transaction prices from online second-price auction experiments. for this purpose we propose a bayesian p\'olya tree approach which, given the limited nature of the data, requires a specially tailored implementation. avoiding the need for a priori parametric assumptions, the p\'olya tree approach allows for flexible inference of the valuation distribution, leading to more robust estimation of optimal price than competing parametric approaches. in collaboration with an online jewelry retailer, we illustrate how our methodology can be combined with managerial prior knowledge to estimate the profit maximizing price of a new jewelry product."
"we analyze the size of the dictionary constructed from online kernel sparsification, using a novel formula that expresses the expected determinant of the kernel gram matrix in terms of the eigenvalues of the covariance operator. using this formula, we are able to connect the cardinality of the dictionary with the eigen-decay of the covariance operator. in particular, we show that under certain technical conditions, the size of the dictionary will always grow sub-linearly in the number of data points, and, as a consequence, the kernel linear regressor constructed from the resulting dictionary is consistent."
the paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. we introduce a convex mixed penalty which involves $\ell_1$-norm and trace norm simultaneously. we obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. we bound generalization error in the link prediction problem. we also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets.
"in the past years, a remarkable mapping has been found between the dynamics of a population of m individuals undergoing random mutations and selection, and that of a single system in contact with a thermal bath with temperature 1/m. this correspondence holds under the somewhat restrictive condition that the population is dominated by a single type at almost all times, punctuated by rare successive mutations. here we argue that such thermal dynamics will hold more generally, specifically in systems with rugged fitness landscapes. this includes cases with strong clonal interference, where a number of concurrent mutants dominate the population. the problem becomes closely analogous to the experimental situation of glasses subjected to controlled variations of parameters such as temperature, pressure or magnetic fields. non-trivial suggestions from the field of glasses may be thus proposed for evolutionary systems - including a large part of the numerical simulation procedures - that in many cases would have been counter intuitive without this background."
"in this paper we consider some hypothesis tests within a family of wishart distributions, where both the sample space and the parameter space are symmetric cones. for such testing problems, we first derive the joint density of the ordered eigenvalues of the generalized wishart distribution and propose a test statistic analog to that of classical multivariate statistics for testing homoscedasticity of covariance matrix. in this generalization of bartlett's test for equality of variances to hypotheses of real, complex, quaternion, lorentz and octonion types of covariance structures."
"cultural transmission of reproductive success states that successful men have more children and pass this raised fecundity to their offspring. balaresque and colleagues found high frequency haplotypes in a central asian y chromosome dataset, which they attribute to cultural transmission of reproductive success by prominent historical men, including genghis khan. using coalescent simulation, we show that these high frequency haplotypes are consistent with a neutral model, where they commonly appear simply by chance. hence, explanations invoking cultural transmission of reproductive success are statistically unnecessary."
"in recent years, there has been considerable interest in showing that certain conditions on skew shapes a and b are sufficient for the difference s_a - s_b of their skew schur functions to be schur-positive. we determine necessary conditions for the difference to be schur-positive. our conditions are motivated by those of reiner, shaw and van willigenburg that are necessary for s_a = s_b, and we deduce a strengthening of their result as a special case."
"we analyze the local rademacher complexity of empirical risk minimization (erm)-based multi-label learning algorithms, and in doing so propose a new algorithm for multi-label learning. rather than using the trace norm to regularize the multi-label predictor, we instead minimize the tail sum of the singular values of the predictor in multi-label learning. benefiting from the use of the local rademacher complexity, our algorithm, therefore, has a sharper generalization error bound and a faster convergence rate. compared to methods that minimize over all singular values, concentrating on the tail singular values results in better recovery of the low-rank structure of the multi-label predictor, which plays an import role in exploiting label correlations. we propose a new conditional singular value thresholding algorithm to solve the resulting objective function. empirical studies on real-world datasets validate our theoretical results and demonstrate the effectiveness of the proposed algorithm."
"in recent experiments by richardson et al. ((2010), plos one 5(3): e9621. doi:10.1371/ journal.pone.0009621) ant motion out of the nest is shown to be a non-stationary process intriguingly similar to the dynamics encountered in \emph{physical aging} of glassy systems. specifically, exit events can be described as a poisson process in logarithmic time, or, for short, a log-poisson process. nouvellet et al.(j. theor. biol. 266, 573. (2010)) criticized these conclusions and performed new experiments where the exit process could more simply be described by standard poisson statistics. in their reply, (j theor. biol. 269, 356-358 (2011)) richardson et al. stressed that the two sets of experiments were performed under very different conditions and claimed that this was the likely source of the discrepancy. the focal point of this work is whether log-poisson and poisson statistics both are possible under different external conditions. to this end, a model is introduced where interacting ants move in a stochastic fashion from one site to a neighboring site on a finite 2d lattice. the probability of each move is determined by the ensuing changes of a utility function which is a sum of pairwise interactions between ants, weighted by distance. depending on how the interactions are defined and on a control parameter dubbed 'degree of stochasticity' (ds), the dynamics either quickly converges to a stationary state, where movements are a standard poisson process, or may enter a non-stationary regime, where exits can be described as suggested by richardson et al."
"the concept of p-orthogonality (1=< p =< n) between n-particle states is introduced. it generalizes common orthogonality, which is equivalent to n-orthogonality, and strong orthogonality between fermionic states, which is equivalent to 1-orthogonality. within the class of non p-orthogonal states a finer measure of non p-orthogonality is provided by araki's angles between p-internal spaces. the p-orthogonality concept is a geometric measure of indistinguishability that is independent of the representation chosen for the quantum states. it induces a new hierarchy of approximations for group function methods. the simplifications that occur in the calculation of matrix elements between p-orthogonal group functions are presented."
"unparticles ($\u$) interact weakly with particles. the direct signature of unparticles will be in the form of missing energy. we study constraints on unparticle interactions using totally invisible decay modes of $z$, vector quarkonia $v$ and neutrinos. the constraints on the unparticle interaction scale $\lambda_\u$ are very sensitive to the dimension $d_\u$ of the unparticles. from invisible $z$ and $v$ decays, we find that with $d_\u$ close to 1 for vector $\u$, the unparticle scale $\lambda_\u$ can be more than $10^4$ tev, and for $d_\u$ around 2, the scale can be lower than one tev. from invisible neutrino decays, we find that if $d_\u$ is close to 3/2, the scale can be more than the planck mass, but with $d_\u$ around 2 the scale can be as low as a few hundred gev. we also study the possibility of using $v (z)\to \gamma + \u$ to constrain unparticle interactions, and find that present data give weak constraints."
"this paper surveys some well-established approaches on the approximation of bayes factors used in bayesian model choice, mostly as covered in chen et al. (2000). our focus here is on methods that are based on importance sampling strategies rather than variable dimension techniques like reversible jump mcmc, including: crude monte carlo, maximum likelihood based importance sampling, bridge and harmonic mean sampling, as well as chib's method based on the exploitation of a functional equality. we demonstrate in this survey how these different methods can be efficiently implemented for testing the significance of a predictive variable in a probit model. finally, we compare their performances on a real dataset."
"we introduce real log canonical threshold and real jumping numbers for real algebraic functions. a real jumping number is a root of the $b$-function up to a sign if its difference with the minimal one is less than 1. the real log canonical threshold, which is the minimal real jumping number, coincides up to a sign with the maximal pole of the distribution defined by the complex power of the absolute value of the function. however, this number may be greater than 1 if the codimension of the real zero locus of the function is greater than 1. so it does not necessarily coincide with the maximal root of the b-function up to a sign, nor with the log canonical threshold of the complexification. in fact, the real jumping numbers can be even disjoint from the non-integral jumping numbers of the complexification."
"data extracted from social media platforms, such as twitter, are both large in scale and complex in nature, since they contain both unstructured text, as well as structured data, such as time stamps and interactions between users. a key question for such platforms is to determine influential users, in the sense that they generate interactions between members of the platform. common measures used both in the academic literature and by companies that provide analytics services are variants of the popular web-search pagerank algorithm applied to networks that capture connections between users. in this work, we develop a modeling framework using multivariate interacting counting processes to capture the detailed actions that users undertake on such platforms, namely posting original content, reposting and/or mentioning other users' postings. based on the proposed model, we also derive a novel influence measure. we discuss estimation of the model parameters through maximum likelihood and establish their asymptotic properties. the proposed model and the accompanying influence measure are illustrated on a data set covering a five year period of the twitter actions of the members of the us senate, as well as mainstream news organizations and media personalities."
"we study a hybrid tree-finite difference method which permits to obtain efficient and accurate european and american option prices in the heston hull-white and heston hull-white2d models. moreover, as a by-product, we provide a new simulation scheme to be used for monte carlo evaluations. numerical results show the reliability and the efficiency of the proposed methods"
"this article investigates the causality structure of financial time series. we concentrate on three main approaches to measuring causality: linear granger causality, kernel generalisations of granger causality (based on ridge regression and the hilbert--schmidt norm of the cross-covariance operator) and transfer entropy, examining each method and comparing their theoretical properties, with special attention given to the ability to capture nonlinear causality. we also present the theoretical benefits of applying non-symmetrical measures rather than symmetrical measures of dependence. we apply the measures to a range of simulated and real data. the simulated data sets were generated with linear and several types of nonlinear dependence, using bivariate, as well as multivariate settings. an application to real-world financial data highlights the practical difficulties, as well as the potential of the methods. we use two real data sets: (1) u.s. inflation and one-month libor; (2) s$\&$p data and exchange rates for the following currencies: audjpy, cadjpy, nzdjpy, audchf, cadchf, nzdchf. overall, we reach the conclusion that no single method can be recognised as the best in all circumstances, and each of the methods has its domain of best applicability. we also highlight areas for improvement and future research."
"in a plethora of applications dealing with inverse problems, e.g. in image processing, social networks, compressive sensing, biological data processing etc., the signal of interest is known to be structured in several ways at the same time. this premise has recently guided the research to the innovative and meaningful idea of imposing multiple constraints on the parameters involved in the problem under study. for instance, when dealing with problems whose parameters form sparse and low-rank matrices, the adoption of suitably combined constraints imposing sparsity and low-rankness, is expected to yield substantially enhanced estimation results. in this paper, we address the spectral unmixing problem in hyperspectral images. specifically, two novel unmixing algorithms are introduced, in an attempt to exploit both spatial correlation and sparse representation of pixels lying in homogeneous regions of hyperspectral images. to this end, a novel convex mixed penalty term is first defined consisting of the sum of the weighted $\ell_1$ and the weighted nuclear norm of the abundance matrix corresponding to a small area of the image determined by a sliding square window. this penalty term is then used to regularize a conventional quadratic cost function and impose simultaneously sparsity and row-rankness on the abundance matrix. the resulting regularized cost function is minimized by a) an incremental proximal sparse and low-rank unmixing algorithm and b) an algorithm based on the alternating minimization method of multipliers (admm). the effectiveness of the proposed algorithms is illustrated in experiments conducted both on simulated and real data."
"gaussian graphical models are parametric statistical models for jointly normal random variables whose dependence structure is determined by a graph. in previous work, we introduced trek separation, which gives a necessary and sufficient condition in terms of the graph for when a subdeterminant is zero for all covariance matrices that belong to the gaussian graphical model. here we extend this result to give explicit cancellation-free formulas for the expansions of nonzero subdeterminants."
"in the quest for signatures of coherent transport we consider exciton trapping in the continuous-time quantum walk framework. the survival probability displays different decay domains, related to distinct regions of the spectrum of the hamiltonian. for linear systems and at intermediate times the decay obeys a power-law, in contrast to the corresponding exponential decay found in incoherent continuous-time random walk situations. to differentiate between the coherent and incoherent mechanisms, we present an experimental protocol based on a frozen rydberg gas structured by optical dipole traps."
"we describe a new method to estimate the geometry of a room given room impulse responses. the method utilises convolutional neural networks to estimate the room geometry and uses the mean square error as the loss function. in contrast to existing methods, we do not require the position or distance of sources or receivers in the room. the method can be used with only a single room impulse response between one source and one receiver for room geometry estimation. the proposed estimation method can achieve an average of six centimetre accuracy. in addition, the proposed method is shown to be computationally efficient compared to state-of-the-art methods."
"we compute the generating function of random planar quadrangulations with three marked vertices at prescribed pairwise distances. in the scaling limit of large quadrangulations, this discrete three-point function converges to a simple universal scaling function, which is the continuous three-point function of pure 2d quantum gravity. we give explicit expressions for this universal three-point function both in the grand-canonical and canonical ensembles. various limiting regimes are studied when some of the distances become large or small. by considering the case where the marked vertices are aligned, we also obtain the probability law for the number of geodesic points, namely vertices that lie on a geodesic path between two given vertices, and at prescribed distances from these vertices."
"the data augmentation (da) algorithm is a widely used markov chain monte carlo (mcmc) algorithm that is based on a markov transition density of the form $p(x|x')=\int_{\mathsf{y}}f_{x|y}(x|y)f_{y|x}(y|x') dy$, where $f_{x|y}$ and $f_{y|x}$ are conditional densities. the px-da and marginal augmentation algorithms of liu and wu [j. amer. statist. assoc. 94 (1999) 1264--1274] and meng and van dyk [biometrika 86 (1999) 301--320] are alternatives to da that often converge much faster and are only slightly more computationally demanding. the transition densities of these alternative algorithms can be written in the form $p_r(x|x')=\int_{\mathsf{y}}\int _{\mathsf{y}}f_{x|y}(x|y')r(y,dy')f_{y|x}(y|x') dy$, where $r$ is a markov transition function on $\mathsf{y}$. we prove that when $r$ satisfies certain conditions, the mcmc algorithm driven by $p_r$ is at least as good as that driven by $p$ in terms of performance in the central limit theorem and in the operator norm sense. these results are brought to bear on a theoretical comparison of the da, px-da and marginal augmentation algorithms. our focus is on situations where the group structure exploited by liu and wu is available. we show that the px-da algorithm based on haar measure is at least as good as any px-da algorithm constructed using a proper prior on the group."
"hardware support for deep convolutional neural networks (cnns) is critical to advanced computer vision in mobile and embedded devices. current designs, however, accelerate generic cnns; they do not exploit the unique characteristics of real-time vision. we propose to use the temporal redundancy in natural video to avoid unnecessary computation on most frames. a new algorithm, activation motion compensation, detects changes in the visual input and incrementally updates a previously-computed output. the technique takes inspiration from video compression and applies well-known motion estimation techniques to adapt to visual changes. we use an adaptive key frame rate to control the trade-off between efficiency and vision quality as the input changes. we implement the technique in hardware as an extension to existing state-of-the-art cnn accelerator designs. the new unit reduces the average energy per frame by 54.2%, 61.7%, and 87.6% for three cnns with less than 1% loss in vision accuracy."
"in this paper we present a kinetic model with stochastic game-type interactions, analyzing the relationship between the level of political competition in a society and the degree of economic liberalization. the above issue regards the complex interactions between economy and institutional policies intended to introduce technological innovations in a society, where technological innovations are intended in a broad sense comprehending reforms critical to production. a special focus is placed on the political replacement effect described in a macroscopic model by acemoglu and robinson (ar-model, henceforth), which can determine the phenomenon of innovation 'blocking', possibly leading to economic backwardness. one of the goals of our modelization is to obtain a mesoscopic dynamical model whose macroscopic outputs are qualitatively comparable with stylized facts of the ar-model. a set of numerical solutions is presented showing the non monotonous relationship between economic liberization and political competition, which can be considered as an emergent phenomenon of the complex socio-economic interaction dynamic."
"isoprene is one of the most abundant endogenous volatile organic compounds (vocs) contained in human breath and is considered to be a potentially useful biomarker for diagnostic and monitoring purposes. however, neither the exact biochemical origin of isoprene nor its physiological role are understood in sufficient depth, thus hindering the validation of breath isoprene tests in clinical routine.   exhaled isoprene concentrations are reported to change under different clinical and physiological conditions, especially in response to enhanced cardiovascular and respiratory activity. investigating isoprene exhalation kinetics under dynamical exercise helps to gather the relevant experimental information for understanding the gas exchange phenomena associated with this important voc.   a first model for isoprene in exhaled breath has been developed by our research group. in the present paper, we aim at giving a concise overview of this model and describe its role in providing supportive evidence for a peripheral (extrahepatic) source of isoprene. in this sense, the results presented here may enable a new perspective on the biochemical processes governing isoprene formation in the human body."
"dose-finding studies are frequently conducted to evaluate the effect of different doses or concentration levels of a compound on a response of interest. applications include the investigation of a new medicinal drug, a herbicide or fertilizer, a molecular entity, an environmental toxin, or an industrial chemical. in pharmaceutical drug development, dose-finding studies are of critical importance because of regulatory requirements that marketed doses are safe and provide clinically relevant efficacy. motivated by a dose-finding study in moderate persistent asthma, we propose response-adaptive designs addressing two major challenges in dose-finding studies: uncertainty about the dose-response models and large variability in parameter estimates. to allocate new cohorts of patients in an ongoing study, we use optimal designs that are robust under model uncertainty. in addition, we use a bayesian shrinkage approach to stabilize the parameter estimates over the successive interim analyses used in the adaptations. this approach allows us to calculate updated parameter estimates and model probabilities that can then be used to calculate the optimal design for subsequent cohorts. the resulting designs are hence robust with respect to model misspecification and additionally can efficiently adapt to the information accrued in an ongoing study. we focus on adaptive designs for estimating the minimum effective dose, although alternative optimality criteria or mixtures thereof could be used, enabling the design to address multiple objectives."
"in this paper, we propose a new differentiable neural network alignment mechanism for text-dependent speaker verification which uses alignment models to produce a supervector representation of an utterance. unlike previous works with similar approaches, we do not extract the embedding of an utterance from the mean reduction of the temporal dimension. our system replaces the mean by a phrase alignment model to keep the temporal structure of each phrase which is relevant in this application since the phonetic information is part of the identity in the verification task. moreover, we can apply a convolutional neural network as front-end, and thanks to the alignment process being differentiable, we can train the whole network to produce a supervector for each utterance which will be discriminative with respect to the speaker and the phrase simultaneously. as we show, this choice has the advantage that the supervector encodes the phrase and speaker information providing good performance in text-dependent speaker verification tasks. in this work, the process of verification is performed using a basic similarity metric, due to simplicity, compared to other more elaborate models that are commonly used. the new model using alignment to produce supervectors was tested on the rsr2015-part i database for text-dependent speaker verification, providing competitive results compared to similar size networks using the mean to extract embeddings."
"this paper deals with the long-time behavior of solutions of nonlinear reaction-diffusion equations describing formation of morphogen gradients, the concentration fields of molecules acting as spatial regulators of cell differentiation in developing tissues. for the considered class of models, we establish existence of a new type of ultra-singular self-similar solutions. these solutions arise as limits of the solutions of the initial value problem with zero initial data and infinitely strong source at the boundary. we prove existence and uniqueness of such solutions in the suitable weighted energy spaces. moreover, we prove that the obtained self-similar solutions are the long-time limits of the solutions of the initial value problem with zero initial data and a time-independent boundary source."
"the term active matter describes diverse systems, spanning macroscopic (e.g. shoals of fish and flocks of birds) to microscopic scales (e.g. migrating cells, motile bacteria and gels formed through the interaction of nanoscale molecular motors with cytoskeletal filaments within cells). such systems are often idealizable in terms of collections of individual units, referred to as active particles or self-propelled particles, which take energy from an internal replenishable energy depot or ambient medium and transduce it into useful work performed on the environment, in addition to dissipating a fraction of this energy into heat. these individual units may interact both directly as well as through disturbances propagated via the medium in which they are immersed. active particles can exhibit remarkable collective behaviour as a consequence of these interactions, including non-equilibrium phase transitions between novel dynamical phases, large fluctuations violating expectations from the central limit theorem and substantial robustness against the disordering effects of thermal fluctuations. in this chapter, following a brief summary of experimental systems which may be classified as examples of active matter, i describe some of the principles which underlie the modeling of such systems."
a survey on algorithms for computing discrete logarithms in jacobians of curves over finite fields.
"we investigate two classes of transformations of cosine similarity and pearson and spearman correlations into metric distances, utilising the simple tool of metric-preserving functions. the first class puts anti-correlated objects maximally far apart. previously known transforms fall within this class. the second class collates correlated and anti-correlated objects. an example of such a transformation that yields a metric distance is the sine function when applied to centered data."
"gaussian mixtures are a common density representation in nonlinear, non-gaussian bayesian state estimation. selecting an appropriate number of gaussian components, however, is difficult as one has to trade of computational complexity against estimation accuracy. in this paper, an adaptive gaussian mixture filter based on statistical linearization is proposed. depending on the nonlinearity of the considered estimation problem, this filter dynamically increases the number of components via splitting. for this purpose, a measure is introduced that allows for quantifying the locally induced linearization error at each gaussian mixture component. the deviation between the nonlinear and the linearized state space model is evaluated for determining the splitting direction. the proposed approach is not restricted to a specific statistical linearization method. simulations show the superior estimation performance compared to related approaches and common filtering algorithms."
"spatial ecological networks are widely used to model interactions between georeferenced biological entities (e.g., populations or communities). the analysis of such data often leads to a two-step approach where groups containing similar biological entities are firstly identified and the spatial information is used afterwards to improve the ecological interpretation. we develop an integrative approach to retrieve groups of nodes that are geographically close and ecologically similar. our model-based spatially-constrained method embeds the geographical information within a regularization framework by adding some constraints to the maximum likelihood estimation of parameters. a simulation study and the analysis of real data demonstrate that our approach is able to detect complex spatial patterns that are ecologically meaningful. the model-based framework allows us to consider external information (e.g., geographic proximities, covariates) in the analysis of ecological networks and appears to be an appealing alternative to consider such data."
"we investigate the multiple use of a ferromagnetic spin chain for quantum and classical communications without resetting. we find that the memory of the state transmitted during the first use makes the spin chain a qualitatively different quantum channel during the second transmission, for which we find the relevant kraus operators. we propose a parameter to quantify the amount of memory in the channel and find that it influences the quality of the channel, as reflected through fidelity and entanglement transmissible during the second use. for certain evolution times, the memory allows the channel to exceed the memoryless classical capacity (achieved by separable inputs) and in some cases it can also enhance the quantum capacity."
"the beta function of a two-dimensional massless dirac hamiltonian subject to a random scalar potential, which e.g., underlies the theoretical description of graphene, is computed numerically. although it belongs to, from a symmetry standpoint, the two-dimensional symplectic class, the beta function monotonically increases with decreasing $g$. we also provide an argument based on the spectral flows under twisting boundary conditions, which shows that none of states of the massless dirac hamiltonian can be localized."
"efficient and automated classification of periodic variable stars is becoming increasingly important as the scale of astronomical surveys grows. several recent papers have used methods from machine learning and statistics to construct classifiers on databases of labeled, multi--epoch sources with the intention of using these classifiers to automatically infer the classes of unlabeled sources from new surveys. however, the same source observed with two different synoptic surveys will generally yield different derived metrics (features) from the light curve. since such features are used in classifiers, this survey-dependent mismatch in feature space will typically lead to degraded classifier performance. in this paper we show how and why feature distributions change using ogle and \textit{hipparcos} light curves. to overcome survey systematics, we apply a method, \textit{noisification}, which attempts to empirically match distributions of features between the labeled sources used to construct the classifier and the unlabeled sources we wish to classify. results from simulated and real--world light curves show that noisification can significantly improve classifier performance. in a three--class problem using light curves from \textit{hipparcos} and ogle, noisification reduces the classifier error rate from 27.0% to 7.0%. we recommend that noisification be used for upcoming surveys such as gaia and lsst and describe some of the promises and challenges of applying noisification to these surveys."
"in statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. as in previous studies, we propose to solve this problem through latent factorization. however, here we make use of complex valued embeddings. the composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. compared to state-of-the-art models such as neural tensor network and holographic embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the hermitian dot product, the complex counterpart of the standard dot product between real vectors. our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks."
"the key limiting factor in graphical model inference and learning is the complexity of the partition function. we thus ask the question: what are general conditions under which the partition function is tractable? the answer leads to a new kind of deep architecture, which we call sum-product networks (spns). spns are directed acyclic graphs with variables as leaves, sums and products as internal nodes, and weighted edges. we show that if an spn is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes. essentially all tractable graphical models can be cast as spns, but spns are also strictly more general. we then propose learning algorithms for spns, based on backpropagation and em. experiments show that inference and learning with spns can be both faster and more accurate than with standard deep networks. for example, spns perform image completion better than state-of-the-art deep networks for this task. spns also have intriguing potential connections to the architecture of the cortex."
"we postulate a principle stating that the initial condition of a physical system is typically algorithmically independent of the dynamical law. we argue that this links thermodynamics and causal inference. on the one hand, it entails behaviour that is similar to the usual arrow of time. on the other hand, it motivates a statistical asymmetry between cause and effect that has recently postulated in the field of causal inference, namely, that the probability distribution p(cause) contains no information about the conditional distribution p(effect|cause) and vice versa, while p(effect) may contain information about p(cause|effect)."
"bacteriocins are peptide-derived molecules produced by bacteria, whose recently-discovered functions include virulence factors and signalling molecules as well as their better known roles as antibiotics. to date, close to five hundred bacteriocins have been identified and classified. recent discoveries have shown that bacteriocins are highly diverse and widely distributed among bacterial species. given the heterogeneity of bacteriocin compounds, many tools struggle with identifying novel bacteriocins due to their vast sequence and structural diversity. many bacteriocins undergo post-translational processing or modifications necessary for the biosynthesis of the final mature form. enzymatic modification of bacteriocins as well as their export is achieved by proteins whose genes are often located in a discrete gene cluster proximal to the bacteriocin precursor gene, referred to as \textit{context genes} in this study. although bacteriocins themselves are structurally diverse, context genes have been shown to be largely conserved across unrelated species. using this knowledge, we set out to identify new candidates for context genes which may clarify how bacteriocins are synthesized, and identify new candidates for bacteriocins that bear no sequence similarity to known toxins. to achieve these goals, we have developed a software tool, bacteriocin operon and gene block associator (boa) that can identify homologous bacteriocin associated gene clusters and predict novel ones. we discover that several phyla have a strong preference for bactericon genes, suggesting distinct functions for this group of molecules. availability: https://github.com/idoerg/boa"
"in this paper, we develop simple, yet efficient, procedures for sampling approximations of the two-parameter poisson-dirichlet process and the normalized inverse-gaussian process. we compare the efficiency of the new approximations to the corresponding stick-breaking approximations, in which we demonstrate a substantial improvement."
"the modular variety of non singular and complete hyperelliptic curves with level-two structure of genus 3 is a 5-dimensional quasi projective variety which admits several standard compactifications. the first one, x, comes from the realization of this variety as a sub-variety of the siegel modular variety of level two and genus three .we will be to describe the equations of x in a suitable projective embedding and its hilbert function. it will turn out that   x is normal. a further model comes from geometric invariant theory using so-called semistable degenerated point configurations in (p^1)^8 . we denote this git-compactification by y. the equations of this variety in a suitable projective embedding are known. this variety also can by identified with a baily-borel compactified ball-quotient. we will describe these results in some detail and obtain new proofs including some finer results for them. we have a birational map between y and x . in this paper we use the fact that there are graded algebras (closely related to algebras of modular forms) a,b such that x=proj(a) and y=proj(b). this homomorphism rests on the theory of thomae (19th century), in which the thetanullwerte of hyperelliptic curves have been computed. using the explicit equations for $a,b$ we can compute the base locus of the map from y to x.   blowing up the base locus and the singularity of y, we get a dominant, smooth model {\tilde y}. we will see that {\tilde y} is isomorphic to the compactification of families of marked projective lines (p^1,x_1,...,x_8), usually denoted by {\bar m_{0,8}}. there are several combinatorial similarities between the models x and y. these similarities can be described best, if one uses the ball-model to describe y."
"in certain neighborhood $u$ of an arbitrary point of a symplectic manifold $m$ we construct a fedosov-type star-product $\ast_l$ such that for an arbitrary leaf $\wp$ of a given polarization $\mathcal{d}\subset tm$ the algebra $c^\infty (\wp \cap u)[[h]]$ has a natural structure of left module over the deformed algebra $(c^\infty (u)[[h]], \ast_l)$. with certain additional assumptions on $m$, $\ast_l$ becomes a so-called star-product with separation of variables."
"we address the problem of approximating the posterior probability distribution of the fixed parameters of a state-space dynamical system using a sequential monte carlo method. the proposed approach relies on a nested structure that employs two layers of particle filters to approximate the posterior probability measure of the static parameters and the dynamic state variables of the system of interest, in a vein similar to the recent ""sequential monte carlo square"" (smc$^2$) algorithm. however, unlike the smc$^2$ scheme, the proposed technique operates in a purely recursive manner. in particular, the computational complexity of the recursive steps of the method introduced herein is constant over time. we analyse the approximation of integrals of real bounded functions with respect to the posterior distribution of the system parameters computed via the proposed scheme. as a result, we prove, under regularity assumptions, that the approximation errors vanish asymptotically in $l_p$ ($p \ge 1$) with convergence rate proportional to $\frac{1}{\sqrt{n}} + \frac{1}{\sqrt{m}}$, where $n$ is the number of monte carlo samples in the parameter space and $n\times m$ is the number of samples in the state space. this result also holds for the approximation of the joint posterior distribution of the parameters and the state variables. we discuss the relationship between the smc$^2$ algorithm and the new recursive method and present a simple example in order to illustrate some of the theoretical findings with computer simulations."
the interaction of charges in nqed is discussed. it is shown that the relativistic correction have the same form as in the commutative case provided the weyl ordering rule is used.
"consider semiparametric models that display local asymptotic exponentiality (ibragimov and has'minskii (1981)), an asymptotic property of the likelihood associated with discontinuities of densities. our interest goes to estimation of the location of such discontinuities while other aspects of the density form a nuisance parameter. it is shown that under certain conditions on model and prior, the posterior distribution displays bernstein-von mises-type asymptotic behaviour, with exponential distributions as the limiting sequence. in contrast to regular settings, the maximum likelihood estimator is inefficient under this form of irregularity. however, bayesian point estimators based on the limiting posterior distribution attain the minimax risk. therefore, the limiting behaviour of the posterior is used to advocate efficiency of bayesian point estimation rather than compare it to frequentist estimation procedures based on the maximum likelihood estimator. results are applied to semiparametric lae location and scaling examples."
"there are several distinct failure modes for overoptimization of systems on the basis of metrics. this occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed goodhart's law. this class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. this paper expands on an earlier discussion by garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to goodhart's law. this paper is intended to explore these mechanisms further, and specify more clearly how they occur. this discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in artificial intelligence alignment. the importance of goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field."
"by a method inspired of the stein's method, we derive an upper-bound of the rubinstein distance between two absolutely continuous probability measures on configurations space. as an application, we show that the best way to approximate a modulated poisson process (see below for the definition) by a poisson process is to equate their intensity."
"i show the equivalence between a model of financial contagion and the threshold model of global cascades proposed by watts (2002). the model financial network comprises banks that hold risky external assets as well as interbank assets. it is shown that a simple threshold model can replicate the size and the frequency of financial contagion without using information about individual balance sheets. keywords: financial network, cascades, financial contagion, systemic risk."
"we study real-space condensation phenomena in a type of classical stochastic processes (site-particle system), such as zero-range processes and urn models. we here study a stochastic process in the ehrenfest class, i.e., particles in a site are distinguishable. in terms of the statistical mechanical analogue, the ehrenfest class obeys the maxwell-boltzmann statistics. we analytically clarify conditions for condensation phenomena in disordered cases in the ehrenfest class. in addition, we discuss the preferential urn model as an example of the disordered urn model. it becomes clear that the quenched disorder property plays an important role in the occurrence of the condensation phenomenon in the preferential urn model. it is revealed that the preferential urn model shows three types of condensation depending on the disorder parameters."
"recent research on the network modeling of complex systems has led to a convenient representation of numerous natural, social, and engineered systems that are now recognized as networks of interacting parts. such systems can exhibit a wealth of phenomena that not only cannot be anticipated from merely examining their parts, as per the textbook definition of complexity, but also challenge intuition even when considered in the context of what is now known in network science. here we review the recent literature on two major classes of such phenomena that have far-reaching implications: (i) antagonistic responses to changes of states or parameters and (ii) coexistence of seemingly incongruous behaviors or properties -- both deriving from the collective and inherently decentralized nature of the dynamics. they include effects as diverse as negative compressibility in engineered materials, rescue interactions in biological networks, negative resistance in fluid networks, and the braess paradox occurring across transport and supply networks. they also include remote synchronization, chimera states and the converse of symmetry breaking in brain, power-grid and oscillator networks as well as remote control in biological and bio-inspired systems. by offering a unified view of these various scenarios, we suggest that they are representative of a yet broader class of unprecedented network phenomena that ought to be revealed and explained by future research."
"we revisit a recently proposed agent-based model of active biological motion and compare its predictions with own experimental findings for the speed distribution of bacterial cells, \emph{salmonella typhimurium}. agents move according to a stochastic dynamics and use energy stored in an internal depot for metabolism and active motion. we discuss different assumptions of how the conversion from internal to kinetic energy $d(v)$ may depend on the actual speed, to conclude that $d_{2}v^{\xi}$ with either $\xi=2$ or $1<\xi<2$ are promising hypotheses. to test these, we compare the model's prediction with the speed distribution of bacteria which were obtained in media of different nutrient concentration and at different times. we find that both hypotheses are in line with the experimental observations, with $\xi$ between 1.67 and 2.0. regarding the influence of a higher nutrient concentration, we conclude that the take-up of energy by bacterial cells is indeed increased. but this energy is not used to increase the speed, with 40$\mu$m/s as the most probable value of the speed distribution, but is rather spend on metabolism and growth."
"we investigate two training-set methods: support vector machines (svms) and kernel regression (kr) for photometric redshift estimation with the data from the sloan digital sky survey data release 5 and two micron all sky survey databases. we probe the performances of svms and kr for different input patterns. our experiments show that the more parameters considered, the accuracy doesn't always increase, and only when appropriate parameters chosen, the accuracy can improve. moreover for different approaches, the best input pattern is different. with different parameters as input, the optimal bandwidth is dissimilar for kr. the rms errors of photometric redshifts based on svm and kr methods are less than 0.03 and 0.02, respectively. finally the strengths and weaknesses of the two approaches are summarized. compared to other methods of estimating photometric redshifts, they show their superiorities, especially kr, in terms of accuracy."
"we use entropy to characterize intrinsic ageing properties of the human brain. analysis of fmri data from a large dataset of individuals, using resting state bold signals, demonstrated that a functional entropy associated with brain activity increases with age. during an average lifespan, the entropy, which was calculated from a population of individuals, increased by approximately 0.1 bits, due to correlations in bold activity becoming more widely distributed. we attribute this to the number of excitatory neurons and the excitatory conductance decreasing with age. incorporating these properties into a computational model leads to quantitatively similar results to the fmri data. our dataset involved males and females and we found significant differences between them. the entropy of males at birth was lower than that of females. however, the entropies of the two sexes increase at different rates, and intersect at approximately 50 years; after this age, males have a larger entropy."
"black box variational inference allows researchers to easily prototype and evaluate an array of models. recent advances allow such algorithms to scale to high dimensions. however, a central question remains: how to specify an expressive variational distribution that maintains efficient computation? to address this, we develop hierarchical variational models (hvms). hvms augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. the algorithm we develop is black box, can be used for any hvm, and has the same computational efficiency as the original approximation. we study hvms on a variety of deep discrete latent variable models. hvms generalize other expressive variational distributions and maintains higher fidelity to the posterior."
"the frank-wolfe (fw) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. however, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. a simple less-known fix is to add the possibility to take 'away steps' during optimization, an operation that importantly does not require a feasibility oracle. in this paper, we highlight and clarify several variants of the frank-wolfe optimization algorithm that have been successfully applied in practice: away-steps fw, pairwise fw, fully-corrective fw and wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of the objective. the constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of a 'condition number' of the constraint set. we provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization."
"a general method to combine several estimators of the same quantity is investigated. in the spirit of model and forecast averaging, the final estimator is computed as a weighted average of the initial ones, where the weights are constrained to sum to one. in this framework, the optimal weights, minimizing the quadratic loss, are entirely determined by the mean square error matrix of the vector of initial estimators. the averaging estimator is built using an estimation of this matrix, which can be computed from the same dataset. a non-asymptotic error bound on the averaging estimator is derived, leading to asymptotic optimality under mild conditions on the estimated mean square error matrix. this method is illustrated on standard statistical problems in parametric and semi-parametric models where the averaging estimator outperforms the initial estimators in most cases."
"compliance to standardized highway design criteria is considered essential to ensure the roadway safety. however, for a variety of reasons, situations arise where exceptions to standard-design criteria are requested and accepted after review. this research explores the impact that design exceptions have on the accident severity and accident frequency in indiana. data on accidents at roadway sites with and without design exceptions are used to estimate appropriate statistical models for the frequency and severity accidents at these sites using some of the most recent statistical advances with mixing distributions. the results of the modeling process show that presence of approved design exceptions has not had a statistically significant effect on the average frequency or severity of accidents -- suggesting that current procedures for granting design exceptions have been sufficiently rigorous to avoid adverse safety impacts."
"this is a review of the authors' recent results on an integrable structure of the melting crystal model with external potentials. the partition function of this model is a sum over all plane partitions (3d young diagrams). by the method of transfer matrices, this sum turns into a sum over ordinary partitions (young diagrams), which may be thought of as a model of q -deformed random partitions. this model can be further translated to the language of a complex fermion system. a fermionic realization of the quantum torus lie algebra is shown to underlie therein. with the aid of hidden symmetry of this lie algebra, the partition function of the melting crystal model turns out to coincide, up to a simple factor, with a tau function of the 1d toda hierarchy. some related issues on 4d and 5d supersymmetric yang-mills theories, topological strings and the 2d toda hierarchy are briefly discussed."
"we re-analyze d8 brane embeddings in the geometry of a d4 brane wrapped on a circle that describe chiral symmetry breaking in a strongly coupled non-supersymmetric gauge theory. we argue that if the holographic fields are correctly interpreted, the original embeddings describe a complex quark mass and condensate in the theory. we show that in this interpretation when a quark mass is present there is a massive pseudo goldstone boson (pion). a previously identified massless fluctuation is, we argue, not a physical state in the field theory. we also determine the behaviour of the quark condensate as a function of the quark mass."
"we obtain an index of the complexity of a random sequence by allowing the role of the measure in classical probability theory to be played by a function we call the generating mechanism. typically, this generating mechanism will be a finite automata. we generate a set of biased sequences by applying a finite state automata with a specified number, $m$, of states to the set of all binary sequences. thus we can index the complexity of our random sequence by the number of states of the automata. we detail optimal algorithms to predict sequences generated in this way."
"we propose a general modeling framework for marked poisson processes observed over time or space. the modeling approach exploits the connection of the nonhomogeneous poisson process intensity with a density function. nonparametric dirichlet process mixtures for this density, combined with nonparametric or semiparametric modeling for the mark distribution, yield flexible prior models for the marked poisson process. in particular, we focus on fully nonparametric model formulations that build the mark density and intensity function from a joint nonparametric mixture, and provide guidelines for straightforward application of these techniques. a key feature of such models is that they can yield flexible inference about the conditional distribution for multivariate marks without requiring specification of a complicated dependence scheme. we address issues relating to choice of the dirichlet process mixture kernels, and develop methods for prior specification and posterior simulation for full inference about functionals of the marked poisson process. moreover, we discuss a method for model checking that can be used to assess and compare goodness of fit of different model specifications under the proposed framework. the methodology is illustrated with simulated and real data sets."
"we systematically investigate the bubble-induced performance degradation for underwater wireless optical communication (uowc) with different bubble sizes and positions. by using different transmit and receive diversities, we investigate the effectiveness of transmit/receive diversity on the mitigation of the bubble-induced impairment to the uowc link. with the help of a 2 by 2 mimo using repetition coding (rc) and maximum ratio combining (mrc), a robust 780-mbit/s uowc transmission is achieved. the corresponding outage probability can be significantly reduced from 34.6% for the system without diversity to less than 1%."
"we show, that photon non-linearities in electromagnetically induced transparency can be at least one order of magnitude larger than predicted in all previous approaches. as an application we demonstrate that, in this regime they give rise to very strong photon - photon interactions which are strong enough to make an experimental realization of a photonic mott insulator state feasible in arrays of coupled ultra high-q micro-cavities."
we demonstrate how information in the form of observable data and moment constraints are introduced into the method of maximum relative entropy (me). a general example of updating with data and moments is shown. a specific econometric example is solved in detail which can then be used as a template for real world problems. a numerical example is compared to a large deviation solution which illustrates some of the advantages of the me method.
"in this paper, an analytical approach for the nonlinear distorted bit error rate performance of optical orthogonal frequency division multiplexing (o-ofdm) with single photon avalanche diode (spad) receivers is presented. major distortion effects of passive quenching (pq) and active quenching (aq) spad receivers are analysed in this study. the performance analysis of dc-biased o-ofdm and asymmetrically clipped o-ofdm with pq and aq spad are derived. the comparison results show the maximum optical irradiance caused by the nonlinear distortion, which limits the transmission power and bit rate. the theoretical maximum bit rate of spad-based ofdm is found which is up to 1~gbits/s. this approach supplies a closed-form analytical solution for designing an optimal spad-based system."
"for $a$ a $c^*$-algebra, $e_1, e_2$ two hilbert bimodules over $a$, and a fixed isomorphism $\chi : e_1\otimes_ae_2\to e_2\otimes_ae_1$, we consider the problem of computing the $k$-theory of the cuntz-pimsner algebra ${\mathcal o}_{e_2\otimes_a{\mathcal o}_{e_1}}$ obtained by extending the scalars and by iterating the pimsner construction.   the motivating examples are a commutative diagram of douglas and howe for the toeplitz operators on the quarter plane, and the toeplitz extensions associated by pimsner and voiculescu to compute the $k$-theory of a crossed product. the applications are for hilbert bimodules arising from rank two graphs and from commuting endomorphisms of abelian $c^*$-algebras."
we present a theoretical study of the electronic structure of modulated graphene in the presence of a perpendicular magnetic field. the density of states and the bandwidth for the dirac electrons in this system are determined. the appearance of unusual weiss oscillations in the bandwidth and density of states is the main focus of this work.
longitudinal spin physics program at rhic-phenix is introduced. recent results of pi0 cross section and a_ll are presented and discussed.
we construct a relative chow-kunneth decomposition for a conic bundle over a surface such that the middle projector gives the prym variety of the associated double covering of the discriminant of the conic bundle. this gives a refinement (up to an isogeny) of beauville's theorem on the relation between the intermediate jacobian of the conic bundle and the prym variety of the double covering.
"this paper raises concerns about the advantages of using statistical significance tests in research assessments as has recently been suggested in the debate about proper normalization procedures for citation indicators. statistical significance tests are highly controversial and numerous criticisms have been leveled against their use. based on examples from articles by proponents of the use statistical significance tests in research assessments, we address some of the numerous problems with such tests. the issues specifically discussed are the ritual practice of such tests, their dichotomous application in decision making, the difference between statistical and substantive significance, the implausibility of most null hypotheses, the crucial assumption of randomness, as well as the utility of standard errors and confidence intervals for inferential purposes. we argue that applying statistical significance tests and mechanically adhering to their results is highly problematic and detrimental to critical thinking. we claim that the use of such tests do not provide any advantages in relation to citation indicators, interpretations of them, or the decision making processes based upon them. on the contrary their use may be harmful. like many other critics, we generally believe that statistical significance tests are over- and misused in the social sciences including scientometrics and we encourage a reform on these matters."
"in this survey paper we present recent results obtained by khare, wintenberger and the author that have led to a proof of serre's conjecture, such as existence of compatible families, modular upper bounds for universal deformation rings and existence of minimal lifts, prime switching and modularity propagation, weight reduction (via existence of conjugates) and (iterated) killing ramification.   the main tools used in the proof of these results are modularity lifting theorems a la wiles and a result of potential modularity due to r. taylor."
"the purpose of this contribution is to give an introduction to quantum geometry and loop quantum gravity for a wide audience of both physicists and mathematicians. from a physical point of view the emphasis will be on conceptual issues concerning the relationship of the formalism with other more traditional approaches inspired in the treatment of the fundamental interactions in the standard model. mathematically i will pay special attention to functional analytic issues, the construction of the relevant hilbert spaces and the definition and properties of geometric operators: areas and volumes."
"in the social sciences, there is a longstanding tension between data collection methods that facilitate quantification and those that are open to unanticipated information. advances in technology now enable new, hybrid methods that combine some of the benefits of both approaches. drawing inspiration from online information aggregation systems like wikipedia and from traditional survey research, we propose a new class of research instruments called wiki surveys. just as wikipedia evolves over time based on contributions from participants, we envision an evolving survey driven by contributions from respondents. we develop three general principles that underlie wiki surveys: they should be greedy, collaborative, and adaptive. building on these principles, we develop methods for data collection and data analysis for one type of wiki survey, a pairwise wiki survey. using two proof-of-concept case studies involving our free and open-source website www.allourideas.org, we show that pairwise wiki surveys can yield insights that would be difficult to obtain with other methods."
"absolute positioning is an essential factor for the arrival of autonomous driving. global navigation satellites system (gnss) receiver provides absolute localization for it. gnss solution can provide satisfactory positioning in open or sub-urban areas, however, its performance suffered in super-urbanized area due to the phenomenon which are well-known as multipath effects and nlos receptions. the effects dominate gnss positioning performance in the area. the recent proposed 3d map aided (3dma) gnss can mitigate most of the multipath effects and nlos receptions caused by buildings based on 3d city models. however, the same phenomenon caused by moving objects in urban area is currently not modelled in the 3d geographic information system (gis). moving objects with tall height, such as the double-decker bus, can also cause nlos receptions because of the blockage of gnss signals by surface of objects. therefore, we present a novel method to exclude the nlos receptions caused by double-decker bus in highly urbanized area, hong kong. to estimate the geometry dimension and orientation relative to gps receiver, a euclidean cluster algorithm and a classification method are used to detect the double-decker buses and calculate their relative locations. to increase the accuracy and reliability of the proposed nlos exclusion method, an nlos exclusion criterion is proposed to exclude the blocked satellites considering the elevation, signal noise ratio (snr) and horizontal dilution of precision (hdop). finally, gnss positioning is estimated by weighted least square (wls) method using the remaining satellites after the nlos exclusion. a static experiment was performed near a double-decker bus stop in hong kong, which verified the effectiveness of the proposed method."
"we study the problem of sampling high and infinite dimensional target measures arising in applications such as conditioned diffusions and inverse problems. we focus on those that arise from approximating measures on hilbert spaces defined via a density with respect to a gaussian reference measure. we consider the metropolis-hastings algorithm that adds an accept-reject mechanism to a markov chain proposal in order to make the chain reversible with respect to the target measure. we focus on cases where the proposal is either a gaussian random walk (rwm) with covariance equal to that of the reference measure or an ornstein-uhlenbeck proposal (pcn) for which the reference measure is invariant. previous results in terms of scaling and diffusion limits suggested that the pcn has a convergence rate that is independent of the dimension while the rwm method has undesirable dimension-dependent behaviour. we confirm this claim by exhibiting a dimension-independent wasserstein spectral gap for pcn algorithm for a large class of target measures. in our setting this wasserstein spectral gap implies an $l^2$-spectral gap. we use both spectral gaps to show that the ergodic average satisfies a strong law of large numbers, the central limit theorem and nonasymptotic bounds on the mean square error, all dimension independent. in contrast we show that the spectral gap of the rwm algorithm applied to the reference measures degenerates as the dimension tends to infinity."
"the problem of existence of adaptive confidence bands for an unknown density $f$ that belongs to a nested scale of h\""{o}lder classes over $\mathbb{r}$ or $[0,1]$ is considered. whereas honest adaptive inference in this problem is impossible already for a pair of h\""{o}lder balls $\sigma(r),\sigma(s),r\ne s$, of fixed radius, a nonparametric distinguishability condition is introduced under which adaptive confidence bands can be shown to exist. it is further shown that this condition is necessary and sufficient for the existence of honest asymptotic confidence bands, and that it is strictly weaker than similar analytic conditions recently employed in gin\'{e} and nickl [ann. statist. 38 (2010) 1122--1170]. the exceptional sets for which honest inference is not possible have vanishingly small probability under natural priors on h\""{o}lder balls $\sigma(s)$. if no upper bound for the radius of the h\""{o}lder balls is known, a price for adaptation has to be paid, and near-optimal adaptation is possible for standard procedures. the implications of these findings for a general theory of adaptive inference are discussed."
"we develop a fully bayesian hierarchical model for trend filtering, itself a new development in nonparametric, univariate regression. the framework more broadly applies to the generalized lasso, but focus is on bayesian trend filtering. we compare two shrinkage priors, double exponential and generalized double pareto. a simulation study, comparing bayesian trend filtering to the original formulation and a number of other popular methods shows our method to improve estimation error while maintaining if not improving coverage probability. two time series data sets demonstrate bayesian trend filtering's robustness to possible violations of its assumptions."
"we prove the eventological $h$-theorem that complements the boltzmann h-theorem from statistical mechanics and serves as a mathematical excuse (mathematically no less convincing than the boltzmann h-theorem for the second law of thermodynamics) for what can be called ""the second law of eventology"", which justifies the application of gibbs and ""anti-gibbs"" distributions of sets of events minimizing relative entropy, as statistical models of the behavior of a rational subject, striving for an equilibrium eventological choice between perception and activity in various spheres of her/his co-being."
a proof of a curious planar embedding theorem.
"rna polymerase (rnap) is an enzyme that synthesizes a messenger rna (mrna) strand which is complementary to a single-stranded dna template. from the perspective of physicists, an rnap is a molecular motor that utilizes chemical energy input to move along the track formed by a dna. in many circumstances, which are described in this paper, a large number of rnaps move simultaneously along the same track; we refer to such collective movements of the rnaps as rnap traffic. here we develop a theoretical model for rnap traffic by incorporating the steric interactions between rnaps as well as the mechano-chemical cycle of individual rnaps during the elongation of the mrna. by a combination of analytical and numerical techniques, we calculate the rates of mrna synthesis and the average density profile of the rnaps on the dna track. we also introduce, and compute, two new measures of fluctuations in the synthesis of rna. analyzing these fluctuations, we show how the level of {\it intrinsic noise} in mrna synthesis depends on the concentrations of the rnaps as well as on those of some of the reactants and the products of the enzymatic reactions catalyzed by rnap. we suggest appropriate experimental systems and techniques for testing our theoretical predictions."
"we analyze the properties of the quasiparticle excitations of metallic antiferromagnetic states in a strongly correlated electron system. the study is based on dynamical mean field theory (dmft) for the infinite dimensional hubbard model with antiferromagnetic symmetry breaking. self-consistent solutions of the dmft equations are calculated using the numerical renormalization group (nrg). the low energy behavior in these results is then analyzed in terms of renormalized quasiparticles. the parameters for these quasiparticles are calculated directly from the nrg derived self-energy, and also from the low energy fixed point of the effective impurity. they are found to be in good agreement. we show that the main low energy features of the $\bf k$-resolved spectral density can be understood in terms of the quasiparticle picture. we also find that luttinger's theorem is satisfied for the total electron number in the doped antiferromagnetic state."
"in this paper we introduce a continuous time stochastic neurite branching model closely related to the discrete time stochastic bes-model. the discrete time bes-model is underlying current attempts to simulate cortical development, but is difficult to analyze. the new continuous time formulation facilitates analytical treatment thus allowing us to examine the structure of the model more closely. we derive explicit expressions for the time dependent probabilities p(\gamma, t) for finding a tree \gamma at time t, valid for arbitrary continuous time branching models with tree and segment dependent branching rates. we show, for the specific case of the continuous time bes-model, that as expected from our model formulation, the sums needed to evaluate expectation values of functions of the terminal segment number \mu(f(n),t) do not depend on the distribution of the total branching probability over the terminal segments. in addition, we derive a system of differential equations for the probabilities p(n,t) of finding n terminal segments at time t. for the continuous bes-model, this system of differential equations gives direct numerical access to functions only depending on the number of terminal segments, and we use this to evaluate the development of the mean and standard deviation of the number of terminal segments at a time t. for comparison we discuss two cases where mean and variance of the number of terminal segments are exactly solvable. then we discuss the numerical evaluation of the s-dependence of the solutions for the continuous time bes-model. the numerical results show clearly that higher s values, i.e. values such that more proximal terminal segments have higher branching rates than more distal terminal segments, lead to more symmetrical trees as measured by three tree symmetry indicators."
"in this paper, we address the problem of stable coordinated motion in multi-robot systems with limited fields of view (fovs). these problems arise naturally for multi-robot systems that interact based on sensing, such as our case study of multiple unmanned aerial vehicles (uavs) each equipped with several cameras that are used for detecting neighboring uavs. in this context, our contributions are: i) first, we derive a framework for studying stable motion and distributed topology control for multi-robot systems with limited fovs; and ii) then, we provide experimental results in indoor and challenging outdoor environments (e.g., with wind speeds up to 10 mph) with a team of uavs to demonstrate the performance of the proposed control framework using a portable multi-robot experimental set-up."
"allometry and growth rates of 8 forest species in the uk. the data were collected from two united kingdom woodlands - wytham woods and alice holt. here we present data from 582 individual trees of eight taxa in the form of summary variables. in addition the raw data files containing the variables from which the summary data were obtained. large sample sizes with longitudinal data spanning 22 years make these datasets useful for future studies concerned with the way trees change in size and shape over their life-span. the allometric relationships include (1) trunk diameter, (2) height, (3) crown height, (4) crown radius and (5) trunk radial growth rate to (a) the light environment of each tree and (b) diameter at breast height."
"we show that sets of conformal data on closed manifolds with the metric in the positive or zero yamabe class, and with the gradient of the mean curvature function sufficiently small, are mapped to solutions of the einstein constraint equations. this result extends previous work which required the conformal metric to be in the negative yamabe class, and required the mean curvature function to be nonzero."
"the presence of different transcripts of a gene across samples can be analysed by whole-transcriptome microarrays. reproducing results from published microarray data represents a challenge due to the vast amounts of data and the large variety of pre-processing and filtering steps employed before the actual analysis is carried out. to guarantee a firm basis for methodological development where results with new methods are compared with previous results it is crucial to ensure that all analyses are completely reproducible for other researchers. we here give a detailed workflow on how to perform reproducible analysis of the genechip human exon 1.0 st array at probe and probeset level solely in r/bioconductor, choosing packages based on their simplicity of use. to exemplify the use of the proposed workflow we analyse differential splicing and differential gene expression in a publicly available dataset using various statistical methods. we believe this study will provide other researchers with an easy way of accessing gene expression data at different annotation levels and with the sufficient details needed for developing their own tools for reproducible analysis of the genechip human exon 1.0 st array."
"we propose a novel hue-preserving tone mapping scheme. various tone mapping operations have been studied so far, but there are very few works on color distortion caused in image tone mapping. first, ldr images produced from hdr ones by using conventional tone mapping operators (tmos) are pointed out to have some distortion in hue values due to clipping and rounding quantization processing. next,we propose a novel method which allows ldr images to have the same maximally saturated color values as those of hdr ones. generated ldr images by the proposed method have smaller hue degradation than ldr ones generated by conventional tmos. moreover, the proposed method is applicable to any tmos. in an experiment, the proposed method is demonstrated not only to produce images with small hue degradation but also to maintain well-mapped luminance, in terms of three objective metrics: tmqi, hue value in ciede2000, and the maximally saturated color on the constant-hue plane in the rgb color space."
"we show that a certain superfield formalism can be used to find an off-shell supersymmetric description for some supersymmetric field theories where conventional superfield formalism does not work. this ""new"" formalism contains even auxiliary variables in addition to conventional odd super-coordinates. the idea of this construction is similar to the pure spinor formalism developed by n.berkovits. it is demonstrated that using this formalism it is possible to prove that the certain chern-simons-like (witten's osft-like) theory can be considered as an off-shell version for some on-shell supersymmetric field theories. we use the simplest non-trivial model found in [2] to illustrate the power of this pure spinor superfield formalism. then we redo all the calculations for the case of 10-dimensional super-yang-mills theory. the construction of off-shell description for this theory is more subtle in comparison with the model of [2] and requires additional z_2 projection. we discover experimentally (through a direct explicit calculation) a non-trivial z_2 duality at the level of feynman diagrams. the nature of this duality requires a better investigation."
we propose in this paper a new generative model for graphs that uses a latent space approach to explain timestamped interactions. the model is designed to provide global estimates of activity dates in historical networks where only the interaction dates between agents are known with reasonable precision. experimental results show that the model provides better results than local averages in dense enough networks
"it is suggested that charged tachyons of extremely large mass m could not only contribute to the dark matter needed to fit astrophysical observations, but could also provide an explanation for gamma ray bursts and ulta high energy cosmic rays. the present paper defines a quantum field theory of tachyons, particles similar to ordinary leptons, but with momenta larger than energy. the theory is invariant under the full cpt transformation, but separately violates p and t invariance. micro causality is broken for space-time intervals smaller than 1/m, but is effectively preserved for larger separations. charged fermionic, rather than charged scalar tachyons are considered in order to minimize the probability of cerenkov-like radiation by the tachyon, thereby permitting a high energy tachyon to retain its energy over galactic distances. topics treated include the choice and schwinger action principle variations of an appropriate lagrangian, spinorial wave functions, relevant green's functions, a functional description of an s-matrix and generating functional, and a variety of interesting kinematical processes, including photon emission and reabsorption, and relevant annihilation and scattering effects. a version of ehrenfest's theorem is developed, in order to provide a foundation for a classical description of charged tachyons in an external electromagnetic field. applications are then made to three outstanding astrophysical puzzles : dark matter, gamma ray bursts and ultra high energy cosmic rays."
"inspired by the all-important conformal invariance of harmonic maps on two-dimensional domains, this article studies the relationship between biharmonicity and conformality. we first give a characterization of biharmonic morphisms, analogues of harmonic morphisms investigated by fuglede and ishihara, which, in particular, explicits the conditions required for a conformal map in dimension four to preserve biharmonicity and helps producing the first example of a biharmonic morphism which is not a special type of harmonic morphism. then, we compute the bitension field of horizontally weakly conformal maps, which include conformal mappings. this leads to several examples of proper (i.e. non-harmonic) biharmonic conformal maps, in which dimension four plays a pivotal role. we also construct a family of riemannian submersions which are proper biharmonic maps."
"the bridge regression estimator generalizes both ridge regression and lasso estimators. since it minimizes the sum of squared residuals with a $l_{\gamma }$ penalty, this estimator is typically not robust against outliers in the data. there have been attempts to define robust versions of the bridge regression method, but while these proposed methods produce bridge regression estimators robust to outliers and heavy-tailed errors, they are not robust against leverage points. we propose a robust bridge regression estimation method combining mm and bridge regression estimation methods. the mm bridge regression estimator obtained from the proposed method is robust against outliers and leverage points. furthermore, for appropriate choices of the penalty function, the proposed method is able to perform variable selection and parameter estimation simultaneously. consistency, asymptotic normality, and sparsity of the mm bridge regression estimator are achieved. we propose an algorithm to compute the mm bridge regression estimate. a simulation study and a real data example are provided to demonstrate the performance of the mm bridge regression estimator for finite sample cases."
"standard decoding approaches rely on model-based channel estimation methods to compensate for varying channel effects, which degrade in performance whenever there is a model mismatch. recently proposed deep learning based neural decoders address this problem by leveraging a model-free approach via gradient-based training. however, they require large amounts of data to retrain to achieve the desired adaptivity, which becomes intractable in practical systems.   in this paper, we propose a new decoder: model independent neural decoder (mind), which builds on the top of neural decoders and equips them with a fast adaptation capability to varying channels. this feature is achieved via the methodology of model-agnostic meta-learning (maml). here the decoder: (a) learns a ""good"" parameter initialization in the meta-training stage where the model is exposed to a set of archetypal channels and (b) updates the parameter with respect to the observed channel in the meta-testing phase using minimal adaptation data and pilot bits. building on top of existing state-of-the-art neural convolutional and turbo decoders, mind outperforms the static benchmarks by a large margin and shows minimal performance gap when compared to the neural (convolutional or turbo) decoders designed for that particular channel. in addition, mind also shows strong learning capability for channels not exposed during the meta training phase."
"trusted identification is critical to secure iot devices. however, the limited memory and computation power of low-end iot devices prevent the direct usage of conventional identification systems. rf fingerprinting is a promising technique to identify low-end iot devices since it only requires the rf signals that most iot devices can produce for communication. however, most existing rf fingerprinting systems are data-dependent and/or not robust to impacts from wireless channels. to address the above problems, we propose to exploit the mathematical expression of the physical-layer process, regarded as a function $\mathbf{\mathcal{f}(\cdot)}$, for device identification. $\mathbf{\mathcal{f}(\cdot)}$ is not directly derivable, so we further propose a model to learn it and employ this function model as the device fingerprint in our system, namely $\mathcal{f}$id. our proposed function model characterizes the unique physical-layer process of a device that is independent of the transmitted data, and hence, our system $\mathcal{f}$id is data-independent and thus resilient against signal replay attacks. modeling and further separating channel effects from the function model makes $\mathcal{f}$id channel-robust. we evaluate $\mathcal{f}$id on thousands of random signal packets from $33$ different devices in different environments and scenarios, and the overall identification accuracy is over $99\%$."
"we consider the problem of estimating the structural function in nonparametric instrumental regression, where in the presence of an instrument w a response y is modeled in dependence of an endogenous explanatory variable z. the proposed estimator is based on dimension reduction and additional thresholding. the minimax optimal rate of convergence of the estimator is derived assuming that the structural function belongs to some ellipsoids which are in a certain sense linked to the conditional expectation operator of z given w. we illustrate these results by considering classical smoothness assumptions. however, the proposed estimator requires an optimal choice of a dimension parameter depending on certain characteristics of the unknown structural function and the conditional expectation operator of z given w, which are not known in practice. the main issue addressed in our work is an adaptive choice of this dimension parameter using a model selection approach under the restriction that the conditional expectation operator of z given w is smoothing in a certain sense. in this situation we develop a penalized minimum contrast estimator with randomized penalty and collection of models. we show that this data-driven estimator can attain the lower risk bound up to a constant over a wide range of smoothness classes for the structural function."
"we perform a noether analysis for a description of translation transformations in 4d $\kappa$-minkowski noncommutative spacetime which is based on the structure of a 5d differential calculus. the techniques that some of us had previously developed (hep-th/0607221) for a description of translation transformations based on a 4d differential calculus turn out to be applicable without any modification, and they allow us to show that the basis usually adopted for the 5d calculus does not take into account certain aspects of the structure of time translations in $\kappa$-minkowski. we propose a change of basis for the 5d calculus which leads to a more intuitive description of time translations."
"we introduce a covariate-specific total variation penalty in two semiparametric models for the rate function of recurrent event process. the two models are a stratified cox model, introduced in prentice et al. (1981), and a stratified aalen's additive model. we show the consistency and asymptotic normality of our penalized estimators. we demonstrate, through a simulation study, that our estimators outperform classical estimators for small to moderate sample sizes. finally an application to the bladder tumour data of byar (1980) is presented."
"i develop a model of a randomized experiment with a binary intervention and a binary outcome. potential outcomes in the intervention and control groups give rise to four types of participants. fixing ideas such that the outcome is mortality, some participants would live regardless, others would be saved, others would be killed, and others would die regardless. these potential outcome types are not observable. however, i use the model to develop estimators of the number of participants of each type. the model relies on the randomization within the experiment and on deductive reasoning. i apply the model to an important clinical trial, the prowess trial, and i perform a monte carlo simulation calibrated to estimates from the trial. the reduced form from the trial shows a reduction in mortality, which provided a rationale for fda approval. however, i find that the intervention killed two participants for every three it saved."
"recently, granville and soundararajan have made fundamental breakthroughs in the study of character sums. building on their work and using estimates on short character sums developed by graham-ringrose and iwaniec, we improve the polya-vinogradov inequality for characters with smooth conductor."
"the traditional node percolation map of directed networks is reanalyzed in terms of edges. in the percolated phase, edges can mainly organize into five distinct giant connected components, interfaces bridging the communication of nodes in the strongly connected component and those in the in- and out-components. formal equations for the relative sizes in number of edges of these giant structures are derived for arbitrary joint degree distributions in the presence of local and two-point correlations. the uncorrelated null model is fully solved analytically and compared against simulations, finding an excellent agreement between the theoretical predictions and the edge percolation map of synthetically generated networks with exponential or scale-free in-degree distribution and exponential out-degree distribution. interfaces, and their internal organization giving place from ""hairy ball"" percolation landscapes to bottleneck straits, could bring new light to the discussion of how structure is interwoven with functionality, in particular in flow networks."
"a transmitter without channel state information (csi) wishes to send a delay-limited gaussian source over a slowly fading channel. the source is coded in superimposed layers, with each layer successively refining the description in the previous one. the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. the expected distortion is minimized by optimally allocating the transmit power among the source layers. for two source layers, the allocation is optimal when power is first assigned to the higher layer up to a power ceiling that depends only on the channel fading distribution; all remaining power, if any, is allocated to the lower layer. for convex distortion cost functions with convex constraints, the minimization is formulated as a convex optimization problem. in the limit of a continuum of infinite layers, the minimum expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the one that maximizes expected capacity. while expected distortion can be improved by acquiring csi at the transmitter (csit) or by increasing diversity from the realization of independent fading paths, at high snr the performance benefit from diversity exceeds that from csit, especially when b is large."
"approximate bayesian computation (abc) has gained popularity in recent years owing to its easy implementation, nice interpretation and good performance. its advantages are more visible when one encounters complex models where maximum likelihood estimation as well as bayesian analysis via markov chain monte carlo demand prohibitively large amount of time. this paper examines properties of abc both from a theoretical as well as from a computational point of view.we consolidate the abc theory by proving theorems related to its limiting behaviour. in particular, we consider partial posteriors, which serve as the first step towards approximating the full posteriors. also, a new semi-automatic algorithm of abc is proposed using sufficient dimension reduction (sdr) method. sdr has primarily surfaced in the frequentist literature. but we have demonstrated in this paper that it has connections with abc as well."
"despite intensive research, the mechanisms underlying how neurons encode external inputs remain poorly understood. recent work has focused on the response of a single neuron to a weak, subthreshold periodic signal. by simulating the fitzhugh-nagumo stochastic model and then using a symbolic method to analyze the firing activity of the neuron, preferred and infrequent spike patterns (defined by the relative timing of the spikes) were detected, whose probabilities encode information about the signal. as not individual neurons in isolation but neuronal populations are responsible for the emergence of complex behaviors, a relevant question is whether this coding mechanism is robust when the neuron is not isolated. we study how a second neuron, which does not perceive the subthreshold signal, affects the detection and the encoding of the signal, done by the first neuron. through simulations of two coupled fitzhugh-nagumo neurons we show that the coding mechanism is indeed robust, as the neuron that perceives the signal fires a spike train that has symbolic patterns whose probabilities depend on the features of the signal. moreover, we show that the second neuron facilitates the detection of the signal, by lowering the firing threshold of the first neuron. this in turn decreases the internal noise level need to fire the spikes that encode the signal. we also show that the probabilities of the symbolic patterns achieve maximum or minimum values when the period of the external signal is close to (or is half of) the mean firing period of the neuron."
"the idea of measuring distance between languages seems to have its roots in the work of the french explorer dumont d'urville (d'urville 1832). he collected comparative words lists of various languages during his voyages aboard the astrolabe from 1826 to1829 and, in his work about the geographical division of the pacific, he proposed a method to measure the degree of relation among languages. the method used by modern glottochronology, developed by morris swadesh in the 1950s (swadesh 1952), measures distances from the percentage of shared cognates, which are words with a common historical origin. recently, we proposed a new automated method which uses normalized levenshtein distance among words with the same meaning and averages on the words contained in a list. another classical problem in glottochronology is the study of the stability of words corresponding to different meanings. words, in fact, evolve because of lexical changes, borrowings and replacement at a rate which is not the same for all of them. the speed of lexical evolution is different for different meanings and it is probably related to the frequency of use of the associated words (pagel et al. 2007). this problem is tackled here by an automated methodology only based on normalized levenshtein distance."
"this paper describes how to convert a machine learning problem into a series of map-reduce tasks. we study logistic regression algorithm. in logistic regression algorithm, it is assumed that samples are independent and each sample is assigned a probability. parameters are obtained by maxmizing the product of all sample probabilities. rapid expansion of training samples brings challenges to machine learning method. training samples are so many that they can be only stored in distributed file system and driven by map-reduce style programs. the main step of logistic regression is inference. according to map-reduce spirit, each sample makes inference through a separate map procedure. but the premise of inference is that the map procedure holds parameters for all features in the sample. in this paper, we propose distributed parameter map-reduce, in which not only samples, but also parameters are distributed in nodes of distributed filesystem. through a series of map-reduce tasks, we assign each sample parameters for its features, make inference for the sample and update paramters of the model. the above processes are excuted looply until convergence. we test the proposed algorithm in actual hadoop production environment. experiments show that the acceleration of the algorithm is in linear relationship with the number of cluster nodes."
"data holders can produce synthetic versions of datasets when concerns about potential disclosure restrict the availability of the original records. this paper is concerned with methods to judge whether such synthetic data have a distribution that is comparable to that of the original data, what we will term general utility. we consider how general utility compares with specific utility, the similarity of results of analyses from the synthetic data and the original data. we adapt a previous general measure of data utility, the propensity score mean-squared-error (pmse), to the specific case of synthetic data and derive its distribution for the case when the correct synthesis model is used to create the synthetic data. our asymptotic results are confirmed by a simulation study. we also consider two specific utility measures, confidence interval overlap and standardized difference in summary statistics, which we compare with the general utility results. we present two examples examining this comparison of general and specific utility to real data syntheses and make recommendations for their use for evaluating synthetic data."
"simple boundary expressions for the k-th power of the cotangent line class on the moduli space of stable 1-pointed genus g curves are found for k >= 2g. the method is by virtual localization on the moduli space of maps to the projective line. as a consequence, nontrivial tautological classes in the kernel of the push-forward map associated to the irreducible boundary divisor of the moduli space of stable g+1 curves are constructed. the geometry of genus g+1 curves then provides universal equations in genus g gromov-witten theory. as an application, we prove all the gromov-witten identities conjectured recently by k. liu and h. xu."
this paper provides extensions of the work on subsampling by bertail et al. (2004) for strongly mixing case to weakly dependent case by application of the results of doukhan and louhichi (1999). we investigate properties of smooth and rough subsampling estimators for distributions of converging and extreme statistics when the underlying time series is {\eta} or {\lambda}-weakly dependent.
"for the quintom models with arbitrary potential $v=v(\phi,\sigma)$, the asymptotic value of equation of state parameter w is obtained by a new method. in this method, w of stable attractors are calculated by using the ratio (d ln v)/(d ln a) in asymptotic region. all the known results, have been obtained by other methods, are reproduced by this method as specific examples."
"negative serial correlations in single spike trains are an effective method to reduce the variability of spike counts. one of the factors contributing to the development of negative correlations between successive interspike intervals is the presence of adaptation currents. in this work, based on a hidden markov model and a proper statistical description of conditional responses, we obtain analytically these correlations in an adequate dynamical neuron model resembling adaptation. we derive the serial correlation coefficients for arbitrary lags, under a small adaptation scenario. in this case, the behavior of correlations is universal and depends on the first-order statistical description of an exponentially driven time-inhomogeneous stochastic process."
"the brain can reproduce memories from partial data; this ability is critical for memory recall. the process of memory recall has been studied using auto-associative networks such as the hopfield model. this kind of model reliably converges to stored patterns which contain the memory. however, it is unclear how the behavior is controlled by the brain so that after convergence to one configuration, it can proceed with recognition of another one. in the hopfield model this happens only through unrealistic changes of an effective global temperature that destabilizes all stored configurations. here we show that spike frequency adaptation (sfa), a common mechanism affecting neuron activation in the brain, can provide state dependent control of pattern retrieval. we demonstrate this in a hopfield network modified to include sfa, and also in a model network of biophysical neurons. in both cases sfa allows for selective stabilization of attractors with different basins of attraction, and also for temporal dynamics of attractor switching that is not possible in standard auto-associative schemes. the dynamics of our models give a plausible account of different sorts of memory retrieval."
"we study geometries that arise from the natural $g_2(k)$ action on the geometry of one-dimensional subspaces, of nonsingular two-dimensional subspaces, and of nonsingular three-dimensional subspaces of the building geometry of type $c_3(k)$ where $k$ is a perfect field of characteristic 2. one of these geometries is intransitive in such a way that the non-standard geometric covering theory by the first and the last author is not applicable. in this paper we introduce the concept of fused amalgams in order to extend the geometric covering theory so that it applies to that geometry. this yields an interesting new amalgamation result for the group $g_2(k)$."
"a toolbox for creation and rendering of dynamic virtual acoustic environments (tascar) that allows direct user interaction was developed for application in hearing aid research and audiology. this technical paper describes the general software structure and the time-domain simulation methods, i.e., transmission model, image source model, and render formats, used to produce virtual acoustic environments with moving objects. implementation-specific properties are described, and the computational performance of the system was measured as a function of simulation complexity. results show that on commercially available commonly used hardware the simulation of several hundred virtual sound sources is possible in the time domain."
"chemotaxis is a ubiquitous biological phenomenon in which cells detect a spatial gradient of chemoattractant, and then move towards the source. here we present a position-dependent advection-diffusion model that quantitatively describes the statistical features of the chemotactic motion of the social amoeba {\it dictyostelium discoideum} in a linear gradient of camp (cyclic adenosine monophosphate). we fit the model to experimental trajectories that are recorded in a microfluidic setup with stationary camp gradients and extract the diffusion and drift coefficients in the gradient direction. our analysis shows that for the majority of gradients, both coefficients decrease in time and become negative as the cells crawl up the gradient. the extracted model parameters also show that besides the expected drift in the direction of chemoattractant gradient, we observe a nonlinear dependency of the corresponding variance in time, which can be explained by the model. furthermore, the results of the model show that the non-linear term in the mean squared displacement of the cell trajectories can dominate the linear term on large time scales."
"information geometry provides a geometric approach to families of statistical models. the key geometric structures are the fisher quadratic form and the amari-chentsov tensor. in statistics, the notion of sufficient statistic expresses the criterion for passing from one model to another without loss of information. this leads to the question how the geometric structures behave under such sufficient statistics. while this is well studied in the finite sample size case, in the infinite case, we encounter technical problems concerning the appropriate topologies. here, we introduce notions of parametrized measure models and tensor fields on them that exhibit the right behavior under statistical transformations. within this framework, we can then handle the topological issues and show that the fisher metric and the amari-chentsov tensor on statistical models in the class of symmetric 2-tensor fields and 3-tensor fields can be uniquely (up to a constant) characterized by their invariance under sufficient statistics, thereby achieving a full generalization of the original result of chentsov to infinite sample sizes. more generally, we decompose markov morphisms between statistical models in terms of statistics. in particular, a monotonicity result for the fisher information naturally follows."
"in this paper we study complete linear series on a hyperelliptic curve $c$ of arithmetic genus $g$. let $a$ be the unique line bundle on $c$ such that $|a|$ is a $g^1_2$, and let $\mathcal{l}$ be a line bundle on $c$ of degree $d$. then $\mathcal{l}$ can be factorized as $\mathcal{l} = a^m \otimes b$ where $m$ is the largest integer satisfying $h^0 (c,\mathcal{l} \otimes a^{-m}) \neq 0$. let $b = {deg}(b)$. we say that \textit{the factorization type of} $\mathcal{l}$ is $(m,b)$. our main results in this paper assert that $(m,b)$ gives a precise answer for many natural questions about $\mathcal{l}$."
"in this paper we investigate the problem of locating multiple non-cooperative radio frequency (rf) emitters using only received signal strength (rss) data. we assume that the number of emitters is unknown and that individual emitters cannot be distinguished in the rss data. moreover, we assume that the environment in which the data has been collected has not been mapped or ""fingerprinted"" by the prior collection of rss data. our primary interest is the limiting resolution that can be obtained by this type of data, and the lowest power emitters that can be detected, as a function of noise level, sensor geometry, and other variables. we formulate the recovery problem as one of sparse approximation or compressed sensing, and investigate an appropriate recovery algorithm for this setting, and use it to illustrate our conclusions. we also include a reconstruction based on sampled data we collected, to illustrate the reasonableness of our parameter choices and conclusions."
"the kalman filter (kf) and the extended kalman filter (ekf) are well established techniques for state estimation. however, the choice of the filter tuning parameters still poses a major challenge for the engineers [1]. in the present work, two new costs have been proposed for determining the filter tuning parameters on the basis of the innovation covariance. this provides a cost function based method for the selection of suitable combination(s) of filter tuning parameters in order to ensure the design of a kf or an ekf having an optimally balanced rmse performance. index terms-kalman filter, tuning parameters, innovation covariance, cost function"
"we present a short way of proving the inequalities obtained by wright in [journal of graph theory, 4: 393 - 407 (1980)] concerning the number of connected graphs with $\ell$ edges more than vertices."
"we find the local rate of convergence of the least squares estimator (lse) of a one dimensional convex regression function when (a) a certain number of derivatives vanish at the point of interest, and (b) the true regression function is locally affine. in each case we derive the limiting distribution of the lse and its derivative. the pointwise limiting distributions depend on the second and third derivatives at 0 of the ""invelope function"" of the integral of a two-sided brownian motion with polynomial drifts. we also investigate the inconsistency of the lse and the unboundedness of its derivative at the boundary of the domain of the covariate space. an estimator of the argmin of the convex regression function is proposed and its asymptotic distribution is derived. further, we present some new results on the characterization of the convex lse that may be of independent interest."
"feature extraction and dimensionality reduction are important tasks in many fields of science dealing with signal processing and analysis. the relevance of these techniques is increasing as current sensory devices are developed with ever higher resolution, and problems involving multimodal data sources become more common. a plethora of feature extraction methods are available in the literature collectively grouped under the field of multivariate analysis (mva). this paper provides a uniform treatment of several methods: principal component analysis (pca), partial least squares (pls), canonical correlation analysis (cca) and orthonormalized pls (opls), as well as their non-linear extensions derived by means of the theory of reproducing kernel hilbert spaces. we also review their connections to other methods for classification and statistical dependence estimation, and introduce some recent developments to deal with the extreme cases of large-scale and low-sized problems. to illustrate the wide applicability of these methods in both classification and regression problems, we analyze their performance in a benchmark of publicly available data sets, and pay special attention to specific real applications involving audio processing for music genre prediction and hyperspectral satellite images for earth and climate monitoring."
"we show that the number of perfect matching in a simple graph $g$ with an even number of vertices and degree sequence $d_1,d_2, ..., d_n$ is at most $\prod_{i=1}^n (d_i !)^{\frac{1}{2d_i}}$. this bound is sharp if and only if $g$ is a union of complete balanced bipartite graphs."
"we propose an efficient nonparametric strategy for learning a message operator in expectation propagation (ep), which takes as input the set of incoming messages to a factor node, and produces an outgoing message as output. this learned operator replaces the multivariate integral required in classical ep, which may not have an analytic expression. we use kernel-based regression, which is trained on a set of probability distributions representing the incoming messages, and the associated outgoing messages. the kernel approach has two main advantages: first, it is fast, as it is implemented using a novel two-layer random feature representation of the input message distributions; second, it has principled uncertainty estimates, and can be cheaply updated online, meaning it can request and incorporate new training data when it encounters inputs on which it is uncertain. in experiments, our approach is able to solve learning problems where a single message operator is required for multiple, substantially different data sets (logistic regression for a variety of classification problems), where it is essential to accurately assess uncertainty and to efficiently and robustly update the message operator."
"the ongoing rapid development of the e-commercial and interest-base websites make it more pressing to evaluate objects' accurate quality before recommendation by employing an effective reputation system. the objects' quality are often calculated based on their historical information, such as selected records or rating scores, to help visitors to make decisions before watching, reading or buying. usually high quality products obtain a higher average ratings than low quality products regardless of rating biases or errors. however many empirical cases demonstrate that consumers may be misled by rating scores added by unreliable users or deliberate tampering. in this case, users' reputation, i.e., the ability to rating trustily and precisely, make a big difference during the evaluating process. thus, one of the main challenges in designing reputation systems is eliminating the effects of users' rating bias on the evaluation results. to give an objective evaluation of each user's reputation and uncover an object's intrinsic quality, we propose an iterative balance (ib) method to correct users' rating biases. experiments on two online video-provided web sites, namely movielens and netflix datasets, show that the ib method is a highly self-consistent and robust algorithm and it can accurately quantify movies' actual quality and users' stability of rating. compared with existing methods, the ib method has higher ability to find the ""dark horses"", i.e., not so popular yet good movies, in the academy awards."
"cryo-electron microscopy provides 2-d projection images of the 3-d electron scattering intensity of many instances of the particle under study (e.g., a virus). both symmetry (rotational point groups) and heterogeneity are important aspects of biological particles and both aspects can be combined by describing the electron scattering intensity of the particle as a stochastic process with a symmetric probability law and therefore symmetric moments. a maximum likelihood estimator implemented by an expectation-maximization algorithm is described which estimates the unknown statistics of the electron scattering intensity stochastic process from images of instances of the particle. the algorithm is demonstrated on the bacteriophage hk97 and the virus n$\omega$v. the results are contrasted with existing algorithms which assume that each instance of the particle has the symmetry rather than the less restrictive assumption that the probability law has the symmetry."
"we present an example of a highly connected closed network of servers, where the time correlations do not go to zero in the infinite volume limit. this phenomenon is similar to the continuous symmetry breaking at low temperatures in statistical mechanics. the role of the inverse temperature is played by the average load."
"the high dissipation of integrated circuits means serious problems for packaging and for the design of complex electronic systems. another important area of research and development nowadays is the integration of sensors and micromechanical systems (mems) with electronic circuits. the original successive node reduction (sunred) algorithm handles well the first area but require revision for electro-thermal or mechanical fields. as a first stage the updated algorithm is able to solve thermal fields as the original, but with the application of flexible boundary connection handling, it can be much faster than the original. by using object-oriented program model the algorithm can handle non-rectangular 3d fields, and sunred mesh resolution is arbitrary, not have to be the power of two anymore."
"we propose a statistical model for weighted temporal networks capable of measuring the level of heterogeneity in a financial system. our model focuses on the level of diversification of financial institutions; that is, whether they are more inclined to distribute their assets equally among partners, or if they rather concentrate their commitment towards a limited number of institutions. crucially, a markov property is introduced to capture time dependencies and to make our measures comparable across time. we apply the model on an original dataset of austrian interbank exposures. the temporal span encompasses the onset and development of the financial crisis in 2008 as well as the beginnings of european sovereign debt crisis in 2011. our analysis highlights an overall increasing trend for network homogeneity, whereby core banks have a tendency to distribute their market exposures more equally across their partners."
"given an overcomplete dictionary $a$ and a signal $b$ that is a linear combination of a few linearly independent columns of $a$, classical sparse recovery theory deals with the problem of recovering the unique sparse representation $x$ such that $b = a x$. it is known that under certain conditions on $a$, $x$ can be recovered by the basis pursuit (bp) and the orthogonal matching pursuit (omp) algorithms. in this work, we consider the more general case where $b$ lies in a low-dimensional subspace spanned by some columns of $a$, which are possibly linearly dependent. in this case, the sparsest solution $x$ is generally not unique, and we study the problem that the representation $x$ identifies the subspace, i.e. the nonzero entries of $x$ correspond to dictionary atoms that are in the subspace. such a representation $x$ is called subspace-sparse. we present sufficient conditions for guaranteeing subspace-sparse recovery, which have clear geometric interpretations and explain properties of subspace-sparse recovery. we also show that the sufficient conditions can be satisfied under a randomized model. our results are applicable to the traditional sparse recovery problem and we get conditions for sparse recovery that are less restrictive than the canonical mutual coherent condition. we also use the results to analyze the sparse representation based classification (src) method, for which we get conditions to show its correctness."
"microbiome-based stratification of healthy individuals into compositional categories, referred to as ""community types"", holds promise for drastically improving personalized medicine. despite this potential, the existence of community types and the degree of their distinctness have been highly debated. here we adopted a dynamic systems approach and found that heterogeneity in the interspecific interactions or the presence of strongly interacting species is sufficient to explain community types, independent of the topology of the underlying ecological network. by controlling the presence or absence of these strongly interacting species we can steer the microbial ecosystem to any desired community type. this open-loop control strategy still holds even when the community types are not distinct but appear as dense regions within a continuous gradient. this finding can be used to develop viable therapeutic strategies for shifting the microbial composition to a healthy configuration"
"we address the rectangular matrix completion problem by lifting the unknown matrix to a positive semidefinite matrix in higher dimension, and optimizing a nonconvex objective over the semidefinite factor using a simple gradient descent scheme. with $o( \mu r^2 \kappa^2 n \max(\mu, \log n))$ random observations of a $n_1 \times n_2$ $\mu$-incoherent matrix of rank $r$ and condition number $\kappa$, where $n = \max(n_1, n_2)$, the algorithm linearly converges to the global optimum with high probability."
"this paper studies a two-person trading game in continuous time that generalizes garivaltis (2018) to allow for stock prices that both jump and diffuse. analogous to bell and cover (1988) in discrete time, the players start by choosing fair randomizations of the initial dollar, by exchanging it for a random wealth whose mean is at most 1. each player then deposits the resulting capital into some continuously-rebalanced portfolio that must be adhered to over $[0,t]$. we solve the corresponding `investment $\phi$-game,' namely the zero-sum game with payoff kernel $\mathbb{e}[\phi\{\textbf{w}_1v_t(b)/(\textbf{w}_2v_t(c))\}]$, where $\textbf{w}_i$ is player $i$'s fair randomization, $v_t(b)$ is the final wealth that accrues to a one dollar deposit into the rebalancing rule $b$, and $\phi(\bullet)$ is any increasing function meant to measure relative performance. we show that the unique saddle point is for both players to use the (leveraged) kelly rule for jump diffusions, which is ordinarily defined by maximizing the asymptotic almost-sure continuously-compounded capital growth rate. thus, the kelly rule for jump diffusions is the correct behavior for practically anybody who wants to outperform other traders (on any time frame) with respect to practically any measure of relative performance."
"vanishing long-term gradients are a major issue in training standard recurrent neural networks (rnns), which can be alleviated by long short-term memory (lstm) models with memory cells. however, the extra parameters associated with the memory cells mean an lstm layer has four times as many parameters as an rnn with the same hidden vector size. this paper addresses the vanishing gradient problem using a high order rnn (hornn) which has additional connections from multiple previous time steps. speech recognition experiments using british english multi-genre broadcast (mgb3) data showed that the proposed hornn architectures for rectified linear unit and sigmoid activation functions reduced word error rates (wer) by 4.2% and 6.3% over the corresponding rnns, and gave similar wers to a (projected) lstm while using only 20%--50% of the recurrent layer parameters and computation."
"the quotient correlation is defined here as an alternative to pearson's correlation that is more intuitive and flexible in cases where the tail behavior of data is important. it measures nonlinear dependence where the regular correlation coefficient is generally not applicable. one of its most useful features is a test statistic that has high power when testing nonlinear dependence in cases where the fisher's $z$-transformation test may fail to reach a right conclusion. unlike most asymptotic test statistics, which are either normal or $\chi^2$, this test statistic has a limiting gamma distribution (henceforth, the gamma test statistic). more than the common usages of correlation, the quotient correlation can easily and intuitively be adjusted to values at tails. this adjustment generates two new concepts--the tail quotient correlation and the tail independence test statistics, which are also gamma statistics. due to the fact that there is no analogue of the correlation coefficient in extreme value theory, and there does not exist an efficient tail independence test statistic, these two new concepts may open up a new field of study. in addition, an alternative to spearman's rank correlation, a rank based quotient correlation, is also defined. the advantages of using these new concepts are illustrated with simulated data and a real data analysis of internet traffic."
"we describe an occupation-number-like picture of fractional quantum hall (fqh) states in terms of polynomial wavefunctions characterized by a dominant occupation-number configuration. the bosonic variants of single-component abelian and non-abelian fqh states are modeled by jacks (jack symmetric polynomials), characterized by dominant occupation-number configurations satisfying a generalized pauli principle. in a series of well-known quantum hall states, including the laughlin, read-moore, and read-rezayi, the jack polynomials naturally implement a ``squeezing rule'' that constrains allowed configurations to be restricted to those obtained by squeezing the dominant configuration. the jacks describing uniform fqh states satisfy a highest-weight condition, and a clustering condition which can be generalized to describe quasiparticle states."
including vaidya metric into the model of expansive nondecelerative universe allows to localize the energy of gravitational field. a term of effective gravitational range is introduced and classic newton potential is substituted for yukawa-type potential. it allows to allocate a typical frequency value to each gravitational field. derived theoretical conclusions led us to investigate the effect of electromagnetic field with a precisely predetermined frequency and intensity on iron. we believe that under certain circumstances a decrease in iron gravitational mass should be observed. two model experiments verifying the theoretical conclusions are proposed.
"let $g$ be a locally lipschitz continuous real valued function which satisfies the keller-osserman condition and is convex at infinity, then any large solution of $-\delta u+g(u)=0$ in a ball is radially symmetric."
"let $\mathcal{p}$ be a proper smooth formal $\mathcal{v}$-scheme, $x$ a closed subscheme of the special fiber of $\mathcal{p}$, $\mathcal{e} \in f\text{-}d ^\mathrm{b}_\mathrm{coh} (\d ^\dag_{\mathcal{p},\mathbb{q}})$ with support in $x$. we check that $\mathcal{e}$ is $\d ^\dag _{\mathcal{p},\mathbb{q}}$-overcoherent if and only if, for any morphism $f : \mathcal{p}' \to \mathcal{p}$ of smooth formal $\mathcal{v}$-schemes, $f ^! (\mathcal{e}) $ is $\d ^\dag_{\mathcal{p}', \mathbb{q}}$-coherent."
"we consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. we focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of o(1/n^{1/2}). we consider and analyze two algorithms that achieve a rate of o(1/n) for classical supervised learning problems. for least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. for logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. for these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments on standard machine learning benchmarks showing that they often outperform existing approaches."
"we use convex relaxation techniques to provide a sequence of solutions to the matrix completion problem. using the nuclear norm as a regularizer, we provide simple and very efficient algorithms for minimizing the reconstruction error subject to a bound on the nuclear norm. our algorithm iteratively replaces the missing elements with those obtained from a thresholded svd. with warm starts this allows us to efficiently compute an entire regularization path of solutions."
"higher-order data with high dimensionality arise in a diverse set of application areas such as computer vision, video analytics and medical imaging. tensors provide a natural tool for representing these types of data. although there has been a lot of work in the area of tensor decomposition and low-rank tensor approximation, extensions to supervised learning, feature extraction and classification are still limited. moreover, most of the existing supervised tensor learning approaches are based on the orthogonal tucker model. however, this model has some limitations for large tensors including high memory and computational costs. in this paper, we introduce a supervised learning approach for tensor classification based on the tensor-train model. in particular, we introduce a multi-branch tensor network structure for efficient implementation of tensor-train discriminant analysis (ttda). the proposed approach takes advantage of the flexibility of the tensor train structure to implement various computationally efficient versions of ttda. this approach is then evaluated on image and video classification tasks with respect to computation time, storage cost and classification accuracy and is compared to both vector and tensor based discriminant analysis methods."
"estimating the unknown number of classes in a population has numerous important applications. in a poisson mixture model, the problem is reduced to estimating the odds that a class is undetected in a sample. the discontinuity of the odds prevents the existence of locally unbiased and informative estimators and restricts confidence intervals to be one-sided. confidence intervals for the number of classes are also necessarily one-sided. a sequence of lower bounds to the odds is developed and used to define pseudo maximum likelihood estimators for the number of classes."
"we have discovered an extremely broad, double-peaked h-alpha emission line in the polarized flux spectrum of ngc 2110, establishing that this well-studied seyfert 2 galaxy contains a disk-like hidden broad-line region (blr). several properties of ngc 2110 suggest that it is an obscured twin of arp 102b, the prototypical double-peaked emission-line active galactic nucleus (agn). a comparison between our data and previous spectra of ngc 2110 indicates that the double-peaked h-alpha feature is transient. the presence of a disk-like blr in ngc 2110 has important implications for agns: it expands the range of properties exhibited by seyfert 2 galaxies, and the fact that the blr is obscured by a torus-like structure provides the first evidence that double-peaked emitters and classical seyfert nuclei may have the same basic parsec-scale geometry."
"we compare the robustness of humans and current convolutional deep neural networks (dnns) on object recognition under twelve different types of image degradations. first, using three well known dnns (resnet-152, vgg-19, googlenet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and dnns when the signal gets weaker. secondly, we show that dnns trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. for example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. our new dataset consisting of 83k carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system."
"this paper studies cox's regression hazard model with an unobservable random frailty where no specific distribution is postulated for the frailty variable, and the marginal lifetime distribution allows both parametric and non-parametric models. laplace's approximation method and gradient search on smooth manifolds embedded in euclidean space are applied, and a non-iterative profile likelihood optimization method is proposed for estimating the regression coefficients. the proposed method is compared with the expected-maximization method developed based on a gamma frailty assumption, and also in the case when the frailty model is misspecified."
"gross tumor volume (gtv) segmentation is a critical step in esophageal cancer radiotherapy treatment planning. inconsistencies across oncologists and prohibitive labor costs motivate automated approaches for this task. however, leading approaches are only applied to radiotherapy computed tomography (rtct) images taken prior to treatment. this limits the performance as rtct suffers from low contrast between the esophagus, tumor, and surrounding tissues. in this paper, we aim to exploit both rtct and positron emission tomography (pet) imaging modalities to facilitate more accurate gtv segmentation. by utilizing pet, we emulate medical professionals who frequently delineate gtv boundaries through observation of the rtct images obtained after prescribing radiotherapy and pet/ct images acquired earlier for cancer staging. to take advantage of both modalities, we present a two-stream chained segmentation approach that effectively fuses the ct and pet modalities via early and late 3d deep-network-based fusion. furthermore, to effect the fusion and segmentation we propose a simple yet effective progressive semantically nested network (psnn) model that outperforms more complicated models. extensive 5-fold cross-validation on 110 esophageal cancer patients, the largest analysis to date, demonstrates that both the proposed two-stream chained segmentation pipeline and the psnn model can significantly improve the quantitative performance over the previous state-of-the-art work by 11% in absolute dice score (dsc) (from 0.654 to 0.764) and, at the same time, reducing the hausdorff distance from 129 mm to 47 mm."
the evans lemma is basic for myron w. evans' gcuft or ece theory. evans has given two proofs of his lemma. both proofs are shown here to be in error and beyond repair.
"analysis of repair systems usually uses an as good as new or as bad as old repair assumptions. in practice, repair actions do not result in such extreme situations, but rather in a complex transitional one, that is imperfect maintenance, i.e. generalized renewal process. maximum likelihood estimation method is often used for reliability parameter estimation, but is it correct to use it for generalized renewal process"
"this paper considers sparse linear discriminant analysis of high-dimensional data. in contrast to the existing methods which are based on separate estimation of the precision matrix $\o$ and the difference $\de$ of the mean vectors, we introduce a simple and effective classifier by estimating the product $\o\de$ directly through constrained $\ell_1$ minimization. the estimator can be implemented efficiently using linear programming and the resulting classifier is called the linear programming discriminant (lpd) rule.   the lpd rule is shown to have desirable theoretical and numerical properties. it exploits the approximate sparsity of $\o\de$ and as a consequence allows cases where it can still perform well even when $\o$ and/or $\de$ cannot be estimated consistently. asymptotic properties of the lpd rule are investigated and consistency and rate of convergence results are given. the lpd classifier has superior finite sample performance and significant computational advantages over the existing methods that require separate estimation of $\o$ and $\de$. the lpd rule is also applied to analyze real datasets from lung cancer and leukemia studies. the classifier performs favorably in comparison to existing methods."
"the use of reinforcement learning in real-world scenarios is strongly limited by issues of scale. most rl learning algorithms are unable to deal with problems composed of hundreds or sometimes even dozens of possible actions, and therefore cannot be applied to many real-world problems. we consider the rl problem in the supervised classification framework where the optimal policy is obtained through a multiclass classifier, the set of classes being the set of actions of the problem. we introduce error-correcting output codes (ecocs) in this setting and propose two new methods for reducing complexity when using rollouts-based approaches. the first method consists in using an ecoc-based classifier as the multiclass classifier, reducing the learning complexity from o(a2) to o(alog(a)). we then propose a novel method that profits from the ecoc's coding dictionary to split the initial mdp into o(log(a)) seperate two-action mdps. this second method reduces learning complexity even further, from o(a2) to o(log(a)), thus rendering problems with large action sets tractable. we finish by experimentally demonstrating the advantages of our approach on a set of benchmark problems, both in speed and performance."
"high-dimensional inference refers to problems of statistical estimation in which the ambient dimension of the data may be comparable to or possibly even larger than the sample size. we study an instance of high-dimensional inference in which the goal is to estimate a matrix $\theta^* \in \real^{k \times p}$ on the basis of $n$ noisy observations, and the unknown matrix $\theta^*$ is assumed to be either exactly low rank, or ``near'' low-rank, meaning that it can be well-approximated by a matrix with low rank. we consider an $m$-estimator based on regularization by the trace or nuclear norm over matrices, and analyze its performance under high-dimensional scaling. we provide non-asymptotic bounds on the frobenius norm error that hold for a general class of noisy observation models, and then illustrate their consequences for a number of specific matrix models, including low-rank multivariate or multi-task regression, system identification in vector autoregressive processes, and recovery of low-rank matrices from random projections. simulation results show excellent agreement with the high-dimensional scaling of the error predicted by our theory."
"we introduce a numerical procedure to investigate the spectrum of massive modes and its contribution for gravity localization on thick branes. after considering a model with an analytically known schroedinger potential, we present the method and discuss its applicability. with this procedure we can study several models even when the schroedinger potential is not known analytically. we discuss both the occurrence of localization of gravity and the correction to the newtonian potential given by the massive modes."
"we study the problem of learning markov decision processes with finite state and action spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time. we introduce an algorithm whose regret with respect to any policy in a comparison class grows as the square root of the number of rounds of the game, provided the transition probabilities satisfy a uniform mixing condition. our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy. designing an efficient algorithm with small regret for the general case remains an open problem."
we introduce a new variational estimator for the intensity function of an inhomogeneous spatial point process with points in the $d$-dimensional euclidean space and observed within a bounded region. the variational estimator applies in a simple and general setting when the intensity function is assumed to be of log-linear form $\beta+{\theta }^{\top}z(u)$ where $z$ is a spatial covariate function and the focus is on estimating ${\theta }$. the variational estimator is very simple to implement and quicker than alternative estimation procedures. we establish its strong consistency and asymptotic normality. we also discuss its finite-sample properties in comparison with the maximum first order composite likelihood estimator when considering various inhomogeneous spatial point process models and dimensions as well as settings were $z$ is completely or only partially known.
"the edges of any hypergraph parametrize a monomial algebra called the edge subring of the hypergraph. we study presentation ideals of these edge subrings, and describe their generators in terms of balanced walks on hypergraphs. our results generalize those for the defining ideals of edge subrings of graphs, which are well-known in the commutative algebra community, and popular in the algebraic statistics community. one of the motivations for studying toric ideals of hypergraphs comes from algebraic statistics, where generators of the toric ideal give a basis for random walks on fibers of the statistical model specified by the hypergraph. further, understanding the structure of the generators gives insight into the model geometry."
we consider the problem of testing means from samples of two populations for which the labels are not defined with certainty. we show that this problem is connected to another one that is testing expected values of components of mixture-models from two data samples. the underlying mixture-model is associated with known varying mixing-weights. we provide a testing procedure that performs well. then we point out the loss of performance of our method due to the mixing-effect by comparing its numerical performances to the welch's t-test on means which would have been done if true labels were available.
"the reconstruction of phylogenies from dna or protein sequences is a major task of computational evolutionary biology. common phenomena, notably variations in mutation rates across genomes and incongruences between gene lineage histories, often make it necessary to model molecular data as originating from a mixture of phylogenies. such mixed models play an increasingly important role in practice. using concentration of measure techniques, we show that mixtures of large trees are typically identifiable. we also derive sequence-length requirements for high-probability reconstruction."
"this paper investigates the applications of various multilingual approaches developed in conventional hidden markov model (hmm) systems to sequence-to-sequence (seq2seq) automatic speech recognition (asr). on a set composed of babel data, we first show the effectiveness of multi-lingual training with stacked bottle-neck (sbn) features. then we explore various architectures and training strategies of multi-lingual seq2seq models based on ctc-attention networks including combinations of output layer, ctc and/or attention component re-training. we also investigate the effectiveness of language-transfer learning in a very low resource scenario when the target language is not included in the original multi-lingual training data. interestingly, we found multilingual features superior to multilingual models, and this finding suggests that we can efficiently combine the benefits of the hmm system with the seq2seq system through these multilingual feature techniques."
"a weaker form of the multiplicity conjecture of herzog, huneke, and srinivasan is proven for two classes of monomial ideals: quadratic monomial ideals and squarefree monomial ideals with sufficiently many variables relative to the krull dimension. it is also shown that tensor products, as well as stanley-reisner ideals of certain unions, satisfy the multiplicity conjecture if all the components do. conditions under which the bounds are achieved are also studied."
"the purpose of the current paper is to introduce some new methods for studying the $p$-adic banach spaces introduced by emerton \cite{emerton}. we first relate these spaces to more familiar sheaf cohomology groups. as an application, we obtain a more general version of emerton's spectral sequence. we also calculate the spaces in some easy cases. as a consequence, we obtain a number of vanishing theorems."
"in $\r^m\times\r^{n-m}$, endowed with coordinates $x=(x,y)$, we consider the pde $$ -{\rm div} \big(\alpha(\x) |\nabla u(\x)|^{p(x)-2}\nabla u(\x)\big)=f(x,u(\x)).$$ we prove a geometric inequality and a symmetry result."
"suppose that local characteristics of several independent compound poisson and wiener processes change suddenly and simultaneously at some unobservable disorder time. the problem is to detect the disorder time as quickly as possible after it happens and minimize the rate of false alarms at the same time. these problems arise, for example, from managing product quality in manufacturing systems and preventing the spread of infectious diseases. the promptness and accuracy of detection rules improve greatly if multiple independent information sources are available. earlier work on sequential change detection in continuous time does not provide optimal rules for situations in which several marked count data and continuously changing signals are simultaneously observable. in this paper, optimal bayesian sequential detection rules are developed for such problems when the marked count data is in the form of independent compound poisson processes, and the continuously changing signals form a multi-dimensional wiener process. an auxiliary optimal stopping problem for a jump-diffusion process is solved by transforming it first into a sequence of optimal stopping problems for a pure diffusion by means of a jump operator. this method is new and can be very useful in other applications as well, because it allows the use of the powerful optimal stopping theory for diffusions."
"for biological experiments aiming at calibrating models with unknown parameters, a good experimental design is crucial, especially for those subject to various constraints, such as financial limitations, time consumption and physical practicability. in this paper, we discuss a sequential experimental design based on information theory for parameter estimation and apply it to two biological systems. two specific issues are addressed in the proposed applications, namely the determination of the optimal sampling time and the optimal choice of observable. the optimal design, either sampling time or observable, is achieved by an information-theoretic sensitivity analysis. it is shown that this is equivalent with maximizing the mutual information and contrasted with non-adaptive designs, this information theoretic strategy provides the fastest reduction of uncertainty."
"a system of two interacting photon modes, without constraints on the photon number, in the presence of a kerr nonlinearity, exhibits bec if the transfer amplitude is greater than the mode frequency. a symmetry-breaking field (sbf) can be introduced by taking into account a classical electron current. the ground state, in the limit of small nonlinearity, becomes a squeezed state, and thus the modes become entangled. the smaller is the sbf, the greater is entanglement. superfluid-like behavior is observed in the study of entanglement growth from an initial coherent state, since in the short-time range the growth does not depend on the sbf amplitude, and on the initial state amplitude. on the other hand, the latter is the only parameter which determines entanglement in the absence of the sbf."
"how will the climate system respond to anthropogenic forcings? one approach to this question relies on climate model projections. current climate projections are considerably uncertain. characterizing and, if possible, reducing this uncertainty is an area of ongoing research. we consider the problem of making projections of the north atlantic meridional overturning circulation (amoc). uncertainties about climate model parameters play a key role in uncertainties in amoc projections. when the observational data and the climate model output are high-dimensional spatial data sets, the data are typically aggregated due to computational constraints. the effects of aggregation are unclear because statistically rigorous approaches for model parameter inference have been infeasible for high-resolution data. here we develop a flexible and computationally efficient approach using principal components and basis expansions to study the effect of spatial data aggregation on parametric and projection uncertainties. our bayesian reduced-dimensional calibration approach allows us to study the effect of complicated error structures and data-model discrepancies on our ability to learn about climate model parameters from high-dimensional data. considering high-dimensional spatial observations reduces the effect of deep uncertainty associated with prior specifications for the data-model discrepancy. also, using the unaggregated data results in sharper projections based on our climate model. our computationally efficient approach may be widely applicable to a variety of high-dimensional computer model calibration problems."
"we consider the statistical experiment of functional linear regression (flr). furthermore, we introduce a white noise model where one observes an ito process, which contains the covariance operator of the corresponding flr model in its construction. we prove asymptotic equivalence of flr and this white noise model in lecam's sense under known design distribution. moreover, we show equivalence of flr and an empirical version of the white noise model for finite sample sizes. as an application, we derive sharp minimax constants in the flr model which are still valid in the case of unknown design distribution."
"one of the most widely used samplers in practice is the component-wise metropolis-hastings (cmh) sampler that updates in turn the components of a vector valued markov chain using accept-reject moves generated from a proposal distribution. when the target distribution of a markov chain is irregularly shaped, a `good' proposal distribution for one part of the state space might be a `poor' one for another part of the state space. we consider a component-wise multiple-try metropolis (cmtm) algorithm that can automatically choose from a set of candidate moves sampled from different distributions. the computational efficiency is increased using an adaptation rule for the cmtm algorithm that dynamically builds a better set of proposal distributions as the markov chain runs. the ergodicity of the adaptive chain is demonstrated theoretically. the performance is studied via simulations and real data examples."
"in this paper, we construct a quantization functor, associating a complex vector space h(v) to a finite dimensional symplectic vector space v over a finite field of odd characteristic. as a result, we obtain a canonical model for the weil representation of the symplectic group sp(v). the main new technical result is a proof of a stronger form of the stone-von neumann property for the heisenberg group. our result answers, for the case of the heisenberg group, a question of kazhdan about the possible existence of a canonical vector space attached to a coadjoint orbit of a general unipotent group over finite field."
"in this paper we use jacobi fields to describe the motion of a charged particle in the classical gravitational, electromagnetic, and yang-mills fields."
"suppose that we observe entries or, more generally, linear combinations of entries of an unknown $m\times t$-matrix $a$ corrupted by noise. we are particularly interested in the high-dimensional setting where the number $mt$ of unknown entries can be much larger than the sample size $n$. motivated by several applications, we consider estimation of matrix $a$ under the assumption that it has small rank. this can be viewed as dimension reduction or sparsity assumption. in order to shrink toward a low-rank representation, we investigate penalized least squares estimators with a schatten-$p$ quasi-norm penalty term, $p\leq1$. we study these estimators under two possible assumptions---a modified version of the restricted isometry condition and a uniform bound on the ratio ""empirical norm induced by the sampling operator/frobenius norm."" the main results are stated as nonasymptotic upper bounds on the prediction risk and on the schatten-$q$ risk of the estimators, where $q\in[p,2]$. the rates that we obtain for the prediction risk are of the form $rm/n$ (for $m=t$), up to logarithmic factors, where $r$ is the rank of $a$. the particular examples of multi-task learning and matrix completion are worked out in detail. the proofs are based on tools from the theory of empirical processes. as a by-product, we derive bounds for the $k$th entropy numbers of the quasi-convex schatten class embeddings $s_p^m\hookrightarrow s_2^m$, $p<1$, which are of independent interest."
"much of current machine learning (ml) research has lost its connection to problems of import to the larger world of science and society. from this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. what changes are needed to how we conduct research to increase the impact that ml has? we present six impact challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. we aim to inspire ongoing discussion and focus on ml that matters."
"in recent years, algebras and modules of differential operators have been extensively studied. equivariant quantization and dequantization establish a tight link between invariant operators connecting modules of differential operators on tensor densities, and module morphisms that connect the corresponding dequantized spaces. in this paper, we investigate dequantized differential operators as modules over a lie subalgebra of vector fields that preserve an additional structure. more precisely, we take an interest in invariant operators between dequantized spaces, viewed as modules over the lie subalgebra of infinitesimal contact or projective contact transformations. the principal symbols of these invariant operators are invariant tensor fields. we first provide full description of the algebras of such affine-contact- and contact-invariant tensor fields. these characterizations allow showing that the algebra of projective-contact-invariant operators between dequantized spaces implemented by the same density weight, is generated by the vertical cotangent lift of the contact form and a generalized contact hamiltonian. as an application, we prove a second key-result, which asserts that the casimir operator of the lie algebra of infinitesimal projective contact transformations, is diagonal. eventually, this upshot entails that invariant operators between spaces induced by different density weights, are made up by a small number of building bricks that force the parameters of the source and target spaces to verify diophantine-type equations."
"we consider a real-time communication system with noisy feedback consisting of a markov source, a forward and a backward discrete memoryless channels, and a receiver with finite memory. the objective is to design an optimal communication strategy (that is, encoding, decoding, and memory update strategies) to minimize the total expected distortion over a finite horizon. we present a sequential decomposition for the problem, which results in a set of nested optimality equations to determine optimal communication strategies. this provides a systematic methodology to determine globally optimal joint source-channel encoding and decoding strategies for real-time communication systems with noisy feedback."
"we define the notion of admissible pair for an algebra $a$, consisting on a couple $(\gamma,r)$, where $\gamma$ is a quiver and $r$ a unital, splitted and factorizable representation of $\gamma$, and prove that the set of admissible pairs for $a$ is in one to one correspondence with the points of the variety of twisting maps $\mathcal{t}_a^n:=\mathcal{t}(k^n,a)$. we describe all these representations in the case $a=k^m$."
"we report on the dewetting of a monolayer on a solid substrate, where mass transport occurs via surface diffusion. for a wide range of parameters, a labyrinthine pattern of bilayer islands is formed. an irreversible regime and a thermodynamic regime are identified. in both regimes, the velocity of a dewetting front, the wavelength of the bilayer island pattern, and the rate of nucleation of dewetted zones are obtained. we also point out the existence of a scaling behavior, which is analyzed by means of a geometrical model."
"we propose a new regularization technique, named hybrid spatio-spectral total variation (hsstv), for hyperspectral (hs) image denoising and compressed sensing. regularization techniques based on total variation (tv) focus on local differences of an hs image to model its underlying smoothness and have been recognized as a popular approach to hs image restoration. however, existing tvs do not fully exploit underlying spectral correlation in their designs and/or require a high computational cost in optimization. our hsstv is designed to simultaneously evaluates two types of local differences: direct local spatial differences and local spatio-spectral differences in a unified manner with a balancing weight. this design resolves the said drawbacks of existing tvs. then, we formulate hs image restoration as a constrained convex optimization problem involving hsstv and develop an efficient algorithm based on the alternating direction method of multipliers (admm) for solving it. in the experiments, we illustrate the advantages of hsstv over several state-of-the-art methods."
"in regression models involving economic variables such as income, log transformation is typically taken to achieve approximate normality and stabilize the variance. however, often the interest is predicting individual values or means of the variable in the original scale. back transformation of predicted values introduces a non-negligible bias. moreover, assessing the uncertainty of the actual predictor is not straightforward. in this paper, a nested error model for the log transformation of the target variable is considered. nested error models are widely used for estimation of means in subpopulations with small sample sizes (small areas), by linking all the areas through common parameters. these common parameters are estimated using the overall set of sample data, which leads to much more efficient small area estimators. analytical expressions for the best predictors of individual values of the original variable and of small area means are obtained under the nested error model with log transformation of the target variable. empirical best predictors are defined by estimating the unknown model parameters in the best predictors. exact mean squared errors of the best predictors and second order approximations to the mean squared errors of the empirical best predictors are derived. mean squared error estimators that are second order correct are also obtained. an example with mexican data on living conditions illustrates the procedures."
"we consider an auction market in which market makers fill the order book during a given time period while some other investors send market orders. we define the clearing price of the auction as the price maximizing the exchanged volume at the clearing time according to the supply and demand of each market participants. then we derive in a semi-explicit form the error made between this clearing price and the fundamental price as a function of the auction duration. we study the impact of the behavior of market takers on this error. to do so we consider the case of naive market takers and that of rational market takers playing a nash equilibrium to minimize their transaction costs. we compute the optimal duration of the auctions for 77 stocks traded on euronext and compare the quality of price formation process under this optimal value to the case of a continuous limit order book. continuous limit order books are found to be usually sub-optimal. however, in term of our metric, they only moderately impair the quality of price formation process. order of magnitude of optimal auction durations is from 2 to 10 minutes."
"if a positive definite hermitian lattice represents all positive integers, we call it universal. several mathematicians, including the author, found 25 universal binary hermitian lattices. but their ad hoc proofs are complicated. we give simple and unified proofs."
"quantitative computed tomography (qct) is a widely used tool for osteoporosis diagnosis and monitoring. the assessment of cortical markers like cortical bone mineral density (bmd) and thickness is a demanding task, mainly because of the limited spatial resolution of qct. we propose a direct model based method to automatically identify the surface through the center of the cortex of human vertebra. we develop a statistical bone model and analyze its probability distribution after the imaging process. using an as-rigid-as-possible deformation we find the cortical surface that maximizes the likelihood of our model given the input volume. using the european spine phantom (esp) and a high resolution \mu ct scan of a cadaveric vertebra, we show that the proposed method is able to accurately identify the real center of cortex ex-vivo. to demonstrate the in-vivo applicability of our method we use manually obtained surfaces for comparison."
"we suggest a novel extension to the kaluza-klein scheme that allows us to obtain consistently all su(n) einstein-yang-mills theories. this construction is based on allowing the five-dimensional spacetime to carry some non-vanishing torsion; however, the four-dimensional spacetime remains intrinsically torsion-free."
"we expand the item response theory to study the case of ""cheating students"" for a set of exams, trying to detect them by applying a greedy algorithm of inference. this extended model is closely related to the boltzmann machine learning. in this paper we aim to infer the correct biases and interactions of our model by considering a relatively small number of sets of training data. nevertheless, the greedy algorithm that we employed in the present study exhibits good performance with a few number of training data. the key point is the sparseness of the interactions in our problem in the context of the boltzmann machine learning: the existence of cheating students is expected to be very rare (possibly even in real world). we compare a standard approach to infer the sparse interactions in the boltzmann machine learning to our greedy algorithm and we find the latter to be superior in several aspects."
"a method is presented for generation of a subwavelength (0.43 lambda) longitudinally polarized beam, which propagates without divergence over lengths of about 4 lambda in free space. this is achieved by controlling the amplitude, phase and polarization property of the bessel-gaussian field on the aperture of a high numerical aperture focusing lens."
"i consider how cell shape and environmental geometry affect the rate of nutrient capture and the consequent maximum growth rate of a cell, focusing on rod-like species like \textit{e.\ coli}. simple modeling immediately implies that it is the elongated profiles of such cells that allows for them to grow -- as observed -- at exponential rates in nutrient-rich media. growth is strongly suppressed when nutrient capture is diffusion-limited: in three dimensions, the length is bounded by $\log l \lesssim t^{1/2}$, and in lower dimensions growth is algebraic. similar bounds are easily obtained for other cell geometries, groups of cells, \textit{etc}. fits of experimental growth curves to such bounds can be used to estimate various quantities of interest, including generalized metabolic rates."
"in this paper, a bayesian approach is developed for simultaneously comparing multiple experimental treatments with a common control treatment in an exploratory clinical trial. the sample size is set to ensure that, at the end of the study, there will be at least one treatment for which the investigators have a strong belief that it is better than control, or else they have a strong belief that none of the experimental treatments are substantially better than control. this criterion bears a direct relationship with conventional frequentist power requirements, while allowing prior opinion to feature in the analysis with a consequent reduction in sample size. if it is concluded that at least one of the experimental treatments shows promise, then it is envisaged that one or more of these promising treatments will be developed further in a definitive phase iii trial. the approach is developed in the context of normally distributed responses sharing a common standard deviation regardless of treatment. to begin with, the standard deviation will be assumed known when the sample size is calculated. the final analysis will not rely upon this assumption, although the intended properties of the design may not be achieved if the anticipated standard deviation turns out to be inappropriate. methods that formally allow for uncertainty about the standard deviation, expressed in the form of a bayesian prior, are then explored. illustrations of the sample sizes computed from the new method are presented, and comparisons are made with frequentist methods devised for the same situation."
we study chains in an $h$-closed topological partially ordered space. we give sufficient conditions for a maximal chain $l$ in an $h$-closed topological partially ordered space such that $l$ contains a maximal (minimal) element. also we give sufficient conditions for a linearly ordered topological partially ordered space to be $h$-closed. we prove that any $h$-closed topological semilattice contains a zero. we show that a linearly ordered $h$-closed topological semilattice is an $h$-closed topological pospace and show that in the general case this is not true. we construct an example an $h$-closed topological pospace with a non-$h$-closed maximal chain and give sufficient conditions that a maximal chain of an $h$-closed topological pospace is an $h$-closed topological pospace.
"the melting behavior of long, heterogeneous dna chains is examined within the framework of the nonlinear lattice dynamics based peyrard-bishop-dauxois (pbd) model. data for the pbr322 plasmid and the complete t7 phage have been used to obtain model fits and determine parameter dependence on salt content. melting curves predicted for the complete fd phage and the y1 and y2 fragments of the $\phi$x174 phage without any adjustable parameters are in good agreement with experiment. the calculated probabilities for single base-pair opening are consistent with values obtained from imino proton exchange experiments."
"independent component analysis (ica) has become a standard data analysis technique applied to an array of problems in signal processing and machine learning. this tutorial provides an introduction to ica based on linear algebra formulating an intuition for ica from first principles. the goal of this tutorial is to provide a solid foundation on this advanced topic so that one might learn the motivation behind ica, learn why and when to apply this technique and in the process gain an introduction to this exciting field of active research."
"we propose a possibility of spontaneous cp-violation (scpv) at high scale in a susy so(10) theory. the model is l-r symmetric susy so(10) with \textbf{\noun{10}} and \textbf{126} dimensional higgs generating fermion masses, and the cp phase is generated through complex vev of b-l breaking \textbf{126} higgs . the model can have potential application in explaining $\nu$ masses and leptogenesis as well."
"in this article we discuss some of the consequences of the mixed membership perspective on time series analysis. in its most abstract form, a mixed membership model aims to associate an individual entity with some set of attributes based on a collection of observed data. although much of the literature on mixed membership models considers the setting in which exchangeable collections of data are associated with each member of a set of entities, it is equally natural to consider problems in which an entire time series is viewed as an entity and the goal is to characterize the time series in terms of a set of underlying dynamic attributes or ""dynamic regimes"". indeed, this perspective is already present in the classical hidden markov model, where the dynamic regimes are referred to as ""states"", and the collection of states realized in a sample path of the underlying process can be viewed as a mixed membership characterization of the observed time series. our goal here is to review some of the richer modeling possibilities for time series that are provided by recent developments in the mixed membership framework."
"we report the proof that the extension of gibrat's law in the middle scale region is unique and the probability distribution function (pdf) is also uniquely derived from the extended gibrat's law and the law of detailed balance. in the proof, two approximations are employed. the pdf of growth rate is described as tent-shaped exponential functions and the value of the origin of the growth rate distribution is constant. these approximations are confirmed in profits data of japanese companies 2003 and 2004. the resultant profits pdf fits with the empirical data with high accuracy. this guarantees the validity of the approximations."
"we consider a one dimensional euclidean network which is grown using a preferential attachment. here the $j$th incoming node gets attached to the $i$th existing node with the probability $\pi_i \propto k_i {{l}}_{ij}^\alpha$, where ${l}_{ij}$ is the euclidean distance between them and $k_i$ the degree of the $i$th node. this network is known to have a static phase transition point at $\alpha_c \simeq 0.5$. on this network, we employ three different searching strategies based on degrees or distances or both, where the possibility of termination of search chains is allowed. a detailed analysis shows that these strategies are significantly affected by the presence of the static critical point. the distributions of the search path lengths and the success rates are also estimated and compared for the different strategies. these distributions appear to be marginally affected by the static phase transition."
"the analysis of spatial point patterns that occur in the network domain have recently gained much attraction and various intensity functions and measures have been proposed. however, the linkage of spatial network statistics to regression models has not been approached so far. this paper presents a new regression approach which treats a generic intensity function of a planar point pattern that occurred on a network as the outcome of a set of different covariates and various graph statistics. different to all alternative approaches, our model is the first which permits the statistical analysis of complex regression data in the context of network intensity functions for spatial point patterns. the potential of our new technique to model the structural dependencies of network intensity functions on various covariates and graph statistics is illustrated using call-in data on neighbour and community disturbances in an urban context."
"in this article we give an algorithm for the computation of the number of rational points on the jacobian variety of a generic ordinary hyperelliptic curve defined over a finite field of cardinality $q$ with time complexity $o(n^{2+o(1)})$ and space complexity $o(n^2)$, where $n=\log(q)$. in the latter complexity estimate the genus and the characteristic are assumed as fixed. our algorithm forms a generalization of both, the agm algorithm of j.-f. mestre and the canonical lifting method of t. satoh. we canonically lift a certain arithmetic invariant of the jacobian of the hyperelliptic curve in terms of theta constants. the theta null values are computed with respect to a semi-canonical theta structure of level $2^\nu p$ where $\nu >0$ is an integer and $p=\mathrm{char}(\f_q)>2$. the results of this paper suggest a global positive answer to the question whether there exists a quasi-quadratic time algorithm for the computation of the number of rational points on a generic ordinary abelian variety defined over a finite field."
we introduce new models of very weakly coupled logistic and tent maps for which orbits of very long period are found. the length of these periods is far greater than one billion. the property of these models relatively to the distribution of the iterated points (invariant measure) is described.
"we present a new mathematical model of colorectal cancer growth and its response to monoclonal-antibody (mab) therapy. although promising, most mab drugs are still in trial phases, and the possible variations in the dosing schedules of those currently approved for use have not yet been thoroughly explored. to investigate the effectiveness of current mab treatment schedules, and to test hypothetical treatment strategies, we have created a system of nonlinear ordinary differential equations (ode) to model colorectal cancer growth and treatment. the model includes tumor cells, elements of the host's immune response, and treatments. model treatments include the chemotherapy agent irinotecan and one of two monoclonal antibodies - cetuximab, which is fda-approved for colorectal cancer, and panitumumab, which is still being evaluated in clinical trials. the model incorporates patient-specific parameters to account for individual variations in immune system strength and in medication efficacy against the tumor. we have simulated outcomes for groups of virtual patients on treatment protocols for which clinical trial data are available, using a range of biologically reasonable patient-specific parameter values. our results closely match clinical trial results for these protocols. we also simulated experimental dosing schedules, and have found new schedules which, in our simulations, reduce tumor size more effectively than current treatment schedules. additionally, we examined the system's equilibria and sensitivity to parameter values. in the absence of treatment, tumor evolution is most affected by the intrinsic tumor growth rate and carrying capacity. when treatment is introduced, tumor growth is most affected by drug-specific pk/pd parameters."
"healthy nutrition promotions and regulations have long been regarded as a tool for increasing social welfare. one of the avenues taken in the past decade is sugar consumption regulation by introducing a sugar tax. such a tax increases the price of extensive sugar containment in products such as soft drinks. in this article we consider a typical problem of optimal regulatory policy design, where the task is to determine the sugar tax rate maximizing the social welfare. we model the problem as a sequential game represented by the three-level mathematical program. on the upper level, the government decides upon the tax rate. on the middle level, producers decide on the product pricing. on the lower level, consumers decide upon their preferences towards the products. while the general problem is computationally intractable, the problem with a few product types is polynomially solvable, even for an arbitrary number of heterogeneous consumers. this paper presents a simple, intuitive and easily implementable framework for computing optimal sugar tax in a market with a few products. this resembles the reality as the soft drinks, for instance, are typically categorized in either regular or no-sugar drinks, e.g. coca-cola and coca-cola zero. we illustrate the algorithm using an example based on the real data and draw conclusions for a specific local market."
"different directed acyclic graphs (dags) may be markov equivalent in the sense that they entail the same conditional independence relations among the observed variables. meek (1995) characterizes markov equivalence classes for dags (with no latent variables) by presenting a set of orientation rules that can correctly identify all arrow orientations shared by all dags in a markov equivalence class, given a member of that class. for dag models with latent variables, maximal ancestral graphs (mags) provide a neat representation that facilitates model search. earlier work (ali et al. 2005) has identified a set of orientation rules sufficient to construct all arrowheads common to a markov equivalence class of mags. in this paper, we provide extra rules sufficient to construct all common tails as well. we end up with a set of orientation rules sound and complete for identifying commonalities across a markov equivalence class of mags, which is particularly useful for causal inference."
"we report on the calculation of multi-loop feynman integrals for single-scale problems by means of difference equations in mellin space. the solution to these difference equations in terms of harmonic sums can be constructed algorithmically over difference fields, the so-called pi-sigma-fields. we test the implementaion of the mathematica package sigma on examples from recent higher order perturbative calculations in quantum chromodynamics."
"a challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. the standard techniques include $k$-fold cross-validation ($k$-cv), akaike information criterion (aic), and bayesian information criterion (bic). though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. in this paper, we present stars: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. the method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. this interpretation requires essentially no conditions. under mild conditions, we show that stars is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. empirically, the performance of stars is compared with the state-of-the-art model selection procedures, including $k$-cv, aic, and bic, on both synthetic data and a real microarray dataset. stars outperforms all these competing procedures."
"long-range interactions in finite density qcd necessitate a non-perturbative approach in order to reliably map out the key features and spectrum of the qcd phase diagram. however, the complex nature of the fermion determinant in this sector prohibits the use of established monte carlo techniques that utilize importance sampling. whilst significant progress has been made in the low density, high temperature region, this remains a considerable challenge at mid to high density. at large chemical potential, qcd can be approximated using high density effective theory which is free from the sign problem at leading order. we investigate the implementation of this theory on the lattice in conjunction with existing re-weighting techniques."
exact distribution of the moment estimator of shape parameter for the gamma distribution for small samples is derived. order preserving properties of this estimator are presented.
"to appear to mcmc handbook, s. p. brooks, a. gelman, g. jones and x.-l. meng (eds), chapman & hall."
"this paper concerns the transmission of two independent gaussian sources over a two-user decentralized interference channel, assuming that the transmitters are unaware of the instantaneous csis. the availability of the channel state information at receivers (csir) is considered in two scenarios of perfect and imperfect csir. in the imperfect csir case, we consider a more practical assumption of having an mmse estimation of the channel gain at the receivers. in this case, minimizing the expected achievable distortion associated with each link is considered. due to the absence of csi at the transmitters, the gaussian sources are encoded in a successively refinable manner and the resulting code words are transmitted over the channel using a multi-layer coding technique. accordingly, the optimal power assignment between code layers leading to the least expected achievable distortion, under a mean-square error criterion is derived for both, the perfect and imperfect csir scenarios. finally, some numerical examples are provided and it is demonstrated that the proposed method results in better performance as compared with the conventional single-layer approach, termed as outage approach."
"through a short sale, a person borrows a share of stock from a lender, sells the borrowed share to a third person at the current price, and purchases an identical share in the market at a future date and at a future price to replace the borrowed share of stock. this only makes sense if the short seller anticipates a downward trend in share price. the short seller incurs a gain if share price decreases because the cost of replacing the borrowed share falls below the selling price. the reverse is true in an ordinary sale, where a person owning a share of stock incurs a loss if price decreases because the selling price falls below the basis or acquisition cost. therefore, when a taxpayer simultaneously owns a share of stock and short sells an identical stock, any gain in an ordinary sale of the owned stock is offset by a corresponding loss in the short sale of the borrowed identical stock, vice versa. this offsetting effect, in turn, creates an unexpected tax deferral opportunity abused in other jurisdictions and which remains unregulated in the philippine tax system."
"we show that for a fixed free group f and an arbitrary finitely generated subgroup h (as given above) we can perform the stalling's folding process in time o(n log^*(n)), where n is the sum of the word lengths of the given generators of h."
"producing overlapping schemes is a major issue in clustering. recent proposed overlapping methods relies on the search of an optimal covering and are based on different metrics, such as euclidean distance and i-divergence, used to measure closeness between observations. in this paper, we propose the use of another measure for overlapping clustering based on a kernel similarity metric .we also estimate the number of overlapped clusters using the gram matrix. experiments on both iris and eachmovie datasets show the correctness of the estimation of number of clusters and show that measure based on kernel similarity metric improves the precision, recall and f-measure in overlapping clustering."
"in a recent paper by barton (j. phys. a40, 1011 (2007)), the 1-dimensional klein-gordon equation was solved analytically for the non-singular coulomb-like potential v_1(|x|) = -\alpha/(|x|+a). in the present paper, these results are completely confirmed by a numerical formulation that also allows a solution for an alternative cutoff coulomb potential v_2(|x|) = -\alpha/|x|, ~|x| > a, and otherwise v_2(|x|) = -\alpha/a."
"we present a comprehensive analysis of holography for the bubbling solutions of lin-lunin-maldacena. these solutions are uniquely determined by a coloring of a 2-plane, which was argued to correspond to the phase space of free fermions. we show that in general this phase space distribution does not determine fully the 1/2 bps state of n=4 sym that the gravitational solution is dual to, but it does determine it enough so that vevs of all single trace 1/2 bps operators in that state are uniquely determined to leading order in the large n limit. these are precisely the vevs encoded in the asymptotics of the llm solutions. we extract these vevs for operators up to dimension 4 using holographic renormalization and kk holography and show exact agreement with the field theory expressions."
"we analyze a quasi-continuous linear chain with self-similar distribution of harmonic interparticle springs as recently introduced for one dimension (michelitsch et al., phys. rev. e 80, 011135 (2009)). we define a continuum limit for one dimension and generalize it to $n=1,2,3,..$ dimensions of the physical space. application of hamilton's (variational) principle defines then a self-similar and as consequence non-local laplacian operator for the $n$-dimensional space where we proof its ellipticity and its accordance (up to a strictly positive prefactor) with the fractional laplacian $-(-\delta)^\frac{\alpha}{2}$. by employing this laplacian we establish a fokker planck diffusion equation: we show that this laplacian generates spatially isotropic l\'evi stable distributions which correspond to l\'evi flights in $n$-dimensions. in the limit of large scaled times $\sim t/r^{\alpha} >>1$ the obtained distributions exhibit an algebraic decay $\sim t^{-\frac{n}{\alpha}} \rightarrow 0$ independent from the initial distribution and spacepoint. this universal scaling depends only on the ratio $n/\alpha$ of the dimension $n$ of the physical space and the l\'evi parameter $\alpha$."
"knowledge of the exact distribution of meiotic crossovers (cos) and gene conversions (gcs) is essential for understanding many aspects of population genetics and evolution, from haplotype structure and long-distance genetic linkage to the generation of new allelic variants of genes. to this end, we resequenced the four products of 13 meiotic tetrads along with 10 doubled haploids derived from arabidopsis thaliana hybrids. gc detection through short reads has previously been confounded by genomic rearrangements. rigid filtering for misaligned reads allowed gc identification at high accuracy and revealed an ~80-kb transposition, which undergoes copy-number changes mediated by meiotic recombination. non-crossover associated gcs were extremely rare most likely due to their short average length of ~25-50 bp, which is significantly shorter than the length of co associated gcs. overall, recombination preferentially targeted non-methylated nucleosome-free regions at gene promoters, which showed significant enrichment of two sequence motifs."
"dealing with uncertainty in bayesian network structures using maximum a posteriori (map) estimation or bayesian model averaging (bma) is often intractable due to the superexponential number of possible directed, acyclic graphs. when the prior is decomposable, two classes of graphs where efficient learning can take place are tree structures, and fixed-orderings with limited in-degree. we show how map estimates and bma for selectively conditioned forests (scf), a combination of these two classes, can be computed efficiently for ordered sets of variables. we apply scfs to temporal data to learn dynamic bayesian networks having an intra-timestep forest and inter-timestep limited in-degree structure, improving model accuracy over dbns without the combination of structures. we also apply scfs to bayes net classification to learn selective forest augmented naive bayes classifiers. we argue that the built-in feature selection of selective augmented bayes classifiers makes them preferable to similar non-selective classifiers based on empirical evidence."
"in this paper, we study typical ranks of 3-tensors and show that there are plural typical ranks for m\times n\times p tensors over r in the following cases: (1) 3\leq m\leq \rho(n) and (m-1)(n-1)+1\leq p\leq (m-1)n, where \rho\ is the hurwitz-radon function, (2) m=3, n\equiv 3\pmod 4 and p=2n-1, (3) m=4, n\equiv 2\pmod 4, n\geq 6 and p=3n-2, (4) m=6, n\equiv 4\pmod 8, n\geq 12 and p=5n-4. (5) m=10, n\equiv 24\pmod{32} and p=9n-8."
"in this technical report we presented a novel approach to machine learning. once the new framework is presented, we will provide a simple and yet very powerful learning algorithm which will be benchmark on various dataset.   the framework we proposed is based on booleen circuits; more specifically the classifier produced by our algorithm have that form. using bits and boolean gates instead of real numbers and multiplication enable the the learning algorithm and classifier to use very efficient boolean vector operations. this enable both the learning algorithm and classifier to be extremely efficient. the accuracy of the classifier we obtain with our framework compares very favorably those produced by conventional techniques, both in terms of efficiency and accuracy."
"we introduce a novel multichannel blind deconvolution (bd) method that extracts sparse and front-loaded impulse responses from the channel outputs, i.e., their convolutions with a single arbitrary source. a crucial feature of this formulation is that it doesn't encode support restrictions on the unknowns, unlike most prior work on bd. the indeterminacy inherent to bd, which is difficult to resolve with a traditional l1 penalty on the impulse responses, is resolved in our method because it seeks a first approximation where the impulse responses are: ""maximally white"" -- encoded as the energy focusing near zero lag of the impulse-response auto-correlations; and ""maximally front-loaded"" -- encoded as the energy focusing near zero time of the impulse responses. hence we call the method focused blind deconvolution (fbd). the focusing constraints are relaxed as the iterations progress. note that fbd requires the duration of the channel outputs to be longer than that of the unknown impulse responses.   a multichannel blind deconvolution problem that is appropriately formulated by sparse and front-loaded impulse responses arises in seismic inversion, where the impulse responses are the green's function evaluations at different receiver locations, and the operation of a drill bit inputs the noisy and correlated source signature into the subsurface. we demonstrate the benefits of fbd using seismic-while-drilling numerical experiments, where the noisy data recorded at the receivers are hard to interpret, but fbd can provide the processing essential to separate the drill-bit (source) signature from the interpretable green's function."
"statistical machine learning methods are increasingly used for neuroimaging data analysis. their main virtue is their ability to model high-dimensional datasets, e.g. multivariate analysis of activation images or resting-state time series. supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g. resting state functional mri) or find sub-populations in large cohorts. by considering different functional neuroimaging applications, we illustrate how scikit-learn, a python machine learning library, can be used to perform some key analysis steps. scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain."
"this issue includes six articles that develop and apply statistical methods for the analysis of gene sequencing data of different types. the methods are tailored to the different data types and, in each case, lead to biological insights not readily identified without the use of statistical methods. a common feature in all articles is the development of methods for analyzing simultaneously data of different types (e.g., genotype, phenotype, pedigree, etc.); that is, using data of one type to inform the analysis of data from another type."
"characterizations of entire subsolutions for the 1-harmonic equation of a constant 1$-tension field are given with applications in geometry via transformation group theory. in particular, we prove that every level hypersurface of such a subsolution is calibrated and hence is area-minimizing over $\mathbb{r}$; and every 7-dimensional $so(2)\times so(6)$-invariant absolutely area-minimizing integral current in $\mathbb{r}^8$ is real analytic. the assumption on the $so(2) \times so(6)$-invariance cannot be removed, due to the first counter-example in $\mathbb{r}^8$, proved by bombieri, de girogi and giusti."
"asymptotic lower bounds for estimation play a fundamental role in assessing the quality of statistical procedures. in this paper we propose a framework for obtaining semi-parametric efficiency bounds for sparse high-dimensional models, where the dimension of the parameter is larger than the sample size. we adopt a semi-parametric point of view: we concentrate on one dimensional functions of a high-dimensional parameter. we follow two different approaches to reach the lower bounds: asymptotic cram\'er-rao bounds and le cam's type of analysis. both these approaches allow us to define a class of asymptotically unbiased or ""regular"" estimators for which a lower bound is derived. consequently, we show that certain estimators obtained by de-sparsifying (or de-biasing) an $\ell_1$-penalized m-estimator are asymptotically unbiased and achieve the lower bound on the variance: thus in this sense they are asymptotically efficient. the paper discusses in detail the linear regression model and the gaussian graphical model."
"preprocessing forms an oft-neglected foundation for a wide range of statistical and scientific analyses. however, it is rife with subtleties and pitfalls. decisions made in preprocessing constrain all later analyses and are typically irreversible. hence, data analysis becomes a collaborative endeavor by all parties involved in data collection, preprocessing and curation, and downstream inference. even if each party has done its best given the information and resources available to them, the final result may still fall short of the best possible in the traditional single-phase inference framework. this is particularly relevant as we enter the era of ""big data"". the technologies driving this data explosion are subject to complex new forms of measurement error. simultaneously, we are accumulating increasingly massive databases of scientific analyses. as a result, preprocessing has become more vital (and potentially more dangerous) than ever before."
"blind ptychography is a phase retrieval method using multiple coded diffraction patterns from different, overlapping parts of the unknown extended object illuminated with an unknown window function. the window function is also known as the probe in the optics literature. as such blind ptychography is an inverse problem of simultaneous recovery of the object and the window function given the intensities of the windowed fourier transform and has a multi-scale set-up in which the probe has an intermediate scale between the pixel scale and the macro-scale of the extended object. uniqueness problem for blind ptychography is analyzed rigorously for the raster scan (of a constant step size {\tau}) and its variants, in which another scale comes into play: the overlap between adjacent blocks (the shifted windows). the block phases are shown to form an arithmetic progression and the complete characterization of the raster scan ambiguities is given, including: first, the periodic raster grid pathology of degrees of freedom proportional to {\tau}^2 and, second, a non-periodic, arithmetically progressing phase shift from block to block. finally irregularly perturbed raster scans are shown to remove all ambiguities other than the inherent ambiguities of the scaling factor and the affine phase ambiguity under the minimum requirement of roughly 50% overlap ratio."
"we propose a causal analysis of the mother's educational level on the health status of the newborn, in terms of gestational weeks and weight. the analysis is based on a finite mixture structural equation model, the parameters of which have a causal interpretation. the model is applied to a dataset of almost ten thousand deliveries collected in an italian region. the analysis confirms that standard regression overestimates the impact of education on the child health. with respect to the current economic literature, our findings indicate that only high education has positive consequences on child health, implying that policy efforts in education should have benefits for welfare."
"in this review, we address the use of monte carlo methods for approximating definite integrals of the form $z = \int l(x) d p(x)$, where $l$ is a target function (often a likelihood) and $p$ a finite measure. we present vertical-likelihood monte carlo, which is an approach for designing the importance function $g(x)$ used in importance sampling. our approach exploits a duality between two random variables: the random draw $x \sim g$, and the corresponding random likelihood ordinate $y\equiv l(x)$ of the draw. it is natural to specify $g(x)$ and ask: what is the the implied distribution of $y$? in this paper, we take up the opposite question: what should the distribution of $y$ be so that the implied importance function $g(x)$ is good for approximating $z$? our answer turns out to unite seven seemingly disparate classes of algorithms under the vertical-likelihood perspective: importance sampling, slice sampling, simulated annealing/tempering, the harmonic-mean estimator, the vertical-density sampler, nested sampling, and energy-level sampling (a suite of related methods from statistical physics). in particular, we give an alterate presentation of nested sampling, paying special attention to the connection between this method and the vertical-likelihood perspective articulated here. as an alternative to nested sampling, we describe an mcmc method based on re-weighted slice sampling. this method's convergence properties are studied, and two examples demonstrate the promise of the overall approach."
"we have explored the magnetic flux evolution and temperature variation in a coronal-hole region, using big bear solar observatory (bbso) deep magnetograms and {\it soho}/eit images observed from 2005 october 10 to 14. for comparison, we also investigated a neighboring quiet region of the sun. the coronal hole evolved from its mature stage to its disappearance during the observing period. we have obtained the following results: (1) when the coronal hole was well developed on october 10, about 60 % of the magnetic flux was positive. the euv brightness was 420 counts pixel$^{-1}$, and the coronal temperature, estimated from the line ratio of the eit 195 {\aa} and 171 {\aa} images, was 1.07 mk. (2) on october 14, when the coronal hole had almost disappeared, 51 % of the magnetic flux was positive, the euv radiance was 530 counts pixel$^{-1}$, and the temperature was 1.10 mk. (3) in the neighboring quiet region, the fraction of positive flux varied between 0.49 and 0.47. the euv brightness displayed an irregular variation, with a mean value of 870 counts pixel$^{-1}$. the temperature was almost constant at 1.11 mk during the five-day observation. our results demonstrate that in a coronal hole less imbalance of the magnetic flux in opposite polarities leads to stronger euv brightness and higher coronal temperatures."
"neuronal network dynamics depends on network structure. in this paper we study how network topology underpins the emergence of different dynamical behaviors in neuronal networks. in particular, we consider neuronal network dynamics on erd\h{o}s-r\'enyi (er) networks, regular random (rr) networks, ring lattices, and all-to-all networks. we solve analytically a neuronal network model with stochastic binary-state neurons in all the network topologies, except ring lattices. given that apart from network structure, all four models are equivalent, this allows us to understand the role of network structure in neuronal network dynamics. whilst er and rr networks are characterized by similar phase diagrams, we find strikingly different phase diagrams in the all-to-all network. neuronal network dynamics is not only different within certain parameter ranges, but it also undergoes different bifurcations (with a richer repertoire of bifurcations in er and rr compared to all-to-all networks). this suggests that local heterogeneity in the ratio between excitation and inhibition plays a crucial role on emergent dynamics. furthermore, we also observe one subtle discrepancy between er and rr networks, namely er networks undergo a neuronal activity jump at lower noise levels compared to rr networks, presumably due to the degree heterogeneity in er networks that is absent in rr networks. finally, a comparison between network oscillations in rr networks and ring lattices shows the importance of small-world properties in sustaining stable network oscillations."
"principal component analysis (pca) is widely used for dimensionality reduction, with well-documented merits in various applications involving high-dimensional data, including computer vision, preference measurement, and bioinformatics. in this context, the fresh look advocated here permeates benefits from variable selection and compressive sampling, to robustify pca against outliers. a least-trimmed squares estimator of a low-rank bilinear factor analysis model is shown closely related to that obtained from an $\ell_0$-(pseudo)norm-regularized criterion encouraging sparsity in a matrix explicitly modeling the outliers. this connection suggests robust pca schemes based on convex relaxation, which lead naturally to a family of robust estimators encompassing huber's optimal m-class as a special case. outliers are identified by tuning a regularization parameter, which amounts to controlling sparsity of the outlier matrix along the whole robustification path of (group) least-absolute shrinkage and selection operator (lasso) solutions. beyond its neat ties to robust statistics, the developed outlier-aware pca framework is versatile to accommodate novel and scalable algorithms to: i) track the low-rank signal subspace robustly, as new data are acquired in real time; and ii) determine principal components robustly in (possibly) infinite-dimensional feature spaces. synthetic and real data tests corroborate the effectiveness of the proposed robust pca schemes, when used to identify aberrant responses in personality assessment surveys, as well as unveil communities in social networks, and intruders from video surveillance data."
"the paper considers two-phase random design linear regression models. the errors and the regressors are stationary long-range dependent gaussian. the regression parameters, the scale parameters and the change-point are estimated using a method introduced by rousseeuw and yohai(1984). this is called s-estimator and it has the property that is more robust than the classical estimators; the outliers don't spoil the estimation results. some asymptotic results, including the strong consistency and the convergence rate of the s-estimators, are proved."
"in the previous article (found phys. lett. {\bf{16}} 325-341), we showed that a reciprocity of the gauss sums is connected with the wave and particle complementary. in this article, we revise the previous investigation by considering a relation between the gauss optics and the gauss sum based upon the recent studies of the weil representation for a finite group."
"a new type of statistical analysis of the science and technical information (sti) in the web context is produced. we propose a set of indicators about web users, visualized bibliographic records, and e-commercial transactions. in addition, we introduce two web usage factors. finally, we give an overview of the co-usage analysis. for these tasks, we introduce a computer based system, called miri@d, which produces descriptive statistical information about the web users' searching behaviour, and what is effectively used from a free access digital bibliographical database. the system is conceived as a server of statistical data which are carried out beforehand, and as an interactive server for online statistical work. the results will be made available to analysts, who can use this descriptive statistical information as raw data for their indicator design tasks, and as input for multivariate data analysis, clustering analysis, and mapping. managers also can exploit the results in order to improve management and decision-making."
"graphical models are widely used to study biological networks. interventions on network nodes are an important feature of many experimental designs for the study of biological networks. in this paper we put forward a causal variant of dynamic bayesian networks (dbns) for the purpose of modeling time-course data with interventions. the models inherit the simplicity and computational efficiency of dbns but allow interventional data to be integrated into network inference. we show empirical results, on both simulated and experimental data, that demonstrate the need to appropriately handle interventions when interventions form part of the design."
"in this paper the methodology and the results of a quasi real-time thermal characterization tool and method for the temperature mapping of circuits and boards based on sensing the infrared radiation will be introduced. with the proposed method the ir radiation-distribution of boards from the close proximity of the sensor card is monitored in quasi real-time. the proposed method is enabling in situ ir measurement among operating cards of a system e.g. in a rack, enabling the immediate detection of potential hot spots in the system. . the elevated temperature encountered in different packaged electronic devices, like digital processors, high power amplifier, high power switches, etc., demands the application of careful temperature-aware design methodologies and the electro-thermal simulations of pcbs. the results of different electro-thermal simulations and modeling in most of the cases give good approximating results and consider the coupled effects of the real surroundings of these cards and other dissipation elements in an operating system. however the simulation time may take hours, and different systems, different surroundings should be simulated again and again. in our expectation, by using contactless temperature measurement procedure the heat distribution and the places of high dissipation elements on an operating pcb board (pci or agp cards in a rack-house of a pc) can be measured and localized in a dense rack system, where only a thin measuring board can be inserted between the cards during operation."
"we show how to generalize the lattice switch monte carlo method to calculate the phase diagram of a binary system. a global coordinate transformation is combined with a modification of particle diameters, enabling the multi-component system in question to be explored and directly compared to a suitable reference state in a single monte carlo simulation. we use the method to evaluate the free energies of binary hard sphere crystals. calculations at moderate size ratios, \alpha=0.58 and \alpha=0.73, are in agreement with previous results, and confirm ab2 and ab13 as stable structures. we also find that the ab(cscl) structure is not entropically stable at the size ratio and volume at which it has been reported experimentally, and therefore that those observations cannot be explained by packing effects alone."
employing an arbitrary velocity gauge transformation this contribution argues that the breaking of time symmetry is a natural consequence of irreversibility.
"parkinson's disease (pd) is a degenerative condition of the nervous system, which manifests itself primarily as muscle stiffness, hypokinesia, bradykinesia, and tremor. in patients suffering from advanced stages of pd, deep brain stimulation neurosurgery (dbs) is the best alternative to medical treatment, especially when they become tolerant to the drugs. this surgery produces a neuronal activity, a result from electrical stimulation, whose quantification is known as volume of tissue activated (vta). to locate correctly the vta in the cerebral volume space, one should be aware exactly the location of the tip of the dbs electrodes, as well as their spatial projection.   in this paper, we automatically locate dbs electrodes using a threshold-based medical imaging segmentation methodology, determining the optimal value of this threshold adaptively. the proposed methodology allows the localization of dbs electrodes in computed tomography (ct) images, with high noise tolerance, using automatic threshold detection methods."
"in this paper we present acemod, an agent-based modelling framework for studying influenza epidemics in australia. the simulator is designed to analyse the spatiotemporal spread of contagion and influenza spatial synchrony across the nation. the individual-based epidemiological model accounts for mobility (worker and student commuting) patterns and human interactions derived from the 2006 australian census and other national data sources. the high-precision simulation comprises 19.8 million stochastically generated software agents and traces the dynamics of influenza viral infection and transmission at several scales. using this approach, we are able to synthesise epidemics in australia with varying outbreak locations and severity. for each scenario, we investigate the spatiotemporal profiles of these epidemics, both qualitatively and quantitatively, via incidence curves, prevalence choropleths, and epidemic synchrony. this analysis exemplifies the nature of influenza pandemics within australia and facilitates future planning of effective intervention, mitigation and crisis management strategies."
"cold atmospheric plasma (cap) has shown its promising application in cancer treatment both in vitro and in vivo. however, the anti-cancer mechanism is still largely unknown. cap may kill cancer cells via triggering the rise of intracellular ros, dna damage, mitochondrial damage, or cellular membrane damage. while, the specific vulnerability of cancer cells to cap has been observed, the underlying mechanism of such cell-based specific vulnerability to cap is completely unknown. here, through the comparison of cap treatment and h2o2 treatment on 10 different cancer cell lines in vitro, we observed that the h2o2 consumption speed by cancer cells was strongly correlated to the cytotoxicity of cap treatment on cancer cells. cancer cells that clear extracellular h2o2 more quickly are more resistant to the cytotoxicity of cap treatment. this finding strongly indicates that the anti-oxidant system in cancer cells play a key role in the specific vulnerability of cancer cells to cap treatment in vitro."
"for better or for worse, rankings of institutions, such as universities, schools and hospitals, play an important role today in conveying information about relative performance. they inform policy decisions and budgets, and are often reported in the media. while overall rankings can vary markedly over relatively short time periods, it is not unusual to find that the ranks of a small number of ""highly performing"" institutions remain fixed, even when the data on which the rankings are based are extensively revised, and even when a large number of new institutions are added to the competition. in the present paper, we endeavor to model this phenomenon. in particular, we interpret as a random variable the value of the attribute on which the ranking should ideally be based. more precisely, if $p$ items are to be ranked then the true, but unobserved, attributes are taken to be values of $p$ independent and identically distributed variates. however, each attribute value is observed only with noise, and via a sample of size roughly equal to $n$, say. these noisy approximations to the true attributes are the quantities that are actually ranked. we show that, if the distribution of the true attributes is light-tailed (e.g., normal or exponential) then the number of institutions whose ranking is correct, even after recalculation using new data and even after many new institutions are added, is essentially fixed. formally, $p$ is taken to be of order $n^c$ for any fixed $c>0$, and the number of institutions whose ranking is reliable depends very little on $p$."
"at the high densities and low temperatures found in star forming regions, all molecules other than h2 should stick on dust grains on timescales shorter than the cloud lifetimes. yet these clouds are detected in the millimeter lines of gaseous co. at these temperatures, thermal desorption is negligible and hence a non-thermal desorption mechanism is necessary to maintain molecules in the gas phase. here, the first laboratory study of the photodesorption of pure co ice under ultra high vacuum is presented, which gives a desorption rate of 3e-3 co molecules per uv (7-10.5 ev) photon at 15 k. this rate is factors of 1e2-1e5 larger than previously estimated and is comparable to estimates of other non-thermal desorption rates. the experiments constrains the mechanism to a single photon desorption process of ice surface molecules. the measured efficiency of this process shows that the role of co photodesorption in preventing total removal of molecules in the gas has been underestimated."
"the projection congruent subset (pcs) is new method for finding multivariate outliers. pcs returns an outlyingness index which can be used to construct affine equivariant estimates of multivariate location and scatter. in this note, we derive the finite sample breakdown point of these estimators."
"we have recognized that 2d codes, i.e., a group of strongly connected neurosomes that can be simultaneously excited, are the basic data carriers for memory in a brain. an echoing mechanism between two neighboring layers of neurosomes is assumed to establish temporary memory, and repeating processes enhance the formation of long-term memory. creation and degradation of memory information are statistically. the maximum capacity of memory storage in a human brain is estimated to be one billion of 2d codes. by triggering one or more neurosomes in a neurosome-based 2d code, the whole strongly connected neurosome network is capable of exciting simultaneously and projecting its excitation onto an analysis layer of neurons in cortex, thus retrieving the stored memory data. the capability of comparing two 2d codes in the analysis layer is one of the major brain functions."
"we investigate usage of dynamic time warping (dtw) algorithm for aligning raw signal data from minion sequencer. dtw is mostly using for fast alignment for selective sequencing to quickly determine whether a read comes from sequence of interest.   we show that standard usage of dtw has low discriminative power mainly due to problem with accurate estimation of scaling parameters. we propose a simple variation of dtw algorithm, which does not suffer from scaling problems and has much higher discriminative power."
"transport properties of ultrasmall quantum dots with a single unpaired electron are commonly modeled by the nonequilibrium kondo model, describing the exchange interaction of a spin-1/2 local moment with two leads of noninteracting electrons. remarkably, the model possesses an exact solution when tuned to a special manifold in its parameter space known as the toulouse limit. we use the toulouse limit to exactly calculate the adiabatically pumped spin current in the kondo regime. in the absence of both potential scattering and a voltage bias, the instantaneous charge current is strictly zero for a generic kondo model. however, a nonzero spin current can be pumped through the system in the presence of a finite magnetic field, provided the spin couples asymmetrically to the two leads. tunneling through a kondo impurity thus offers a natural mechanism for generating a pure spin current. we show, in particular, that one can devise pumping cycles along which the average spin pumped per cycle is closely equal to $\hbar$. by analogy with brouwer's formula for noninteracting systems with two driven parameters, the pumped spin current is expressed as a geometrical property of a scattering matrix. however, the relevant %alex: i replaced topological with geometrical in the sentence above scattering matrix that enters the formulation pertains to the majorana fermions that appear at the toulouse limit rather than the physical electrons that carry the current. these results are obtained by combining the nonequilibrium keldysh green function technique with a systematic gradient expansion, explicitly exposing the small parameter controlling the adiabatic limit."
"in compressed sensing, in order to recover a sparse or nearly sparse vector from possibly noisy measurements, the most popular approach is $\ell_1$-norm minimization. upper bounds for the $\ell_2$- norm of the error between the true and estimated vectors are given in [1] and reviewed in [2], while bounds for the $\ell_1$-norm are given in [3]. when the unknown vector is not conventionally sparse but is ""group sparse"" instead, a variety of alternatives to the $\ell_1$-norm have been proposed in the literature, including the group lasso, sparse group lasso, and group lasso with tree structured overlapping groups. however, no error bounds are available for any of these modified objective functions. in the present paper, a unified approach is presented for deriving upper bounds on the error between the true vector and its approximation, based on the notion of decomposable and $\gamma$-decomposable norms. the bounds presented cover all of the norms mentioned above, and also provide a guideline for choosing norms in future to accommodate alternate forms of sparsity."
"we introduce a minimal model description for the dynamics of transcriptional regulatory networks. it is studied within a mean-field approximation, i.e., by deterministic ode's representing the reaction kinetics, and by stochastic simulations employing the gillespie algorithm. we elucidate the different results both approaches can deliver, depending on the network under study, and in particular depending on the level of detail retained in the respective description. two examples are addressed in detail: the repressilator, a transcriptional clock based on a three-gene network realized experimentally in e. coli, and a bistable two-gene circuit under external driving, a transcriptional network motif recently proposed to play a role in cellular development."
"we have hiked many miles alongside several professors as we traversed our statistical path -- a regime switching trail which changed direction following a class on the foundations of our discipline. as we play the game of research in that limbo between student and academic, one thing among prof. bernardi's teachings has never been more clear: to draw a route in the research map you not only need to know your destination, but you must also understand where you are and how you arrived there."
"this paper reports on recent results related to audiophonic signals encoding using time-scale and time-frequency transform. more precisely, non-linear, structured approximations for tonal and transient components using local cosine and wavelet bases will be described, yielding expansions of audio signals in the form tonal + transient + residual. we describe a general formulation involving hidden markov models, together with corresponding rate estimates. estimators for the balance transient/tonal are also discussed."
"let $\{x_i\}_{i\geq1}$ be an i.i.d. sequence of random variables and define, for $n\geq2$, \[t_n=\cases{n^{-1/2}\hat{\sigma}_n^{-1}s_n,\quad \hat{\sigma}_n>0,\cr 0,\quad \hat{\sigma}_n=0,}with s_n=\sum_{i=1}^nx_i, \hat{\sigma}^2_n=\frac{1}{n-1}\sum_{i=1}^n(x_i-n^{-1}s_n)^2.\] we investigate the connection between the distribution of an observation $x_i$ and finiteness of $\mathrm{e}|t_n|^r$ for $(n,r)\in \mathbb{n}_{\geq2}\times\mathbb{r}^+$. moreover, assuming $t_n\stackrel{d}{\longrightarrow}t$, we prove that for any $r>0$, $\lim_{n\to\infty}\mathrm{e}|t_n|^r=\mathrm{e}|t|^r<\infty$, provided there is an integer $n_0$ such that $\mathrm {e}|t_{n_0}|^r$ is finite."
"in a recent paper, bassett et al. (2011) have analyzed the static and dynamic organization of functional brain networks in humans. we here focus on the first claim made in this paper, which states that the static modular structure of such networks is nested with respect to time. bassett et al. (2011) argue that this graded structure underlines a ""multiscale modular structure"". in this letter, however, we show that such a relationship is substantially mediated by an increase in the random variation of the correlation coefficients computed at different time scales."
"let b be an undefined quaternion algebra over q. following the explicit chacterization of some eichler orders in b given by hashimoto, we define explicit embeddings of these orders in some local rings of matrices; we describe the two natural inclusions of an eichler order of leven nq in an eichler order of level n. moreover we provide a basis for a chain of eichler orders in b and prove results about their intersection."
"an accurate method to measure the abundance of high-redshift galaxies consists in the observation of absorbers along the line of sight toward a background quasar. here, we present abundance measurements of 13 z>3 sub-damped lyman-alpha systems (quasar absorbers with hi column density 19 < log n(hi) < 20.3 cm^-2) based on the high resolution observations with vlt uves spectrograph. these observations more than double the metallicity information for sub-dlas previously available at z>3. this new data, combined with other sub-dla measurements from the literature, confirm the stronger metallicity redshift evolution than for the classical damped lyman-alpha absorbers. besides, these observations are used to compute for the first time the fraction of gas ionised from photo-ionisation modelling in a sample of sub-dlas. based on these results, we calculate that sub-dlas contribute no more than 6% of the expected amount of metals at z~2.5. we therefore conclude that even if sub-dlas are found to be more metal-rich than classical dlas, they are insufficient to close the so-called ``missing metals problem''."
"we adapt the classical 3-decomposition of any 2-connected graph to the case of simple graphs (no loops or multiple edges). by analogy with the block-cutpoint tree of a connected graph, we deduce from this decomposition a bicolored tree tc(g) associated with any 2-connected graph g, whose white vertices are the 3-components of g (3-connected components or polygons) and whose black vertices are bonds linking together these 3-components, arising from separating pairs of vertices of g. two fundamental relationships on graphs and networks follow from this construction. the first one is a dissymmetry theorem which leads to the expression of the class b=b(f) of 2-connected graphs, all of whose 3-connected components belong to a given class f of 3-connected graphs, in terms of various rootings of b. the second one is a functional equation which characterizes the corresponding class r=r(f) of two-pole networks all of whose 3-connected components are in f. all the rootings of b are then expressed in terms of f and r. there follow corresponding identities for all the associated series, in particular the edge index series. numerous enumerative consequences are discussed."
"we present a bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. for decades, political scientists have collected and analyzed records of the form ""country $i$ took action $a$ toward country $j$ at time $t$""---known as dyadic events---in order to form and test theories of international relations. we represent these event data as a tensor of counts and develop bayesian poisson tensor factorization to infer a low-dimensional, interpretable representation of their salient patterns. we demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. we also provide a comparison of our variational updates to their maximum likelihood counterparts. in doing so, we identify a better way to form point estimates of the latent factors than that typically used in bayesian poisson matrix factorization. finally, we showcase our model as an exploratory analysis tool for political scientists. we show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international affairs."
"the recent increase in bordetella pertussis incidence (whooping cough) presents a challenge to global health. recent studies have called into question the effectiveness of acellular b. pertussis vaccination in reducing transmission. here we examine the epidemiological consequences of an ineffective b. pertussis vaccine. using a dynamic transmission model, we find that: 1) an ineffective vaccine can account for the observed increase in b. pertussis incidence; 2) asymptomatic infections can bias surveillance and upset situational awareness of b. pertussis; and 3) vaccinating individuals in close contact with infants too young to receive vaccine (so called ""cocooning"" unvaccinated children) may be ineffective. our results have important implications for b. pertussis vaccination policy and paint a complicated picture for achieving herd immunity and possible b. pertussis eradication."
"let q denote a smooth manifold acted upon smoothly by a lie group g. the g-action lifts to an action on the total space t of the cotangent bundle of q and hence on the standard symplectic poisson algebra of smooth functions on t. the poisson algebra of g-invariant functions on t yields a poisson structure on the space t/g of g-orbits. we relate this poisson algebra with extensions of lie-rinehart algebras and derive an explicit formula for this poisson structure in terms of differentials. we then show, for the particular case where the g-action on q is principal, how an explicit description of the poisson algebra derived in the literature by an ad hoc construction is essentially a special case of the formula for the corresponding extension of lie-rinehart algebras. by means of various examples, we also show that this kind of description breaks down when the g-action does not define a principal bundle."
"in this work we present a practical algorithm for calibrating a magnetometer for the presence of magnetic disturbances and for magnetometer sensor errors. to allow for combining the magnetometer measurements with inertial measurements for orientation estimation, the algorithm also corrects for misalignment between the magnetometer and the inertial sensor axes. the calibration algorithm is formulated as the solution to a maximum likelihood problem and the computations are performed offline. the algorithm is shown to give good results using data from two different commercially available sensor units. using the calibrated magnetometer measurements in combination with the inertial sensors to determine the sensor's orientation is shown to lead to significantly improved heading estimates."
"in large but finite populations, weak demographic stochasticity due to random birth and death events can lead to population extinction. the process is analogous to the escaping problem of trapped particles under random forces. methods widely used in studying such physical systems, for instance, wentzel-kramers-brillouin (wkb) and fokker-planck methods, can be applied to solve similar biological problems. in this article, we comparatively analyse applications of wkb and fokker-planck methods to some typical stochastic population dynamical models, including the logistic growth, endemic sir, predator-prey, and competitive lotka-volterra models. the mean extinction time strongly depends on the nature of the corresponding deterministic fixed point(s). for different types of fixed points, the extinction can be driven either by rare events or typical gaussian fluctuations. in the former case, the large deviation function that governs the distribution of rare events can be well-approximated by the wkb method in the weak noise limit. in the later case, the simpler fokker-planck approximation approach is also appropriate."
"let h be a separable hilbert space. given two strongly commuting cp_0-semigroups $\phi$ and $\theta$ on b(h), there is a hilbert space k containing h and two (strongly) commuting e_0-semigroups $\alpha$ and $\beta$ such that $\phi_s \circ \theta_t (p_h a p_h) = p_h \alpha_s \circ \beta_t (a) p_h$ for all s,t and all a in b(k).   in this note we prove that if $\phi$ is not an automorphism semigroup then $\alpha$ is cocycle conjugate to the minimal *-endomorphic dilation of $\phi$, and that if $\phi$ is an automorphism semigroup then $\alpha$ is also an automorphism semigroup. in particular, we conclude that if $\phi$ is not an automorphism semigroup and has a bounded generator (in particular, if h is finite dimensional) then $\alpha$ is a type i e_0-semigroup."
"all color-difference formulas are developed to evaluate color differences for pairs of stimuli with hair-line separation. in printing applications, however, color differences are frequently judged between a pair of samples with no-separation because they are printed adjacent on the same piece of paper. a new formula, dens has been developed for pairs of stimuli with no-separation (ns). an experiment was conducted to investigate the effect of different color-difference magnitudes using sample pairs with ns. 1,012 printed pairs with ns were prepared around 11 cie recommended color centers. the pairs, representing four color-difference magnitudes of 1, 2, 4 and 8 cielab units were visually evaluated by a panel of 19 observers using the gray-scale method. comparison of the present data based on pairs with ns, and previously generated data using pairs with hair-line separation, showed a clear separation effect. a new color-difference equation for the ns viewing condition (dens) is proposed by modifying the ciede2000 formula. the separation effect can be well described by the new formula. for a sample pair with ns, when the ciede2000 color difference is less than 9.1, a larger color difference leads to a larger lightness difference, and thus the total color difference increases. when the ciede2000 color difference is greater than 9.1, the effect is opposite, i.e. the lightness difference decreases, and thus the total color difference also decreases. the new formula is recommended for future research to evaluate its performance in appropriate applications."
"in the framework of noisy quantum homodyne tomography with efficiency parameter $0 < \eta \leq 1$, we propose two estimators of a quantum state whose density matrix elements $\rho_{m,n}$ decrease like $e^{-b(m+n)^{r/ 2}}$, for fixed known $b>0$ and $0<r\leq 2$. the first procedure estimates the matrix coefficients by a projection method on the pattern functions (that we introduce here for $0<\eta \leq 1/2$), the second procedure is a kernel estimator of the associated wigner function. we compute the convergence rates of these estimators, in $\mathbb{l}_2$ risk."
"a very simple closed-form formula for sheppard's corrections is recovered by means of the classical umbral calculus. by means of this symbolic method, a more general closed-form formula for discrete parent distributions is provided and the generalization to the multivariate case turns to be straightforward. all these new formulae are particularly suited to be implemented in any symbolic package."
"there is effect modification if the magnitude or stability of a treatment effect varies systematically with the level of an observed covariate. a larger or more stable treatment effect is typically less sensitive to bias from unmeasured covariates, so it is important to recognize effect modification when it is present. we illustrate a recent proposal for conducting a sensitivity analysis that empirically discovers effect modification by exploratory methods, but controls the family-wise error rate in discovered groups. the example concerns a study of mortality and use of the intensive care unit in 23,715 matched pairs of two medicare patients, one of whom underwent surgery at a hospital identified for superior nursing, the other at a conventional hospital. the pairs were matched exactly for 130 four-digit icd-9 surgical procedure codes and balanced 172 observed covariates. the pairs were then split into five groups of pairs by cart in its effort to locate effect modification. the evidence of a beneficial effect of magnet hospitals on mortality is least sensitive to unmeasured biases in a large group of patients undergoing rather serious surgical procedures, but in the absence of other life-threatening conditions, such as a comorbidity of congestive heart failure or an emergency admission leading to surgery."
"we consider weighted directed networks for analysing, over the period 2000-2013, the interdependencies between volatilities of a large panel of stocks belonging to the s\&p100 index. in particular, we focus on the so-called {\it long-run variance decomposition network} (lvdn), where the nodes are stocks, and the weight associated with edge $(i,j)$ represents the proportion of $h$-step-ahead forecast error variance of variable $i$ accounted for by variable $j$'s innovations. to overcome the curse of dimensionality, we decompose the panel into a component driven by few global, market-wide, factors, and an idiosyncratic one modelled by means of a sparse vector autoregression (var) model. inversion of the var together with suitable identification restrictions, produces the estimated network, by means of which we can assess how {\it systemic} each firm is.~our analysis demonstrates the prominent role of financial firms as sources of contagion, especially during the~2007-2008 crisis."
"recently in [1] a new class of maximal monotone operators has been introduced. in this note we study domain range properties as well as connections with other classes and calculus rules for these operators we called strongly-representable. while not every maximal monotone operator is strongly-representable, every maximal monotone ni operator is strongly-representable, and every strongly representable operator is locally maximal monotone, maximal monotone locally, and ana. as a consequence the conjugate of the fitzpatrick function of a maximal monotone operator is not necessarily a representative function."
"we investigate entropy as a financial risk measure. entropy explains the equity premium of securities and portfolios in a simpler way and, at the same time, with higher explanatory power than the beta parameter of the capital asset pricing model. for asset pricing we define the continuous entropy as an alternative measure of risk. our results show that entropy decreases in the function of the number of securities involved in a portfolio in a similar way to the standard deviation, and that efficient portfolios are situated on a hyperbola in the expected return - entropy system. for empirical investigation we use daily returns of 150 randomly selected securities for a period of 27 years. our regression results show that entropy has a higher explanatory power for the expected return than the capital asset pricing model beta. furthermore we show the time varying behaviour of the beta along with entropy."
"this paper considers point and interval estimation of the $\ell_q$ loss of an estimator in high-dimensional linear regression with random design. we establish the minimax rate for estimating the $\ell_{q}$ loss and the minimax expected length of confidence intervals for the $\ell_{q}$ loss of rate-optimal estimators of the regression vector, including commonly used estimators such as lasso, scaled lasso, square-root lasso and dantzig selector. adaptivity of the confidence intervals for the $\ell_{q}$ loss is also studied. both the setting of known identity design covariance matrix and known noise level and the setting of unknown design covariance matrix and unknown noise level are studied. the results reveal interesting and significant differences between estimating the $\ell_2$ loss and $\ell_q$ loss with $1\le q <2$ as well as between the two settings.   new technical tools are developed to establish rate sharp lower bounds for the minimax estimation error and the expected length of minimax and adaptive confidence intervals for the $\ell_q$ loss. a significant difference between loss estimation and the traditional parameter estimation is that for loss estimation the constraint is on the performance of the estimator of the regression vector, but the lower bounds are on the difficulty of estimating its $\ell_q$ loss. the technical tools developed in this paper can also be of independent interest."
"the fifth-generation (5g) wireless cellular system, which would be deployed by 2020, is expected to deliver significantly higher capacity and better network performance compared to those of the current fourth-generation (4g) system. specifically, it is predicted that tens of billions of wireless devices will be connected to the wireless network over the next few years, which results in an exponential explosion of mobile data traffic. therefore, more advanced wireless architecture, as well as radical and innovative access technologies, must be proposed to meet this urgent increasing growth of mobile data and connectivity requirements in the coming years. toward this end, two important wireless cellular architectures, namely wireless heterogeneous networks (hetnets) based on the dense deployment of small cells and the cloud radio access networks (c-rans) have been proposed and actively studied by both academic and industry communities. besides enabling a lot of advantages in increasing network coverage as well as end-to-end system throughput, these two novel network architectures have also raised some novel technical challenges and opened exciting research areas for further research. motivated by the aforementioned technical challenges, the general objective of this ph.d. research is to develop efficient radio resource allocation and interference management algorithms for the future high-speed wireless cellular networks. in particular, we have developed various efficient resource allocation algorithms for reducing the transmission power and increasing the end-to-end network throughput for both hetnets and c-rans. furthermore, extensive numerical results are presented to gain further insights and to evaluate the performance of our resource allocation designs."
"hiv rna viral load (vl) is an important outcome variable in studies of hiv infected persons. there exists only a handful of methods which classify patients by viral load patterns. most methods place limits on the use of viral load measurements, are often specific to a particular study design, and do not account for complex, temporal variation. to address this issue, we propose a set of four unambiguous computable characteristics (features) of time-varying hiv viral load patterns, along with a novel centroid-based classification algorithm, which we use to classify a population of 1,576 hiv positive clinic patients into one of five different viral load patterns (clusters) often found in the literature: durably suppressed viral load (dsvl), sustained low viral load (slvl), sustained high viral load (shvl), high viral load suppression (hvls), and rebounding viral load (rvl). the centroid algorithm summarizes these clusters in terms of their centroids and radii. we show that this allows new viral load patterns to be assigned pattern membership based on the distance from the centroid relative to its radius, which we term radial normalization classification. this method has the benefit of providing an objective and quantitative method to assign viral load pattern membership with a concise and interpretable model that aids clinical decision making. this method also facilitates meta-analyses by providing computably distinct hiv categories. finally we propose that this novel centroid algorithm could also be useful in the areas of cluster comparison for outcomes research and data reduction in machine learning."
"we propose several exponential inequalities for self-normalized martingales similar to those established by de la pe\~{n}a. the keystone is the introduction of a new notion of random variable heavy on left or right. applications associated with linear regressions, autoregressive and branching processes are also provided."
"the guaranteed minimum withdrawal benefit (gmwb) rider, as an add on to a variable annuity (va), guarantees the return of premiums in the form of peri- odic withdrawals while allowing policyholders to participate fully in any market gains. gmwb riders represent an embedded option on the account value with a fee structure that is different from typical financial derivatives. we consider fair pricing of the gmwb rider from a financial economic perspective. particular focus is placed on the distinct perspectives of the insurer and policyholder and the unifying relationship. we extend a decomposition of the va contract into components that reflect term-certain payments and embedded derivatives to the case where the policyholder has the option to surrender, or lapse, the contract early."
"in (\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machine learning as a coding and dimensionality reduction problem, and further proposed a simple unsupervised dimensionality reduction method, entitled deep distributed random samplings (ddrs). in this paper, we further extend it to supervised learning incrementally. the key idea here is to incorporate label information into the coding process by reformulating that each center in ddrs has multiple output units indicating which class the center belongs to. the supervised learning method seems somewhat similar with random forests (\cite{breiman2001random}), here we emphasize their differences as follows. (i) each layer of our method considers the relationship between part of the data points in training data with all training data points, while random forests focus on building each decision tree on only part of training data points independently. (ii) our method builds gradually-narrowed network by sampling less and less data points, while random forests builds gradually-narrowed network by merging subclasses. (iii) our method is trained more straightforward from bottom layer to top layer, while random forests build each tree from top layer to bottom layer by splitting. (iv) our method encodes output targets implicitly in sparse codes, while random forests encode output targets by remembering the class attributes of the activated nodes. therefore, our method is a simpler, more straightforward, and maybe a better alternative choice, though both methods use two very basic elements---randomization and nearest neighbor optimization---as the core. this preprint is used to protect the incremental idea from (\cite{zhang2014nonlinear,zhang2014nonlinear2}). full empirical evaluation will be announced carefully later."
"most prior work on active learning of classifiers has focused on sequentially selecting one unlabeled example at a time to be labeled in order to reduce the overall labeling effort. in many scenarios, however, it is desirable to label an entire batch of examples at once, for example, when labels can be acquired in parallel. this motivates us to study batch active learning, which iteratively selects batches of $k>1$ examples to be labeled. we propose a novel batch active learning method that leverages the availability of high-quality and efficient sequential active-learning policies by attempting to approximate their behavior when applied for $k$ steps. specifically, our algorithm first uses monte-carlo simulation to estimate the distribution of unlabeled examples selected by a sequential policy over $k$ step executions. the algorithm then attempts to select a set of $k$ examples that best matches this distribution, leading to a combinatorial optimization problem that we term ""bounded coordinated matching"". while we show this problem is np-hard in general, we give an efficient greedy solution, which inherits approximation bounds from supermodular minimization theory. our experimental results on eight benchmark datasets show that the proposed approach is highly effective"
"a genome-wide association study (gwas) correlates marker variation with trait variation in a sample of individuals. each study subject is genotyped at a multitude of snps (single nucleotide polymorphisms) spanning the genome. here we assume that subjects are unrelated and collected at random and that trait values are normally distributed or transformed to normality. over the past decade, researchers have been remarkably successful in applying gwas analysis to hundreds of traits. the massive amount of data produced in these studies present unique computational challenges. penalized regression with lasso or mcp penalties is capable of selecting a handful of associated snps from millions of potential snps. unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. this paper introduces the iterative hard thresholding (iht) algorithm to the gwas analysis of continuous traits. our parallel implementation of iht accommodates snp genotype compression and exploits multiple cpu cores and graphics processing units (gpus). this allows statistical geneticists to leverage commodity desktop computers in gwas analysis and to avoid supercomputing. we evaluate iht performance on both simulated and real gwas data and conclude that it reduces false positive and false negative rates while remaining competitive in computational time with penalized regression. source code is freely available at https://github.com/klkeys/iht.jl."
"high frequency data in finance have led to a deeper understanding on probability distributions of market prices. several facts seem to be well stablished by empirical evidence. specifically, probability distributions have the following properties: (i) they are not gaussian and their center is well adjusted by levy distributions. (ii) they are long-tailed but have finite moments of any order. (iii) they are self-similar on many time scales. finally, (iv) at small time scales, price volatility follows a non-diffusive behavior. we extend merton's ideas on speculative price formation and present a dynamical model resulting in a characteristic function that explains in a natural way all of the above features. the knowledge of such distribution opens a new and useful way of quantifying financial risk. the results of the model agree -with high degree of accuracy- with empirical data taken from historical records of the standard & poor's 500 cash index."
"the current article unveils and analyzes important shades of meaning for the widely discussed term talent management. it not only grounds the outlined perspectives in incremental formulation and elaboration of this construct, but also is oriented to exploring the underlying reasons for the social actors, proposing new nuances. thus, a mind map and a fish-bone diagram are constructed to depict effectively and efficiently the current state of development for talent management and make easier the realizations of future research endeavours in this field."
"protein solubilization for two-dimensional electrophoresis (2de) has to break molecular interactions to separate the biological contents of the material of interest into isolated and intact polypeptides. this must be carried out in conditions compatible with the first dimension of 2de, namely isoelectric focusing. in addition, the extraction process must enable easy removal of any nonprotein component interfering with the isoelectric focusing. the constraints brought in this process by the peculiar features of isoelectric focusing are discussed, as well as their consequences in terms of possible solutions and limits for the solubilization process."
"in this paper, an estimator of $m$ instants ($m$ is known) of abrupt changes of the parameter of long-range dependence or self-similarity is proved to satisfy a limit theorem with an explicit convergence rate for a sample of a gaussian process. in each estimated zone where the parameter is supposed not to change, a central limit theorem is established for the parameter's (of long-range dependence, self-similarity) estimator and a goodness-of-fit test is also built. {\it to cite this article: j.m. bardet, i. kammoun, c. r. acad. sci. paris, ser. i 340 (2007).}"
in this paper we investigate the complexity of model selection and model testing for dynamical systems with toric steady states. such systems frequently arise in the study of chemical reaction networks. we do this by formulating these tasks as a constrained optimization problem in euclidean space. this optimization problem is known as a euclidean distance problem; the complexity of solving this problem is measured by an invariant called the euclidean distance (ed) degree. we determine closed-form expressions for the ed degree of the steady states of several families of chemical reaction networks with toric steady states and arbitrarily many reactions. to illustrate the utility of this work we show how the ed degree can be used as a tool for estimating the computational cost of solving the model testing and model selection problems.
"obtaining more accurate equity value estimates is the starting point for stock selection, value-based indexing in a noisy market, and beating benchmark indices through tactical style rotation. unfortunately, discounted cash flow, method of comparables, and fundamental analysis typically yield discrepant valuation estimates. moreover, the valuation estimates typically disagree with market price. can one form a superior valuation estimate by averaging over the individual estimates, including market price? this article suggests a bayesian framework for combining two or more estimates into a superior valuation estimate. the framework justifies the common practice of averaging over several estimates to arrive at a final point estimate."
"scenario reduction is an important topic in stochastic programming problems. due to the random behavior of load and renewable energy, stochastic programming becomes a useful technique to optimize power systems. thus, scenario reduction gets more attentions in recent years. many scenario reduction methods have been proposed to reduce the scenario set in a fast speed. however, the speed of scenario reduction is still very slow, in which it takes at least several seconds to several minutes to finish the reduction. this limitation of speed prevents stochastic programming to be implemented in real-time optimal control problems. in this paper, a fast scenario reduction method based on deep learning is proposed to solve this problem. inspired by the deep learning based image process, recognition and generation methods, the scenario data are transformed into a 2d image-like data and then to be fed into a deep convolutional neural network (dcnn). the output of the dcnn will be an ""image"" of the reduced scenario set. since images can be processed in a very high speed by neural networks, the scenario reduction by neural network can also be very fast. the results of the simulation show that the scenario reduction with the proposed dcnn method can be completed in very high speed."
"we derive explicit recursive formulas for target close (tc) and implementation shortfall (is) in the almgren-chriss framework. we explain how to compute the optimal starting and stopping times for is and tc, respectively, given a minimum trading size. we also show how to add a minimum participation rate constraint (percentage of volume, pvol) for both tc and is. we also study an alternative set of risk measures for the optimisation of algorithmic trading curves. we assume a self-similar process (e.g. levy process, fractional brownian motion or fractal process) and define a new risk measure, the p-variation, which reduces to the variance if the process is a brownian motion. we deduce the explicit formula for the tc and is algorithms under a self-similar process. we show that there is an equivalence between selfsimilar models and a family of risk measures called p-variations: assuming a self-similar process and calibrating empirically the parameter p for the p-variation yields the same result as assuming a brownian motion and using the p-variation as risk measure instead of the variance. we also show that p can be seen as a measure of the aggressiveness: p increases if and only if the tc algorithm starts later and executes faster. finally, we show how the parameter p of the p-variation can be implied from the optimal starting time of tc, and that under this framework p can be viewed as a measure of the joint impact of market impact (i.e. liquidity) and volatility."
"molecular docking is an essential tool for drug design. it helps the scientist to rapidly know if two molecules, respectively called ligand and receptor, can be combined together to obtain a stable complex. we propose a new multi-objective model combining an energy term and a surface term to gain such complexes. the aim of our model is to provide complexes with a low energy and low surface. this model has been validated with two multi-objective genetic algorithms on instances from the literature dedicated to the docking benchmarking."
this paper is concerned with the asymptotics for greeks of european-style options and the risk-neutral density function calculated under the constant elasticity of variance model. formulae obtained help financial engineers to construct a perfect hedge with known behaviour and to price any options on financial assets.
"in this paper, we give a simple counter example to the famous hodge conjecture."
"inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. however, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. in this paper, we focus on the class of planar ising models, for which exact inference is tractable using techniques of statistical physics. based on these techniques and recent methods for planarity testing and planar embedding, we propose a simple greedy algorithm for learning the best planar ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). given the set of all pairwise correlations among variables, we select a planar graph and optimal planar ising model defined on this graph to best approximate that set of correlations. we demonstrate our method in simulations and for the application of modeling senate voting records."
"image denoising is the process of removing noise from noisy images, which is an image domain transferring task, i.e., from a single or several noise level domains to a photo-realistic domain. in this paper, we propose an effective image denoising method by learning two image priors from the perspective of domain alignment. we tackle the domain alignment on two levels. 1) the feature-level prior is to learn domain-invariant features for corrupted images with different level noise; 2) the pixel-level prior is used to push the denoised images to the natural image manifold. the two image priors are based on $\mathcal{h}$-divergence theory and implemented by learning classifiers in adversarial training manners. we evaluate our approach on multiple datasets. the results demonstrate the effectiveness of our approach for robust image denoising on both synthetic and real-world noisy images. furthermore, we show that the feature-level prior is capable of alleviating the discrepancy between different level noise. it can be used to improve the blind denoising performance in terms of distortion measures (psnr and ssim), while pixel-level prior can effectively improve the perceptual quality to ensure the realistic outputs, which is further validated by subjective evaluation."
"after an elementary derivation of the ""time transformation"", mapping a counting process onto a homogeneous poisson process with rate one, a brief review of ogata's goodness of fit tests is presented and a new test, the ""wiener process test"", is proposed. this test is based on a straightforward application of donsker's theorem to the intervals of time transformed counting processes. the finite sample properties of the test are studied by monte carlo simulations. performances on simulated as well as on real data are presented. it is argued that due to its good finite sample properties, the new test is both a simple and a useful complement to ogata's tests. warnings are moreover given against the use of a single goodness of fit test."
"we develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types. a natural model for chromatin data in one cell type is a hidden markov model (hmm); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure. the main challenge with learning parameters of such models is that iterative methods such as em are very slow, while naive spectral methods result in time and space complexity exponential in the number of cell types. we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. we provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types. finally, we show that beyond our specific model, some of our algorithmic ideas can be applied to other graphical models."
"short-term changes in efficacy have been postulated to enhance the ability of synapses to transmit information between neurons, and within neuronal networks. even at the level of connections between single neurons, direct confirmation of this simple conjecture has proven elusive. by combining paired-cell recordings, realistic synaptic modelling and information theory, we provide evidence that short-term plasticity can not only improve, but also reduce information transfer between neurons. we focus on a concrete example in rat neocortex, but our results may generalise to other systems. when information is contained in the timings of individual spikes, we find that facilitation, depression and recovery affect information transmission in proportion to their impacts upon the probability of neurotransmitter release. when information is instead conveyed by mean spike rate only, the influences of short-term plasticity critically depend on the range of spike frequencies that the target network can distinguish (its effective dynamic range). our results suggest that to efficiently transmit information, the brain must match synaptic type, coding strategy and network connectivity during development and behaviour."
"grid cells recorded in the parahippocampal formation of freely moving rodents provide a strikingly periodic representation of self-location whose underlying mechanism has been the subject of intense interest. our previous work(1) showed that grid cells represent the most stable subset of a larger continuum of spatially periodic cells (spcs) which deviate from the hexagonal symmetry observed in grid cells. recently navratilova et al(2) suggested that our findings reflected poor isolation of the spikes from multiple grid cells, rather than the existence of actual non-grid spcs. here we refute this suggestion by showing that: (i) most spcs cannot be formed from hexagonal grids; (ii) all standard cluster isolation measures are similar between recorded grid cells and non-grid spcs, and are comparable to those reported in other laboratories; (iii) the spikes from different fields of band-like spcs do not differ. thus the theoretical implications of the presence of cells with spatially periodic firing patterns that diverge from perfect hexagonality need to be taken seriously, rather than explained away on the basis of hopeful but unjustified assumptions."
"prostate cancer is one of the most common cancers in men. it is characterized by a slow growth and it can be diagnosed in an early stage by observing the prostate specific antigen (psa). however, a relapse after the primary therapy could arise and different growth characteristics of the new tumor are observed. in order to get a better understanding of the phenomenon, a mathematical model involving several parameters is considered. to estimate the values of the parameters identifying the disease risk level a novel approach, based on combining particle swarm optimization (pso) with a meshfree interpolation method, is proposed."
"we study the scaling limits of three different aggregation models on z^d: internal dla, in which particles perform random walks until reaching an unoccupied site; the rotor-router model, in which particles perform deterministic analogues of random walks; and the divisible sandpile, in which each site distributes its excess mass equally among its neighbors. as the lattice spacing tends to zero, all three models are found to have the same scaling limit, which we describe as the solution to a certain pde free boundary problem in r^d. in particular, internal dla has a deterministic scaling limit. we find that the scaling limits are quadrature domains, which have arisen independently in many fields such as potential theory and fluid dynamics. our results apply both to the case of multiple point sources and to the diaconis-fulton smash sum of domains."
"in multivariate time series, the estimation of the covariance matrix of the observation innovations plays an important role in forecasting as it enables the computation of the standardized forecast error vectors as well as it enables the computation of confidence bounds of the forecasts. we develop an on-line, non-iterative bayesian algorithm for estimation and forecasting. it is empirically found that, for a range of simulated time series, the proposed covariance estimator has good performance converging to the true values of the unknown observation covariance matrix. over a simulated time series, the new method approximates the correct estimates, produced by a non-sequential monte carlo simulation procedure, which is used here as the gold standard. the special, but important, vector autoregressive (var) and time-varying var models are illustrated by considering london metal exchange data consisting of spot prices of aluminium, copper, lead and zinc."
"markov chain monte carlo (mcmc) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. the flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. this paper gives sufficient conditions to guarantee that univariate gibbs sampling on markov random fields (mrfs) will be fast mixing, in a precise sense. further, an algorithm is given to project onto this set of fast-mixing parameters in the euclidean norm. following recent work, we give an example use of this to project in various divergence measures, comparing univariate marginals obtained by sampling after projection to common variational methods and gibbs sampling on the original parameters."
"we derive ginzburg-landau action by systematically integrating out electronic degrees of freedom in the framework of the keldysh nonlinear sigma-model of disordered superconductors. the resulting ginzburg-landau functional contains a nonlocal $\delta$-dependent contribution to the diffusion constant, which leads, for example, to maki-thompson corrections. it also exhibits an anomalous gor'kov-eliashberg coupling between $\delta$ and the scalar potential, as well as a peculiar nonlocal nonlinear term. the action is gauge invariant and satisfies the fluctuation dissipation theorem. it may be employed e.g. for calculation of higher moments of the current fluctuations."
"we provide an asymptotic expansion of the maximal mean squared error (mse) of the sample median to be attained on shrinking gross error neighborhoods about an ideal central distribution. more specifically, this expansion comes in powers of n^{-1/2}, for n the sample size, and uses a shrinking rate of n^{-1/2} as well. this refines corresponding results of first order asymptotics to be found in rieder[94]. in contrast to usual higher order asymptotics, we do not approximate distribution functions (or densities) in the first place, but rather expand the risk directly. our results are illustrated by comparing them to the results of a simulation study and to numerically evaluated exact mse's in both ideal and contaminated situation."
"we treat quantum counterparts of testing problems whose optimal tests are given by chi-square, t and f tests. these quantum counterparts are formulated as quantum hypothesis testing problems concerning quantum gaussian states families, and contain disturbance parameters, which have group symmetry. quantum hunt-stein theorem removes a part of these disturbance parameters, but other types of difficulty still remain. in order to remove them, combining quantum hunt-stein theorem and other reduction methods, we establish a general reduction theorem that reduces a complicated quantum hypothesis testing problem to a fundamental quantum hypothesis testing problem. using these methods, we derive quantum counterparts of chi-square, t and f tests as optimal tests in the respective settings."
"for a binary quartic form $\phi$ without multiple factors, we classify the quartic k3 surfaces $\phi(x,y)=\phi(z,t)$ whose neron-severi group is (rationally) generated by lines. for generic binary forms $\phi$, $\psi$ of prime degree without multiple factors, we prove that the neron-severi group of the surface $\phi(x,y)=\psi(z,t)$ is rationally generated by lines."
"we consider continuous-time mean-variance portfolio selection with bankruptcy prohibition under convex cone portfolio constraints. this is a long-standing and difficult problem not only because of its theoretical significance, but also for its practical importance. first of all, we transform the above problem into an equivalent mean-variance problem with bankruptcy prohibition without portfolio constraints. the latter is then treated using martingale theory. our findings indicate that we can directly present the semi-analytical expressions of the pre-committed efficient mean-variance policy without a viscosity solution technique but within a general framework of the cone portfolio constraints. the numerical simulation also sheds light on results established in this paper."
"in this paper, we study two issues in asynchronous communication systems. the first issue is the derivation of sum capacity bounds for finite dimensional asynchronous systems. in addition, asymptotic results for the sum capacity bounds are obtained. the second issue is the design of practical suboptimal codes for binary chip asynchronous cdma systems that become optimal for high signal-to-noise (snr) ratios. the performance of such suboptimal codes is also compared to gold and optical orthogonal codes. the conclusion is that the proposed suboptimal codes perform favorably compared to other known codes for high snr asynchronous systems and perform more or less the same as the other codes for the low snr values."
"the fundamental problems of pricing high-dimensional path-dependent options and optimal stopping are central to applied probability and financial engineering. modern approaches, often relying on adp, simulation, and/or duality, have limited rigorous guarantees, which may scale poorly and/or require previous knowledge of basis functions. a key difficulty with many approaches is that to yield stronger guarantees, they would necessitate the computation of deeply nested conditional expectations, with the depth scaling with the time horizon t.   we overcome this fundamental obstacle by providing an algorithm which can trade-off between the guaranteed quality of approximation and the level of nesting required in a principled manner, without requiring a set of good basis functions. we develop a novel pure-dual approach, inspired by a connection to network flows. this leads to a representation for the optimal value as an infinite sum for which: 1. each term is the expectation of an elegant recursively defined infimum; 2. the first k terms only require k levels of nesting; and 3. truncating at the first k terms yields an error of 1/k. this enables us to devise a simple randomized algorithm whose runtime is effectively independent of the dimension, beyond the need to simulate sample paths of the underlying process. indeed, our algorithm is completely data-driven in that it only needs the ability to simulate the original process, and requires no prior knowledge of the underlying distribution. our method allows one to elegantly trade-off between accuracy and runtime through a parameter epsilon controlling the associated performance guarantee, with computational and sample complexity both polynomial in t (and effectively independent of the dimension) for any fixed epsilon, in contrast to past methods typically requiring a complexity scaling exponentially in these parameters."
"we consider social systems in which agents are not only characterized by their states but also have the freedom to choose their interaction partners to maximize their utility. we map such systems onto an ising model in which spins are dynamically coupled by links in a dynamical network. in this model there are two dynamical quantities which arrange towards a minimum energy state in the canonical framework: the spins, s_i, and the adjacency matrix elements, c_{ij}. the model is exactly solvable because microcanonical partition functions reduce to products of binomial factors as a direct consequence of the c_{ij} minimizing energy. we solve the system for finite sizes and for the two possible thermodynamic limits and discuss the phase diagrams."
"whether you trade futures for yourself or a hedge fund, your strategy is counted. long and short position limits make the number of unique strategies finite. formulas of the numbers of strategies, transactions, do nothing actions are derived. a discrete distribution of actions, corresponding probability mass, cumulative distribution and characteristic functions, moments, extreme values are presented. strategies time slice distributions are determined. vector properties of trading strategies are studied. algebraic not associative, commutative, initial magmas with invertible elements control trading positions and strategies. maximum profit strategies, mps, and optimal trading elements can define trading patterns. dynkin introduced the term interpreted in english as ""markov time"" in 1963. neftci applied it for the formalization of technical analysis in 1991."
"suppose k is a field of characteristic 2, and $n,m\geq 4$ powers of 2. then the $a_\infty$-structure of the group cohomology algebras $h^*(c_n,k)$ and $(h^*(c_m,k)$ are well known. we give results characterizing an $a_\infty$-structure on $h^*(c_n\times c_m,k)$ including limits on non-vanishing low-arity operations and an infinite family of non-vanishing higher operations."
"the failure rate function plays an important role in studying the lifetime distributions in reliability theory and life testing models. a study of the general failure rate model $r(t)=a+bt^{\theta-1}$, under squared error loss function taking $a$ and $b$ independent exponential random variables has been analyzed in the literature. in this article, we consider $a$ and $b$ not necessarily independent. the estimates of the parameters $a$ and $b$ under squared error loss, linex loss and entropy loss functions are obtained here."
"this paper describes how to specify probability models for data analysis via a backward induction procedure. the new approach yields coherent, prior-free uncertainty assessment. after presenting some intuition-building examples, the new approach is applied to a kernel density estimator, which leads to a novel method for computing point-wise credible intervals in nonparametric density estimation. the new approach has two additional advantages; 1) the posterior mean density can be accurately approximated without resorting to monte carlo simulation and 2) concentration bounds are easily established as a function of sample size."
"a cluster tree provides a highly-interpretable summary of a density function by representing the hierarchy of its high-density clusters. it is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. this paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of topological features of an empirical cluster tree. we first study a variety of metrics that can be used to compare different trees, analyze their properties and assess their suitability for inference. we then propose methods to construct and summarize confidence sets for the unknown true cluster tree. we introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a graft-versus-host disease (gvhd) data set."
"our empirical modeling suggests that deformation of placental vascular growth is associated with abnormal placental chorionic surface shape. altered chorionic surface shape is associated with lowered placental functional efficiency. we hypothesize that placentas with deformed chorionic surface vascular trees and reduced functional efficiency also have irregular vascular arborization that will be reflected in increased variability of placental thickness and a lower mean thickness.   we find that non-centrality of the umbilical cord insertion is strongly and significantly correlated with disk thickness (spearman's rho=0.128, p=0.002). deformed shape is strongly and significantly associated with lower overall thickness and higher variability of thickness with beta between -0.173 and -0.254 (p<0.001) . both lower mean thickness and high variability of thickness are strongly correlated with higher beta (reduced placental efficiency) (p<0.001 and p=0.038 respectively). greater thickness variability is correlated with higher beta independent of the other placental shape variables p=0.004."
"we show that the coherent state quantization of massive particles in 1+1 de sitter space exhibits an ordering property: there exist some classical observables $a$ and $a^*$ such that $o_{a^{*p}}o_{a^q}=o_{a^{*p} a^q}$ $p, q \in \z$, where $o_a$ is the quantum observable corresponding to the classical observable $a$."
"let $(x\_1,\ldots,x\_n)$ be a $d$-dimensional i.i.d sample from a distribution with density $f$. the problem of detection of a two-component mixture is considered. our aim is to decide whether $f$ is the density of a standard gaussian random $d$-vector ($f=\phi\_d$) against $f$ is a two-component mixture: $f=(1-\varepsilon)\phi\_d +\varepsilon \phi\_d (.-\mu)$ where $(\varepsilon,\mu)$ are unknown parameters. optimal separation conditions on $\varepsilon, \mu, n$ and the dimension $d$ are established, allowing to separate both hypotheses with prescribed errors. several testing procedures are proposed and two alternative subsets are considered."
"we introduce a quantile-adaptive framework for nonlinear variable screening with high-dimensional heterogeneous data. this framework has two distinctive features: (1) it allows the set of active variables to vary across quantiles, thus making it more flexible to accommodate heterogeneity; (2) it is model-free and avoids the difficult task of specifying the form of a statistical model in a high dimensional space. our nonlinear independence screening procedure employs spline approximations to model the marginal effects at a quantile level of interest. under appropriate conditions on the quantile functions without requiring the existence of any moments, the new procedure is shown to enjoy the sure screening property in ultra-high dimensions. furthermore, the quantile-adaptive framework can naturally handle censored data arising in survival analysis. we prove that the sure screening property remains valid when the response variable is subject to random right censoring. numerical studies confirm the fine performance of the proposed method for various semiparametric models and its effectiveness to extract quantile-specific information from heteroscedastic data."
"in this paper, we study the price responsiveness of electricity consumption from empirical commercial and industrial load data obtained from texas. employing a dynamical system perspective, we show that price responsive demand can be modeled as a hybrid of a hammerstein model with delay following a price surge, and a linear arx model under moderate price changes. it is observed that electricity consumption therefore has unique characteristics including (1) qualitatively distinct response between moderate and extremely high prices; and (2) a time delay associated with the response to high prices. it is shown that these observed features may render traditional approaches to demand response and retail pricing based on classical economic theories ineffective. in particular, ultimate real-time retail pricing may be limitedly beneficial than as considered in classical economic theories."
"i propose a novel method, called the wasserstein index generation model (wig), to generate public sentiment index automatically. it can be performed off-the-shelf and is especially good at detecting sudden sentiment spikes. to test the model's effectiveness, an application to generate economic policy uncertainty (epu) index is showcased."
"the paper emphasizes the properties of exponential dichotomy and exponential trichotomy for skew-evolution semiflows in banach spaces, by means of evolution semiflows and evolution cocycles. the approach is from uniform point of view. some characterizations which generalize classic results are also provided."
"this paper develops an active sensing method to estimate the relative weight (or trust) agents place on their neighbors' information in a social network. the model used for the regression is based on the steady state equation in the linear degroot model under the influence of stubborn agents, i.e., agents whose opinions are not influenced by their neighbors. this method can be viewed as a \emph{social radar}, where the stubborn agents excite the system and the latter can be estimated through the reverberation observed from the analysis of the agents' opinions. the social network sensing problem can be interpreted as a blind compressed sensing problem with a sparse measurement matrix. we prove that the network structure will be revealed when a sufficient number of stubborn agents independently influence a number of ordinary (non-stubborn) agents. we investigate the scenario with a deterministic or randomized degroot model and propose a consistent estimator of the steady states for the latter scenario. simulation results on synthetic and real world networks support our findings."
"for a class of multiparameter statistical models based on $n^2\times n^2$ braid matrices the eigenvalues of the transfer matrix ${\bf t}^{(r)}$ are obtained explicitly for all $(r,n)$. our formalism yields them as solutions of sets of linear equations with simple constant coefficients. the role of zero-sum multiplets constituted in terms of roots of unity is pointed out and their origin is traced to circular permutations of the indices in the tensor products of basis states induced by our class of ${\bf t}^{(r)}$ matrices. the role of free parameters, increasing as $n^2$ with $n$, is emphasized throughout. spin chain hamiltonians are constructed and studied for all $n$. inverse cayley transforms of yang-baxter matrices corresponding to our braid matrices are obtained for all $n$. they provide potentials for factorizable $s$-matrices. main results are summarized and perspectives are indicated in the concluding remarks."
"we apply the recently developed reduced google matrix algorithm for the analysis of the oecd-wto world network of economic activities. this approach allows to determine interdependences and interactions of economy sectors of several countries, including china, russia and usa, properly taking into account the influence of all other world countries and their economic activities. within this analysis we also obtain the sensitivity of economy sectors and eu countries to petroleum activity sector. we show that this approach takes into account multiplicity of network links with economy interactions between countries and activity sectors thus providing more rich information compared to the usual export-import analysis."
"high resolution magnetic resonance (mr) images are desired for accurate diagnostics. in practice, image resolution is restricted by factors like hardware and processing constraints. recently, deep learning methods have been shown to produce compelling state-of-the-art results for image enhancement/super-resolution. paying particular attention to desired hi-resolution mr image structure, we propose a new regularized network that exploits image priors, namely a low-rank structure and a sharpness prior to enhance deep mr image super-resolution (sr). our contributions are then incorporating these priors in an analytically tractable fashion \color{black} as well as towards a novel prior guided network architecture that accomplishes the super-resolution task. this is particularly challenging for the low rank prior since the rank is not a differentiable function of the image matrix(and hence the network parameters), an issue we address by pursuing differentiable approximations of the rank. sharpness is emphasized by the variance of the laplacian which we show can be implemented by a fixed feedback layer at the output of the network. as a key extension, we modify the fixed feedback (laplacian) layer by learning a new set of training data driven filters that are optimized for enhanced sharpness. experiments performed on publicly available mr brain image databases and comparisons against existing state-of-the-art methods show that the proposed prior guided network offers significant practical gains in terms of improved snr/image quality measures. because our priors are on output images, the proposed method is versatile and can be combined with a wide variety of existing network architectures to further enhance their performance."
"for a compact manifold with boundary $x$ we introduce the   $n$-fold scattering stretched product $x^n_{\text{sc}}$ which is a compact manifold with corners for each $n,$ coinciding with the previously known cases for $n=2,3.$ it is constructed by iterated blow up of boundary faces and boundary faces of multi-diagonals in $x^n.$ the resulting space is shown to map smoothly, by a b-fibration, covering the usual projection, to the lower stretched products. it is anticipated that this manifold with corners, or at least its combinatorial structure, is a universal model for phenomena on asymptotically flat manifolds in which particle clusters emerge at infinity. in particular this is the case for magnetic monopoles on $\mathbb{r}^3$ in which case these spaces are closely related to compactifications of the moduli spaces with the boundary faces mapping to lower charge idealized moduli spaces."
driver drowsiness problem is considered as one of the most important reasons that increases road accidents number. we propose in this paper a new approach for realtime driver drowsiness in order to prevent road accidents. the system uses a smart video camera that takes drivers faces images and supervises the eye blink (open and close) state and head posture to detect the different drowsiness states. face and eye detection are done by viola and jones technique.
"this paper describes asset price and return disturbances as result of relations between transactions and multiple kinds of expectations. we show that disturbances of expectations can cause fluctuations of trade volume, price and return. we model price disturbances for transactions made under all types of expectations as weighted sum of partial price and trade volume disturbances for transactions made under separate kinds of expectations. relations on price allow present return as weighted sum of partial return and trade volume ""return"" for transactions made under separate expectations. dependence of price disturbances on trade volume disturbances as well as dependence of return on trade volume ""return"" cause dependence of volatility and statistical distributions of price and return on statistical properties of trade volume disturbances and trade volume ""return"" respectively."
"in a multicellular organism different cell types express a gene in different amounts. samples from which gene expression levels can be measured typically contain a mixture of different cell types, the resulting measurements thus give only averages over the different cell types present. based on fluctuations in the mixture proportions from sample to sample it is in principle possible to reconstruct the underlying expression levels of each cell type: to deconvolute the sample. we use a statistical mechanics approach to the problem of deconvoluting such partial concentrations from mixed samples, give analytical results for when and how well samples can be unmixed, and suggest an algorithm for sample deconvolution."
"recent studies of massive o-type stars present clear evidences of inhomogeneous and clumped winds. o-type (h-rich) central stars of planetary nebulae (cspns) are in some ways the low mass-low luminosity analogous of those massive stars. in this contribution, we present preliminary results of our on-going multi-wavelength (fuv, uv and optical) study of the winds of galactic cspns. particular emphasis will be given to the clumping factors derived by means of optical lines (halpha and heii4686) and ""classic"" fuv (and uv) lines."
"it is often reasonable to assume that the dependence structure of a bivariate continuous distribution belongs to the class of extreme-value copulas. the latter are characterized by their pickands dependence function. in this paper, a procedure is proposed for testing whether this function belongs to a given parametric family. the test is based on a cram\'{e}r--von mises statistic measuring the distance between an estimate of the parametric pickands dependence function and either one of two nonparametric estimators thereof studied by genest and segers [ann. statist. 37 (2009) 2990--3022]. as the limiting distribution of the test statistic depends on unknown parameters, it must be estimated via a parametric bootstrap procedure, the validity of which is established. monte carlo simulations are used to assess the power of the test and an extension to dependence structures that are left-tail decreasing in both variables is considered."
"under treatment effect heterogeneity, an instrument identifies the instrument-specific local average treatment effect (late). with multiple instruments, two-stage least squares (2sls) estimand is a weighted average of different lates. what is often overlooked in the literature is that the postulated moment condition evaluated at the 2sls estimand does not hold unless those lates are the same. if so, the conventional heteroskedasticity-robust variance estimator would be inconsistent, and 2sls standard errors based on such estimators would be incorrect. i derive the correct asymptotic distribution, and propose a consistent asymptotic variance estimator by using the result of hall and inoue (2003, journal of econometrics) on misspecified moment condition models. this can be used to correctly calculate the standard errors regardless of whether there is more than one late or not."
we give an overview of logical and semantical rules for nonmonotonic and related logics.
"the extremal values of multivariate trigonometric polynomials are of interest in fields ranging from control theory to filter design, but finding the extremal values of such a polynomial is generally np-hard. in this paper, we develop simple and efficiently computable estimates of the extremal values of a multivariate trigonometric polynomial directly from its samples. we provide an upper bound on the modulus of a complex trigonometric polynomial, and develop upper and lower bounds for real trigonometric polynomials. for a univarite polynomial, these bounds are tighter than existing bounds, and the extension to multivariate polynomials is new. as an application, the lower bound provides a sufficient condition to certify global positivity of a real trigonometric polynomial. we use this condition to motivate a new algorithm for multi-dimensional, multirate, perfect reconstruction filter bank design. we demonstrate our algorithm by designing a 2d perfect reconstruction filter bank."
this article focuses on the development of the method for the genetic classification of agamospermous reproduction types in plants using sugar beet as an example. the classification feasibility is ensured by the use of isozymes as genetic markers allowing the identification of all three phenotypic classes in the progeny of individual heterozygous diploid plant and is based on different phenotypic class ratios in the progenies obtained by meiotic and mitotic agamospermy. the data indicate that for sugar beet meiotic agamospermy is the more typical since 13 of 15 explored progenies were classified as those produced by meiotic agamospermy and only 2 as produced by mitotic agamospermy.
"a novel copula-based multivariate panel ordinal model is developed to estimate structural relations among components of well-being. each ordinal time-series is modelled using a copula-based markov model to relate the marginal distributions of the response at each time of observation and then, at each observation time, the conditional distributions of each ordinal time-series are joined using a multivariate t copula. maximum simulated likelihood based on evaluating the multidimensional integrals of the likelihood with randomized quasi monte carlo methods is used for the estimation. asymptotic calculations show that our method is nearly as efficient as maximum likelihood for fully specified multivariate copula models. our findings highlight the importance of one's relative position in evaluating their well-being with no direct effects of socio-economic characteristics on well-being but strong indirect effects through their impact on components of well-being. temporal resilience, habit formation and behavioural traits can explain the dependence in the joint tails over time and across well-being components."
"we consider the problem of fitting a linear model to data held by individuals who are concerned about their privacy. incentivizing most players to truthfully report their data to the analyst constrains our design to mechanisms that provide a privacy guarantee to the participants; we use differential privacy to model individuals' privacy losses. this immediately poses a problem, as differentially private computation of a linear model necessarily produces a biased estimation, and existing approaches to design mechanisms to elicit data from privacy-sensitive individuals do not generalize well to biased estimators. we overcome this challenge through an appropriate design of the computation and payment scheme."
"we introduce a simple model for addressing the controversy in the study of financial systems, sometimes taken as brownian-like processes and other as critical systems with fluctuations of arbitrary magnitude. the model considers a collection of economical agents which establish trade connections among them according to basic economical principles properly translated into physical properties and interaction. with our model we are able to reproduce the evolution of macroscopic quantities (indices) and to correctly retrieve the common exponent value characterizing several indices in financial markets, relating it to the underlying topology of connections."
"we give a uniform geometric realization for the cluster algebra of an arbitrary finite type with principal coefficients at an arbitrary acyclic seed. this algebra is realized as the coordinate ring of a certain reduced double bruhat cell in the simply connected semisimple algebraic group of the same cartan-killing type. in this realization, the cluster variables appear as certain (generalized) principal minors."
"novel concentration inequalities are obtained for the missing mass, i.e. the total probability mass of the outcomes not observed in the sample. we derive distribution-free deviation bounds with sublinear exponents in deviation size for missing mass and improve the results of berend and kontorovich (2013) and yari saeed khanloo and haffari (2015) for small deviations which is the most important case in learning theory."
"in this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. in applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. for experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. in most of the literature on supervised machine learning (e.g. regression trees, random forests, lasso, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. a prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. the challenge is that the ""ground truth"" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. we propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. we then apply the method to a large-scale field experiment re-ranking results on a search engine."
"we consider the `one-shot frame synchronization problem' where a decoder wants to locate a sync pattern at the output of a channel on the basis of sequential observations. we assume that the sync pattern of length n starts being emitted at a random time within some interval of size a, that characterizes the asynchronism level between the transmitter and the receiver. we show that a sequential decoder can optimally locate the sync pattern, i.e., exactly, without delay, and with probability approaching one as n tends to infinity, if and only if the asynchronism level grows as o(exp(n*k)), with k below the `synchronization threshold,' a constant that admits a simple expression depending on the channel. this constant is the same as the one that characterizes the limit for reliable asynchronous communication, as was recently reported by the authors. if k exceeds the synchronization threshold, any decoder, sequential or non-sequential, locates the sync pattern with an error that tends to one as n tends to infinity. hence, a sequential decoder can locate a sync pattern as well as the (non-sequential) maximum likelihood decoder that operates on the basis of output sequences of maximum length a+n-1, but with much fewer observations."
"in genetic studies, not only can the number of predictors obtained from microarray measurements be extremely large, there can also be multiple response variables. motivated by such a situation, we consider semiparametric dimension reduction methods in sparse multivariate regression models. previous studies on joint variable and rank selection have focused on parametric models while here we consider the more challenging varying-coefficient models which make the investigation on nonlinear interactions of variables possible. spline approximation, rank constraints and concave group penalties are utilized for model estimation. asymptotic oracle properties of the estimators are presented. we also propose reduced-rank independent screening to deal with the situation when the dimension is so high that penalized estimation cannot be efficiently applied. in simulations, we show the advantages of simultaneously performing variable and rank selection. a real data set is analyzed to illustrate the good prediction performance when incorporating interactions between genetic variables and an index variable."
"we prove a uniform sobolev inequality for ricci flow, which is independent of the number of surgeries. as an application, under less assumptions, a non-collapsing result stronger than perelman's $\kappa$ non-collapsing with surgery is derived. the proof is shorter and seems more accessible. the result also improves some earlier ones where the sobolev inequality depended on the number of surgeries."
"we present a semi-supervised learning algorithm for learning discrete factor analysis models with arbitrary structure on the latent variables. our algorithm assumes that every latent variable has an ""anchor"", an observed variable with only that latent variable as its parent. given such anchors, we show that it is possible to consistently recover moments of the latent variables and use these moments to learn complete models. we also introduce a new technique for improving the robustness of method-of-moment algorithms by optimizing over the marginal polytope or its relaxations. we evaluate our algorithm using two real-world tasks, tag prediction on questions from the stack overflow website and medical diagnosis in an emergency department."
"managing the prediction of metrics in high-frequency financial markets is a challenging task. an efficient way is by monitoring the dynamics of a limit order book to identify the information edge. this paper describes the first publicly available benchmark dataset of high-frequency limit order markets for mid-price prediction. we extracted normalized data representations of time series data for five stocks from the nasdaq nordic stock market for a time period of ten consecutive days, leading to a dataset of ~4,000,000 time series samples in total. a day-based anchored cross-validation experimental protocol is also provided that can be used as a benchmark for comparing the performance of state-of-the-art methodologies. performance of baseline approaches are also provided to facilitate experimental comparisons. we expect that such a large-scale dataset can serve as a testbed for devising novel solutions of expert systems for high-frequency limit order book data analysis."
"within the framework of lorentz-violating extended electrodynamics, the dirac equation for a bound electron in an external electromagnetic field is considered assuming the interaction with a cpt-odd axial vector background $b_\mu$. the quasi-relativistic hamiltonian is obtained using a $1/c$-series expansion. relativistic dirac eigenstates in a spherically-symmetric potential are found accurate up to the second order in $b_0$. $b_0$-induced cpt-odd corrections to the electromagnetic dipole moment operators of a bound electron are calculated that contribute to the anapole moment of the atomic orbital and may cause a specific asymmetry of the angular distribution of the radiation of a hydrogen atom."
"we summarize here the first results obtained using a technique we recently developed for the noether analysis of hopf-algebra spacetime symmetries, including the derivation of conserved charges for field theories in noncommutative spacetimes of canonical or kappa-minkowski type."
"vortex-antivortex pairs are localized excitations and have been found to be spontaneously created in magnetic elements. in the case that the vortex and the antivortex have opposite polarities the pair has a nonzero topological charge, and it behaves as a rotating vortex dipole. we find theoretically, and confirm numerically, the form of the energy as a function of the angular momentum of the system and the associated rotation frequencies. we discuss the process of annihilation of the pair which changes the topological charge of the system by unity while its energy is monotonically decreasing. such a change in the topological charge affects profoundly the dynamics in the magnetic system. we finally discuss the connection of our results with bloch points (bp) and the implications for bp dynamics."
"in this paper, we consider a statistical problem of learning a linear model from noisy samples. existing work has focused on approximating the least squares solution by using leverage-based scores as an importance sampling distribution. however, no finite sample statistical guarantees and no computationally efficient optimal sampling strategies have been proposed. to evaluate the statistical properties of different sampling strategies, we propose a simple yet effective estimator, which is easy for theoretical analysis and is useful in multitask linear regression. we derive the exact mean square error of the proposed estimator for any given sampling scores. based on minimizing the mean square error, we propose the optimal sampling scores for both estimator and predictor, and show that they are influenced by the noise-to-signal ratio. numerical simulations match the theoretical analysis well."
"deep dynamic generative models are developed to learn sequential dependencies in time-series data. the multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (tsbns), defined as a sequential stack of sigmoid belief networks (sbns). each sbn has a contextual hidden state, inherited from the previous sbns in the sequence, and is used to regulate its hidden bias. scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. this recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences."
"this study explores the potential of crowdfunding as a tool for achieving citizen co-funding of public projects. focusing on philanthropic crowdfunding, we examine whether collaborative projects between public and private organizations are more successful in fundraising than projects initiated solely by private organizations. we argue that government involvement in crowdfunding provides some type of accreditation or certification that attests to a project aim to achieve public rather than private goals, thereby mitigating information asymmetry and improving mutual trust between creators (i.e., private sector organizations) and funders (i.e., crowd). to support this argument, we show that crowdfunding projects with government involvement achieved a greater success rate and attracted a greater amount of funding than comparable projects without government involvement. this evidence shows that governments may take advantage of crowdfunding to co-fund public projects with the citizenry for addressing the complex challenges that we face in the twenty-first century."
"for $\alpha>-1$, let $a^2_{\alpha}$ be the corresponding weighted bergman space of the unit ball in $\mathbb{c}^n$. for a bounded measurable function $f$, let $t_f$ be the toeplitz operator with symbol $f$ on $a^2_{\alpha}$. this paper describes all the functions $f$ for which $t_f$ commutes with a given $t_g$, where $g(z)=z_{1}^{l_1}... z_{n}^{l_n}$ for strictly positive integers $l_1,..., l_n$, or $g(z)=|z_1|^{s_1}... |z_n|^{s_n}h(|z|)$ for non-negative real numbers $s_1,..., s_n$ and a bounded measurable function $h$ on $[0,1)$."
"we address high dimensional covariance estimation for elliptical distributed samples, which are also known as spherically invariant random vectors (sirv) or compound-gaussian processes. specifically we consider shrinkage methods that are suitable for high dimensional problems with a small number of samples (large $p$ small $n$). we start from a classical robust covariance estimator [tyler(1987)], which is distribution-free within the family of elliptical distribution but inapplicable when $n<p$. using a shrinkage coefficient, we regularize tyler's fixed point iterations. we prove that, for all $n$ and $p$, the proposed fixed point iterations converge to a unique limit regardless of the initial condition. next, we propose a simple, closed-form and data dependent choice for the shrinkage coefficient, which is based on a minimum mean squared error framework. simulations demonstrate that the proposed method achieves low estimation error and is robust to heavy-tailed samples. finally, as a real world application we demonstrate the performance of the proposed technique in the context of activity/intrusion detection using a wireless sensor network."
"the differential encoding/decoding setup introduced by kiran et al, oggier-hassibi and jing-jafarkhani for wireless relay networks that use codebooks consisting of unitary matrices is extended to allow codebooks consisting of scaled unitary matrices. for such codebooks to be usable in the jing-hassibi protocol for cooperative diversity, the conditions involving the relay matrices and the codebook that need to be satisfied are identified. using the algebraic framework of extended clifford algebras, a new class of distributed differential space-time codes satisfying these conditions for power of two number of relays and also achieving full cooperative diversity with a low complexity sub-optimal receiver is proposed. simulation results indicate that the proposed codes outperform both the cyclic codes as well as the circulant codes. furthermore, these codes can also be applied as differential space-time codes for non-coherent communication in classical point to point multiple antenna systems."
"exponential random graph models (ergm) behave peculiar in large networks with thousand(s) of actors (nodes). standard models containing two-star or triangle counts as statistics are often unstable leading to completely full or empty networks. moreover, numerical methods break down which makes it complicated to apply ergms to large networks. in this paper we propose two strategies to circumvent these obstacles. first, we fit a model to a subsampled network and secondly, we show how linear statistics (like two-stars etc.) can be replaced by smooth functional components. these two steps in combination allow to fit stable models to large network data, which is illustrated by a data example including a residual analysis."
previously derived expressions for the characteristic function of work performed on a quantum system by a classical external force are generalized to arbitrary initial states of the considered system and to hamiltonians with degenerate spectra. in the particular case of microcanonical initial states explicit expressions for the characteristic function and the corresponding probability density of work are formulated. their classical limit as well as their relations to the respective canonical expressions are discussed. a fluctuation theorem is derived that expresses the ratio of probabilities of work for a process and its time reversal to the ratio of densities of states of the microcanonical equilibrium systems with corresponding initial and final hamiltonians.from this crooks-type fluctuation theorem a relation between entropies of different systems can be derived which does not involve the time reversed process. this entropy-from-work theorem provides an experimentally accessible way to measure entropies.
"in this paper, we study the sensitivity of the spectral clustering based community detection algorithm subject to a erdos-renyi type random noise model. we prove phase transitions in community detectability as a function of the external edge connection probability and the noisy edge presence probability under a general network model where two arbitrarily connected communities are interconnected by random external edges. specifically, the community detection performance transitions from almost perfect detectability to low detectability as the inter-community edge connection probability exceeds some critical value. we derive upper and lower bounds on the critical value and show that the bounds are identical when the two communities have the same size. the phase transition results are validated using network simulations. using the derived expressions for the phase transition threshold we propose a method for estimating this threshold from observed data."
"competing firms can increase profits by setting prices collectively, imposing significant costs on consumers. such groups of firms are known as cartels and because this behavior is illegal, their operations are secretive and difficult to detect. cartels feel a significant internal obstacle: members feel short-run incentives to cheat. here we present a network-based framework to detect potential cartels in bidding markets based on the idea that the chance a group of firms can overcome this obstacle and sustain cooperation depends on the patterns of its interactions. we create a network of firms based on their co-bidding behavior, detect interacting groups, and measure their cohesion and exclusivity, two group-level features of their collective behavior. applied to a market for school milk, our method detects a known cartel and calculates that it has high cohesion and exclusivity. in a comprehensive set of nearly 150,000 public contracts awarded by the republic of georgia from 2011 to 2016, detected groups with high cohesion and exclusivity are significantly more likely to display traditional markers of cartel behavior. we replicate this relationship between group topology and the emergence of cooperation in a simulation model. our method presents a scalable, unsupervised method to find groups of firms in bidding markets ideally positioned to form lasting cartels."
"some scientific publications are under suspicion of fabrication of data. since humans are bad random number generators, there might be some evidential value in favor of fabrication in the statistical results as presented in such papers. in line with uri simonsohn (2012, 2013) we study the evidential value of the results of an anova study in favor of the hypothesis of a dependence structure in the underlying data."
"we formalize the argument that political disagreements can be traced to a ""clash of narratives"". drawing on the ""bayesian networks"" literature, we model a narrative as a causal model that maps actions into consequences, weaving a selection of other random variables into the story. an equilibrium is defined as a probability distribution over narrative-policy pairs that maximizes a representative agent's anticipatory utility, capturing the idea that public opinion favors hopeful narratives. our equilibrium analysis sheds light on the structure of prevailing narratives, the variables they involve, the policies they sustain and their contribution to political polarization."
"mutation rates and fitness costs of deleterious mutations are difficult to measure in vivo but essential for a quantitative understanding of evolution. using whole genome deep sequencing data from longitudinal samples during untreated hiv-1 infection, we estimated mutation rates and fitness costs in hiv-1 from the temporal dynamics of genetic variation. at approximately neutral sites, mutations accumulate with a rate of 1.2 x 10^-5 per site per day, in agreement with the rate measured in cell cultures. the rate from g to a is largest, followed by the other transitions c to t, t to c, and a to g, while transversions are more rare. at non-neutral sites, most mutations reduce virus replication; using a model of mutation selection balance, we estimated the fitness cost of mutations at every site in the hiv-1 genome. about half of all nonsynonymous mutations have large fitness costs (greater than 10\%), while most synonymous mutations have costs below 1\%. the cost of synonymous mutations is especially low in most of gag and pol, while much higher costs are observed in important rna structures and regulatory regions. the intrapatient fitness cost estimates are consistent across multiple patients, suggesting that the deleterious part of the fitness landscape is universal and explains a large fraction of global hiv-1 group m diversity."
"one of the most efficient methods for the evaluation of the vacuum expectation values for physical observables in the casimir effect is based on using the abel-plana summation formula. this enables to derive the renormalized quantities in a manifestly cutoff independent way and to present them in the form of strongly convergent integrals. however, applications of the abel-plana formula, in its usual form, are restricted by simple geometries when the eigenmodes have a simple dependence on quantum numbers. the author generalized the abel-plana formula which essentially enlarges its application range. based on this generalization, formulae have been obtained for various types of series over the zeros of combinations of bessel functions and for integrals involving these functions. it has been shown that these results generalize the special cases existing in literature. further, the derived summation formulae have been used to summarize series arising in the direct mode summation approach to the casimir effect for spherically and cylindrically symmetric boundaries, for boundaries moving with uniform proper acceleration, and in various braneworld scenarios. this allows to extract from the vacuum expectation values of local physical observables the parts corresponding to the geometry without boundaries and to present the boundary-induced parts in terms of integrals strongly convergent for the points away from the boundaries. as a result, the renormalization procedure for these observables is reduced to the corresponding procedure for bulks without boundaries. the present paper reviews these results. we also aim to collect the results on vacuum expectation values for local physical observables such as the field square and the energy-momentum tensor in manifolds with boundaries for various bulk and boundary geometries."
"pair creation supernovae (pcsn) are thought to be produced from very massive low metallicity stars. the spectacularly bright sn 2006gy does show signatures expected from pcsne. here, we investigate the metallicity threshold below which pcsn can form and estimate their occurrence rate. we perform stellar evolution calculations for stars of 150$\mso$ and 250$\mso$ of low metallicity (z$_{\odot}$/5 and z$_{\odot}$/20), and analyze their mass loss rates. we find that the bifurcation between quasi-chemically homogeneous evolution for fast rotation and conventional evolution for slower rotation, which has been found earlier for massive low metallicity stars, persists in the mass range considered here. consequently, there are two separate pcsn progenitor types: (i) fast rotators produce pcsne from very massive wolf-rayet stars, and (ii) slower rotators that generate pcsne in hydrogen-rich massive yellow hypergiants. we find that hydrogen-rich pcsne could occur at metallicities as high as z$_{\odot}$/3, which -- assuming standard imfs are still valid to estimate their birth rates -- results in a rate of about one pcsn per 1000 supernovae in the local universe, and one pcsn per 100 supernovae at a redshift of $z=5$. pcsne from wc-type wolf-rayet stars are restricted to much lower metallicity."
"the rapid worldwide spread of severe viral infections, often involving novel modifications of viruses, poses major challenges to our health care systems. this means that tools that can efficiently and specifically diagnose viruses are much needed. to be relevant for a broad application in local health care centers, such tools should be relatively cheap and easy to use. here we discuss the biophysical potential for the macroscopic detection of viruses based on the induction of a mechanical stress in a bundle of pre-stretched dna molecules upon binding of viruses to the dna. we show that the affinity of the dna to the charged virus surface induces a local melting of the double-helix into two single-stranded dna. this process effects a mechanical stress along the dna chains leading to an overall contraction of the dna. our results suggest that when such dna bundles are incorporated in a supporting matrix such as a responsive hydrogel, the presence of viruses may indeed lead to a significant, macroscopic mechanical deformation of the matrix. we discuss the biophysical basis for this effect and characterize the physical properties of the associated dna melting transition. in particular, we reveal several scaling relations between the relevant physical parameters of the system. we promote this dna-based assay for efficient and specific virus screening."
"sparse representations have proven their efficiency in solving a wide class of inverse problems encountered in signal and image processing. conversely, enforcing the information to be spread uniformly over representation coefficients exhibits relevant properties in various applications such as digital communications. anti-sparse regularization can be naturally expressed through an $\ell_{\infty}$-norm penalty. this paper derives a probabilistic formulation of such representations. a new probability distribution, referred to as the democratic prior, is first introduced. its main properties as well as three random variate generators for this distribution are derived. then this probability distribution is used as a prior to promote anti-sparsity in a gaussian linear inverse problem, yielding a fully bayesian formulation of anti-sparse coding. two markov chain monte carlo (mcmc) algorithms are proposed to generate samples according to the posterior distribution. the first one is a standard gibbs sampler. the second one uses metropolis-hastings moves that exploit the proximity mapping of the log-posterior distribution. these samples are used to approximate maximum a posteriori and minimum mean square error estimators of both parameters and hyperparameters. simulations on synthetic data illustrate the performances of the two proposed samplers, for both complete and over-complete dictionaries. all results are compared to the recent deterministic variational fitra algorithm."
"we study finite set-theoretic solutions $(x,r)$ of the yang-baxter equation of square-free multipermutation type. we show that each such solution over $\c$ with multipermutation level two can be put in diagonal form with the associated yang-baxter algebra $\acal(\c,x,r)$ having a $q$-commutation form of relations determined by complex phase factors. these complex factors are roots of unity and all roots of a prescribed form appear as determined by the representation theory of finite abelian group $\gcal$ of left actions on $x$. we study the structure of $\acal(\c,x,r)$ and show that they have a $\bullet$-product form `quantizing' the commutative algebra of polynomials in $|x|$ variables. we obtain the $\bullet$-product both as a drinfeld cotwist for a certain canonical 2-cocycle and as a braided-opposite product for a certain crossed $\gcal$-module (over any field $k$). we provide first steps in the noncommutative differential geometry of $\acal(k,x,r)$ arising from these results. as a byproduct of our work we find that every such level 2 solution $(x,r)$ factorises as $r=f\circ\tau\circ f^{-1}$ where $\tau$ is the flip map and $(x,f)$ is another solution coming from $x$ as a crossed $\gcal$-set."
"in this work, we address the problem of solving a series of underdetermined linear inverse problems subject to a sparsity constraint. we generalize the spike-and-slab prior distribution to encode a priori correlation of the support of the solution in both space and time by imposing a transformed gaussian process on the spike-and-slab probabilities. an expectation propagation (ep) algorithm for posterior inference under the proposed model is derived. for large scale problems, the standard ep algorithm can be prohibitively slow. we therefore introduce three different approximation schemes to reduce the computational complexity. finally, we demonstrate the proposed model using numerical experiments based on both synthetic and real data sets."
"in the setting of high-dimensional linear models with gaussian noise, we investigate the possibility of confidence statements connected to model selection. although there exist numerous procedures for adaptive point estimation, the construction of adaptive confidence regions is severely limited (cf. li, 1989). the present paper sheds new light on this gap. we develop exact and adaptive confidence sets for the best approximating model in terms of risk. one of our constructions is based on a multiscale procedure and a particular coupling argument. utilizing exponential inequalities for noncentral chi-squared distributions, we show that the risk and quadratic loss of all models within our confidence region are uniformly bounded by the minimal risk times a factor close to one."
"we consider the correction $\delta\sigma_\mathrm{ee}$ due to electron-electron interaction to the conductivity of a weakly disordered metal (al'tshuler-aronov correction). the correction is related to the spectral determinant of the laplace operator. the case of a large square metallic network is considered. the variation of $\delta\sigma_\mathrm{ee}(l_t)$ as a function of the thermal length $l_t$ is found very similar to the variation of the weak localization $\delta\sigma_\mathrm{wl}(l_\phi)$ as a function of the phase coherence length. our result for $\delta\sigma_\mathrm{ee}$ interpolates between the known 1d and 2d results, but the interaction parameter entering the expression of $\delta\sigma_\mathrm{ee}$ keeps a 1d behaviour. quite surprisingly, the result is very close to the 2d logarithmic behaviour already for $l_t\sim{a}/2$, where $a$ is the lattice parameter."
"we propose the supervised hierarchical dirichlet process (shdp), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. we compare the shdp with another leading method for regression on grouped data, the supervised latent dirichlet allocation (slda) model. we evaluate our method on two real-world classification problems and two real-world regression problems. bayesian nonparametric regression models based on the dirichlet process, such as the dirichlet process-generalised linear models (dp-glm) have previously been explored; these models allow flexibility in modelling nonlinear relationships. however, until now, hierarchical dirichlet process (hdp) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the hdp on the grouped data results in learnt clusters that are not predictive of the responses. the shdp solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group."
"higher criticism is a method for detecting signals that are both sparse and weak. although first proposed in cases where the noise variables are independent, higher criticism also has reasonable performance in settings where those variables are correlated. in this paper we show that, by exploiting the nature of the correlation, performance can be improved by using a modified approach which exploits the potential advantages that correlation has to offer. indeed, it turns out that the case of independent noise is the most difficult of all, from a statistical viewpoint, and that more accurate signal detection (for a given level of signal sparsity and strength) can be obtained when correlation is present. we characterize the advantages of correlation by showing how to incorporate them into the definition of an optimal detection boundary. the boundary has particularly attractive properties when correlation decays at a polynomial rate or the correlation matrix is toeplitz."
"this paper is concerned with statistical methods for the segmental classification of linear sequence data where the task is to segment and classify the data according to an underlying hidden discrete state sequence. such analysis is commonplace in the empirical sciences including genomics, finance and speech processing. in particular, we are interested in answering the following question: given data $y$ and a statistical model $\pi(x,y)$ of the hidden states $x$, what should we report as the prediction $\hat{x}$ under the posterior distribution $\pi (x|y)$? that is, how should you make a prediction of the underlying states? we demonstrate that traditional approaches such as reporting the most probable state sequence or most probable set of marginal predictions can give undesirable classification artefacts and offer limited control over the properties of the prediction. we propose a decision theoretic approach using a novel class of markov loss functions and report $\hat{x}$ via the principle of minimum expected loss (maximum expected utility). we demonstrate that the sequence of minimum expected loss under the markov loss function can be enumerated exactly using dynamic programming methods and that it offers flexibility and performance improvements over existing techniques. the result is generic and applicable to any probabilistic model on a sequence, such as hidden markov models, change point or product partition models."
"investigating the pleiotropic effects of genetic variants can increase statistical power, provide important information to achieve deep understanding of the complex genetic structures of disease, and offer powerful tools for designing effective treatments with fewer side effects. however, the current multiple phenotype association analysis paradigm lacks breadth (number of phenotypes and genetic variants jointly analyzed at the same time) and depth (hierarchical structure of phenotype and genotypes). a key issue for high dimensional pleiotropic analysis is to effectively extract informative internal representation and features from high dimensional genotype and phenotype data. to explore multiple levels of representations of genetic variants, learn their internal patterns involved in the disease development, and overcome critical barriers in advancing the development of novel statistical methods and computational algorithms for genetic pleiotropic analysis, we proposed a new framework referred to as a quadratically regularized functional cca (qrfcca) for association analysis which combines three approaches: (1) quadratically regularized matrix factorization, (2) functional data analysis and (3) canonical correlation analysis (cca). large-scale simulations show that the qrfcca has a much higher power than that of the nine competing statistics while retaining the appropriate type 1 errors. to further evaluate performance, the qrfcca and nine other statistics are applied to the whole genome sequencing dataset from the twinsuk study. we identify a total of 79 genes with rare variants and 67 genes with common variants significantly associated with the 46 traits using qrfcca. the results show that the qrfcca substantially outperforms the nine other statistics."
"defining subtypes of complex diseases such as cancer and stratifying patient groups with the same disease but different subtypes for targeted treatments is important for personalized and precision medicine. approaches that incorporate multi-omic data are more advantageous to those using only one data type for patient clustering and disease subtype discovery. however, it is challenging to integrate multi-omic data as they are heterogeneous and noisy. in this paper, we present affinity network fusion (anf) to integrate multi-omic data for patient clustering. anf first constructs patient affinity networks for each omic data type, and then calculates a fused network for spectral clustering. we applied anf to a processed harmonized cancer dataset downloaded from gdc data portal consisting of 2193 patients, and generated promising results on clustering patients into correct disease types. moreover, we developed a semi-supervised model combining anf and neural network for few-shot learning. in several cases, the model can achieve greater than 90% acccuracy on test set with training less than 1% of the data. this demonstrates the power of anf in learning a good representation of patients, and shows the great potential of semi-supervised learning in cancer patient clustering."
"protein synthesis-dependent, late long-term potentiation (ltp) and depression (ltd) at glutamatergic hippocampal synapses are well characterized examples of long-term synaptic plasticity. persistent increased activity of the enzyme protein kinase m (pkm) is thought essential for maintaining ltp. additional spatial and temporal features that govern ltp and ltd induction are embodied in the synaptic tagging and capture (stc) and cross capture hypotheses. only synapses that have been ""tagged"" by an stimulus sufficient for ltp and learning can ""capture"" pkm. a model was developed to simulate the dynamics of key molecules required for ltp and ltd. the model concisely represents relationships between tagging, capture, ltd, and ltp maintenance. the model successfully simulated ltp maintained by persistent synaptic pkm, stc, ltd, and cross capture, and makes testable predictions concerning the dynamics of pkm. the maintenance of ltp, and consequently of at least some forms of long-term memory, is predicted to require continual positive feedback in which pkm enhances its own synthesis only at potentiated synapses. this feedback underlies bistability in the activity of pkm. second, cross capture requires the induction of ltd to induce dendritic pkm synthesis, although this may require tagging of a nearby synapse for ltp. the model also simulates the effects of pkm inhibition, and makes additional predictions for the dynamics of cam kinases. experiments testing the above predictions would significantly advance the understanding of memory maintenance."
"sophisticated malware authors can sneak hidden malicious code into portable executable files, and this code can be hard to detect, especially if encrypted or compressed. however, when an executable file switches between code regimes (e.g. native, encrypted, compressed, text, and padding), there are corresponding shifts in the file's representation as an entropy signal. in this paper, we develop a method for automatically quantifying the extent to which patterned variations in a file's entropy signal make it ""suspicious."" in experiment 1, we use wavelet transforms to define a suspiciously structured entropic change score (ssecs), a scalar feature that quantifies the suspiciousness of a file based on its distribution of entropic energy across multiple levels of spatial resolution. based on this single feature, it was possible to raise predictive accuracy on a malware detection task from 50.0% to 68.7%, even though the single feature was applied to a heterogeneous corpus of malware discovered ""in the wild."" in experiment 2, we describe how wavelet-based decompositions of software entropy can be applied to a parasitic malware detection task involving large numbers of samples and features. by extracting only string and entropy features (with wavelet decompositions) from software samples, we are able to obtain almost 99% detection of parasitic malware with fewer than 1% false positives on good files. moreover, the addition of wavelet-based features uniformly improved detection performance across plausible false positive rates, both in a strings-only model (e.g., from 80.90% to 82.97%) and a strings-plus-entropy model (e.g. from 92.10% to 94.74%, and from 98.63% to 98.90%). overall, wavelet decomposition of software entropy can be useful for machine learning models for detecting malware based on extracting millions of features from executable files."
"the lin-maldacena geometries are nonsingular gravity duals to degenerate vacuum states of a family of field theories with su(2|4) supersymmetry. in this note, we show that at large n, where the number of vacuum states is large, there is a natural `macroscopic' description of typical states, giving rise to a set of coarse-grained geometries. for a given coarse-grained state, we can associate an entropy related to the number of underlying microstates. we find a simple formula for this entropy in terms of the data that specify the geometry. we see that this entropy function is zero for the original microstate geometries and maximized for a certain ``typical state'' geometry, which we argue is the gravity dual to the zero-temperature limit of the thermal state of the corresponding field theory. finally, we note that the coarse-grained geometries are singular if and only if the entropy function is non-zero."
"we study the following class of scalar hyperbolic conservation laws with discontinuous fluxes: \partial_t\rho+\partial_xf(x,\rho)=0. the main feature of such a conservation law is the discontinuity of the flux function in the space variable x. kruzkov's approach for the l1-contraction does not apply since it requires the lipschitz continuity of the flux function; and entropy solutions even for the riemann problem are not unique under the classical entropy conditions. on the other hand, it is known that, in statistical mechanics, some microscopic interacting particle systems with discontinuous speed parameter lambda(x), in the hydrodynamic limit, formally lead to scalar hyperbolic conservation laws with discontinuous fluxes of the form: \partial_t\rho+\partial_x(\lambda(x)h(\rho))=0. the natural question arises which entropy solutions the hydrodynamic limit selects, thereby leading to a suitable, physical relevant notion of entropy solutions of this class of conservation laws. this paper is a first step and provides an answer to this question for a family of discontinuous flux functions. in particular, we identify the entropy condition for our pde and proceed to show the well-posedness by combining our existence result with a uniqueness result of audusse-perthame (2005) for the family of flux functions; we establish a compactness framework for the hydrodynamic limit of large particle systems and the convergence of other approximate solutions to our pde, which is based on the notion and reduction of measure-valued entropy solutions; and we finally establish the hydrodynamic limit for a zrp with discontinuous speed-parameter governed by an entropy solution to our pde."
we present data that show a cycling transition can be used to detect and image metastable he$_2$ triplet molecules in superfluid helium. we demonstrate that limitations on the cycling efficiency due to the vibrational structure of the molecule can be mitigated by the use of repumping lasers. images of the molecules obtained using the method are also shown. this technique gives rise to a new kind of ionizing radiation detector. the use of he$_2$ triplet molecules as tracer particles in the superfluid promises to be a powerful tool for visualization of both quantum and classical turbulence in liquid helium.
"recent approaches based on artificial neural networks (anns) have shown promising results for short-text classification. however, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ann-based systems do not leverage the preceding short texts when classifying a subsequent one. in this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. our model achieves state-of-the-art results on three different datasets for dialog act prediction."
"this paper identifies a problem with the usual procedure for l2-regularization parameter estimation in a domain adaptation setting. in such a setting, there are differences between the distributions generating the training data (source domain) and the test data (target domain). the usual cross-validation procedure requires validation data, which can not be obtained from the unlabeled target data. the problem is that if one decides to use source validation data, the regularization parameter is underestimated. one possible solution is to scale the source validation data through importance weighting, but we show that this correction is not sufficient. we conclude the paper with an empirical analysis of the effect of several importance weight estimators on the estimation of the regularization parameter."
"theoretical analysis, which maps single molecule time trajectories of a molecular motor onto unicyclic markov processes, allows us to evaluate the heat dissipated from the motor and to elucidate its dependence on the mean velocity and diffusivity. unlike passive brownian particles in equilibrium, the velocity and diffusion constant of molecular motors are closely inter-related to each other. in particular, our study makes it clear that the increase of diffusivity with the heat production is a natural outcome of active particles, which is reminiscent of the recent experimental premise that the diffusion of an exothermic enzyme is enhanced by the heat released from its own catalytic turnover. compared with freely diffusing exothermic enzymes, kinesin-1 whose dynamics is confined on one-dimensional tracks is highly efficient in transforming conformational fluctuations into a locally directed motion, thus displaying a significantly higher enhancement in diffusivity with its turnover rate. putting molecular motors and freely diffusing enzymes on an equal footing, our study offers thermodynamic basis to understand the heat enhanced self-diffusion of exothermic enzymes."
"within the preprocessing pipeline of a next generation sequencing sample, its set of single-base mismatches is one of the first outcomes, together with the number of correctly aligned reads. the union of these two sets provides a 4x4 matrix (called single base indicator, sbi in what follows) representing a blueprint of the sample and its preprocessing ingredients such as the sequencer, the alignment software, the pipeline parameters. in this note we show that, under the same technological conditions, there is a strong relation between the sbi and the biological nature of the sample. to reach this goal we need to introduce a similarity measure between sbis: we also show how two measures commonly used in machine learning can be of help in this context."
"probabilistic k-nearest neighbour (pknn) classification has been introduced to improve the performance of original k-nearest neighbour (knn) classification algorithm by explicitly modelling uncertainty in the classification of each feature vector. however, an issue common to both knn and pknn is to select the optimal number of neighbours, $k$. the contribution of this paper is to incorporate the uncertainty in $k$ into the decision making, and in so doing use bayesian model averaging to provide improved classification. indeed the problem of assessing the uncertainty in $k$ can be viewed as one of statistical model selection which is one of the most important technical issues in the statistics and machine learning domain. in this paper, a new functional approximation algorithm is proposed to reconstruct the density of the model (order) without relying on time consuming monte carlo simulations. in addition, this algorithm avoids cross validation by adopting bayesian framework. the performance of this algorithm yielded very good performance on several real experimental datasets."
"we describe a folklore construction of an exceptional representation of sp(4,f_q). this representation has the following remarkable combination of properties, namely it is cuspidal, degenerate and unipotent."
"prediction markets show considerable promise for developing flexible mechanisms for machine learning. here, machine learning markets for multivariate systems are defined, and a utility-based framework is established for their analysis. this differs from the usual approach of defining static betting functions. it is shown that such markets can implement model combination methods used in machine learning, such as product of expert and mixture of expert approaches as equilibrium pricing models, by varying agent utility functions. they can also implement models composed of local potentials, and message passing methods. prediction markets also allow for more flexible combinations, by combining multiple different utility functions. conversely, the market mechanisms implement inference in the relevant probabilistic models. this means that market mechanism can be utilized for implementing parallelized model building and inference for probabilistic modelling."
"we characterize when a convex risk measure associated to a law-invariant acceptance set in $l^\infty$ can be extended to $l^p$, $1\leq p<\infty$, preserving finiteness and continuity. this problem is strongly connected to the statistical robustness of the corresponding risk measures. special attention is paid to concrete examples including risk measures based on expected utility, max-correlation risk measures, and distortion risk measures."
"one of the most unique physical features of cell adhesion to external surfaces is the active generation of mechanical force at the cell-material interface. this includes pulling forces generated by contractile polymer bundles and networks, and pushing forces generated by the polymerization of polymer networks. these forces are transmitted to the substrate mainly by focal adhesions, which are large, yet highly dynamic adhesion clusters. tissue cells use these forces to sense the physical properties of their environment and to communicate with each other. the effect of forces is intricately linked to the material properties of cells and their physical environment. here a review is given of recent progress in our understanding of the role of forces in cell adhesion from the viewpoint of theoretical soft matter physics and in close relation to the relevant experiments."
"adaptive trials are now mainstream science. recently, researchers have taken the adaptive trial concept to its natural conclusion, proposing what we call ""global cumulative treatment analysis"" (gcta). similar to the adaptive trial, decision making and data collection and analysis in the gcta are continuous and integrated, and treatments are ranked in accord with the statistics of this information, combined with what offers the most information gain. where gcta differs from an adaptive trial, or, for that matter, from any trial design, is that all patients are implicitly participants in the gcta process, regardless of whether they are formally enrolled in a trial. this paper discusses some of the theoretical and practical issues that arise in the design of a gcta, along with some preliminary thoughts on how they might be approached."
"cells sense and predict their environment via energy-dissipating pathways. however, it is unclear whether dissipation helps or harms prediction. here we study dissipation and prediction for a minimal sensory module of receptors that reversibly bind ligand. we find that the module performs short-term prediction optimally when operating in an adiabatic regime where dissipation vanishes. in contrast, beyond a critical forecast interval, prediction becomes most precise in a regime of maximal dissipation, suggesting that dissipative sensing in biological systems can serve to enhance prediction performance."
"the cross covariogram g_{k,l} of two convex sets k, l in r^n is the function which associates to each x in r^n the volume of the intersection of k with l+x.   the problem of determining the sets from their covariogram is relevant in stochastic geometry, in probability and it is equivalent to a particular case of the phase retrieval problem in fourier analysis. it is also relevant for the inverse problem of determining the atomic structure of a quasicrystal from its x-ray diffraction image.   the two main results of this paper are that g_{k,k} determines three-dimensional convex polytopes k and that g_{k,l} determines both k and l when k and l are convex polyhedral cones satisfying certain assumptions. these results settle a conjecture of g. matheron in the class of convex polytopes.   further results regard the known counterexamples in dimension n>=4. we also introduce and study the notion of synisothetic polytopes. this concept is related to the rearrangement of the faces of a convex polytope."
"bone remodelling maintains the functionality of skeletal tissue by locally coordinating bone-resorbing cells (osteoclasts) and bone-forming cells (osteoblasts) in the form of bone multicellular units (bmus). understanding the emergence of such structured units out of the complex network of biochemical interactions between bone cells is essential to extend our fundamental knowledge of normal bone physiology and its disorders. to this end, we propose a spatio-temporal continuum model that integrates some of the most important interaction pathways currently known to exist between cells of the osteoblastic and osteoclastic lineage. this mathematical model allows us to test the significance and completeness of these pathways based on their ability to reproduce the spatio-temporal dynamics of individual bmus. we show that under suitable conditions, the experimentally-observed structured cell distribution of cortical bmus is retrieved. the proposed model admits travelling-wave-like solutions for the cell densities with tightly organised profiles, corresponding to the progression of a single remodelling bmu. the shapes of these spatial profiles within the travelling structure can be linked to the intrinsic parameters of the model such as differentiation and apoptosis rates for bone cells. in addition to the cell distribution, the spatial distribution of regulatory factors can also be calculated. this provides new insights on how different regulatory factors exert their action on bone cells leading to cellular spatial and temporal segregation, and functional coordination."
"we study generalized bootstrap confidence regions for the mean of a random vector whose coordinates have an unknown dependency structure. the random vector is supposed to be either gaussian or to have a symmetric and bounded distribution. the dimensionality of the vector can possibly be much larger than the number of observations and we focus on a nonasymptotic control of the confidence level, following ideas inspired by recent results in learning theory. we consider two approaches, the first based on a concentration principle (valid for a large class of resampling weights) and the second on a resampled quantile, specifically using rademacher weights. several intermediate results established in the approach based on concentration principles are of interest in their own right. we also discuss the question of accuracy when using monte carlo approximations of the resampled quantities."
"we introduce 'mixed licors', an algorithm for learning nonlinear, high-dimensional dynamics from spatio-temporal data, suitable for both prediction and simulation. mixed licors extends the recent licors algorithm (goerg and shalizi, 2012) from hard clustering of predictive distributions to a non-parametric, em-like soft clustering. this retains the asymptotic predictive optimality of licors, but, as we show in simulations, greatly improves out-of-sample forecasts with limited data. the new method is implemented in the publicly-available r package ""licors"" (http://cran.r-project.org/web/packages/licors/)."
"we provide an axiomatic foundation for the representation of num\'{e}raire-invariant preferences of economic agents acting in a financial market. in a static environment, the simple axioms turn out to be equivalent to the following choice rule: the agent prefers one outcome over another if and only if the expected (under the agent's subjective probability) relative rate of return of the latter outcome with respect to the former is nonpositive. with the addition of a transitivity requirement, this last preference relation has an extension that can be numerically represented by expected logarithmic utility. we also treat the case of a dynamic environment where consumption streams are the objects of choice. there, a novel result concerning a canonical representation of unit-mass optional measures enables us to explicitly solve the investment--consumption problem by separating the two aspects of investment and consumption. finally, we give an application to the problem of optimal num\'{e}raire investment with a random time-horizon."
"we describe a model for capturing the statistical structure of local amplitude and local spatial phase in natural images. the model is based on a recently developed, factorized third-order boltzmann machine that was shown to be effective at capturing higher-order structure in images by modeling dependencies among squared filter outputs (ranzato and hinton, 2010). here, we extend this model to $l_p$-spherically symmetric subspaces. in order to model local amplitude and phase structure in images, we focus on the case of two dimensional subspaces, and the $l_2$-norm. when trained on natural images the model learns subspaces resembling quadrature-pair gabor filters. we then introduce an additional set of hidden units that model the dependencies among subspace phases. these hidden units form a combinatorial mixture of phase coupling distributions, concentrated in the sum and difference of phase pairs. when adapted to natural images, these distributions capture local spatial phase structure in natural images."
"proteins have evolved to perform diverse cellular functions, from serving as reaction catalysts to coordinating cellular propagation and development. frequently, proteins do not exert their full potential as monomers but rather undergo concerted interactions as either homo-oligomers or with other proteins as hetero-oligomers. the experimental study of such protein complexes and interactions has been arduous. theoretical structure prediction methods are an attractive alternative. here, we investigate homo-oligomeric interfaces by tracing residue coevolution via the global statistical direct coupling analysis (dca). dca can accurately infer spatial adjacencies between residues. these adjacencies can be included as constraints in structure-prediction techniques to predict high-resolution models. by taking advantage of the on-going exponential growth of sequence databases, we go significantly beyond anecdotal cases of a few protein families and apply dca to a systematic large-scale study of nearly 2000 pfam protein families with sufficient sequence information and structurally resolved homo-oligomeric interfaces. we find that large interfaces are commonly identified by dca. we further demonstrate that dca can differentiate between subfamilies of different binding modes within one large pfam family. sequence derived contact information for the subfamilies proves sufficient to assemble accurate structural models of the diverse protein-oligomers. thus, we provide an approach to investigate oligomerization for arbitrary protein families leading to structural models complementary to often difficult experimental methods. combined with ever more abundant sequential data, we anticipate that this study will be instrumental to allow the structural description of many hetero-protein complexes in the future."
"we give an elementary proof of the analytic kam theorem by reducing it to a picard iteration of a pde with quadratic nonlinearity, the so called polchinski renormalization group equation studied in quantum field theory."
conventional deep brain stimulation (dbs) of basal ganglia uses high-frequency regular electrical pulses to treat parkinsonian motor symptoms and has a series of limitations. relatively new and not yet clinically tested optogenetic stimulation is an effective experimental stimulation technique to affect pathological network dynamics. we compared the effects of electrical and optogenetic stimulation of the basal ganglia on the pathological parkinsonian rhythmic neural activity. we studied the network response to electrical stimulation and excitatory and inhibitory optogenetic stimulations. different stimulations exhibit different interactions with pathological activity in the network. we studied these interactions for different network and stimulation parameter values. optogenetic stimulation was found to be more efficient than electrical stimulation in suppressing pathological rhythmicity. our findings indicate that optogenetic control of neural synchrony may be more efficacious than electrical control because of the different ways of how stimulations interact with network dynamics.
"living cells move thanks to assemblies of actin filaments and myosin motors that range from very organized striated muscle tissue to disordered intracellular bundles. the mechanisms powering these disordered structures are debated, and all models studied so far predict that they are contractile. we reexamine this prediction through a theoretical treatment of the interplay of three well-characterized internal dynamical processes in actomyosin bundles: actin treadmilling, the attachement-detachment dynamics of myosin and that of crosslinking proteins. we show that these processes enable an extensive control of the bundle's active mechanics, including reversals of the filaments' apparent velocities and the possibility of generating extension instead of contraction. these effects offer a new perspective on well-studied in vivo systems, as well as a robust criterion to experimentally elucidate the underpinnings of actomyosin activity."
"the extraction of membrane tubes by molecular motors is known to play an important role for the transport properties of eukaryotic cells. by studying a generic class of models for the tube extraction, we discover a rich phase diagram. in particular we show that the density of motors along the tube can exhibit shocks, inverse shocks and plateaux, depending on parameters which could in principle be probed experimentally. in addition the phase diagram exhibits interesting reentrant behavior."
"in this paper, we show that the presence of nonlinear coupling between time series may be detected employing kernel feature space representations alone dispensing with the need to go back to solve the pre-image problem to gauge model adequacy. as a consequence, the canonical methodology for model construction, diagnostics, and granger connectivity inference applies with no change other than computation using kernels in lieu of second-order moments."
"we present a `new generation' model for high energy proton-proton `soft' interactions. it allows for a full set of multi-pomeron vertices, as well as including multichannel eikonal scattering. it describes the behaviour of the proton-proton total, sigma(total), and elastic dsigma(el)/dt, cross sections together with those for low and high mass proton dissociation. although the model contains a comprehensive set of multi-pomeron diagrams, it has a simple partonic interpretation. including the more complicated multi-pomeron vertices reduces the absorptive effects as compared to the predictions in which only the triple-pomeron vertex is considered. tuning the model to describe the available `soft' data in the cern isr - tevatron energy range, we predict the total, elastic, single- and double-diffractive dissociation cross sections at the lhc energy. an inescapable consequence of including multichannel eikonal and multi-pomeron effects is that the total cross section is expected to be lower than before: indeed, we find sigma(total) \simeq 90 mb at the lhc energy. we also present differential forms of the cross sections. in addition we calculate soft diffractive central production."
"blind source separation (bss) is a signal processing tool, which is widely used in various fields. examples include biomedical signal separation, brain imaging and economic time series applications. in bss, one assumes that the observed $p$ time series are linear combinations of $p$ latent uncorrelated weakly stationary time series. the aim is then to find an estimate for an unmixing matrix, which transforms the observed time series back to uncorrelated latent time series. in sobi (second order blind identification) joint diagonalization of the covariance matrix and autocovariance matrices with several lags is used to estimate the unmixing matrix. the rows of an unmixing matrix can be derived either one by one (deflation-based approach) or simultaneously (symmetric approach). the latter of these approaches is well-known especially in signal processing literature, however, the rigorous analysis of its statistical properties has been missing so far. in this paper, we fill this gap and investigate the statistical properties of the symmetric sobi estimate in detail and find its limiting distribution under general conditions. the asymptotical efficiencies of symmetric sobi estimate are compared to those of recently introduced deflation-based sobi estimate under general multivariate ma$(\infty)$ processes. the theory is illustrated by some finite-sample simulation studies as well as a real eeg data example."
"matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. however, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. while existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting.   in this paper, we propose the first provable, efficient online algorithm for matrix completion. our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (sgd). after every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. our proofs introduce a general framework to show that sgd updates tend to stay away from saddle surfaces and could be of broader interests for other non-convex problems to prove tight rates."
"aims: we present a quantitative study of a new data set of high redshift type ia supernovae spectra, observed at the gemini telescopes during the first 34 months of the supernova legacy survey. during this time 123 supernovae candidates were observed, of which 87 have been identified as sne ia at a median redshift of z=0.720. spectra from the entire second year of the survey and part of the third year (59 total sne candidates with 46 confirmed sne ia) are published here for the first time. the spectroscopic measurements made on this data set are used determine if these distant sne comprise a population similar to those observed locally. methods: rest-frame equivalent width and ejection velocity measurements are made on four spectroscopic features. corresponding measurements are presented for a set of 167 spectra from 24 low-z sne ia from the literature. results: we show that there exists a sample at high redshift with properties similar to nearby sne. no significant difference was found between the distributions of measurements at low and high redsift for three of the features. the fourth feature displays a possible difference that should be investigated further. correlations between type ia sne properties and host galaxy morphology were also found to be similar at low and high z, and within each host galaxy class we see no evidence for redshift-evolution in sn properties. a new correlation between sne ia peak magnitude and the equivalent width of siii absorption is presented. we demonstrate that this correlation reduces the scatter in sne ia luminosity distances in a manner consistent with the lightcurve shape-luminosity corrections that are used for type ia sne cosmology. conclusions: we show that this new sample of snls sne ia has spectroscopic properties similar to nearby objects. (abridged)"
"to examine the evolution of giant molecular clouds in the stream of a hot plasma we performed two-dimensional hydrodynamical simulations that take full account of self-gravity, heating and cooling effects and heat conduction by electrons. we use the thermal conductivity of a fully ionized hydrogen plasma proposed by spitzer and a saturated heat flux according to cowie & mckee in regions where the mean free path of the electrons is large compared to the temperature scaleheight. significant structural and evolutionary differences occur between simulations with and without heat conduction. dense clouds in pure dynamical models experience dynamical destruction by kelvin-helmholtz (kh) instability. in static models heat conduction leads to evaporation of such clouds. heat conduction acting on clouds in a gas stream smooths out steep temperature and density gradients at the edge of the cloud because the conduction timescale is shorter than the cooling timescale. this diminishes the velocity gradient between the streaming plasma and the cloud, so that the timescale for the onset of kh instabilities increases, and the surface of the cloud becomes less susceptible to kh instabilities. the stabilisation effect of heat conduction against kh instability is more pronounced for smaller and less massive clouds. as in the static case more realistic cloud conditions allow heat conduction to transfer hot material onto the cloud's surface and to mix the accreted gas deeper into the cloud."
"with the rise of social media, people can now form relationships and communities easily regardless of location, race, ethnicity, or gender. however, the power of social media simultaneously enables harmful online behavior such as harassment and bullying. cyberbullying is a serious social problem, making it an important topic in social network analysis. machine learning methods can potentially help provide better understanding of this phenomenon, but they must address several key challenges: the rapidly changing vocabulary involved in cyber- bullying, the role of social network structure, and the scale of the data. in this study, we propose a model that simultaneously discovers instigators and victims of bullying as well as new bullying vocabulary by starting with a corpus of social interactions and a seed dictionary of bullying indicators. we formulate an objective function based on participant-vocabulary consistency. we evaluate this approach on twitter and ask.fm data sets and show that the proposed method can detect new bullying vocabulary as well as victims and bullies."
"we show that if we have an orthogonal base ($u_1,\ldots,u_p$) in a $p$-dimensional vector space, and select $p+1$ vectors $v_1,\ldots, v_p$ and $w$ such that the vectors traverse the origin, then the probability of $w$ being to closer to all the vectors in the base than to $v_1,\ldots, v_p$ is at least 1/2 and converges as $p$ increases to infinity to a normal distribution on the interval [-1,1]; i.e., $\phi(1)-\phi(-1)\approx0.6826$. this result has relevant consequences for principal components analysis in the context of regression and other learning settings, if we take the orthogonal base as the direction of the principal components."
"autoassociative networks were proposed in the 80's as simplified models of memory function in the brain, using recurrent connectivity with hebbian plasticity to store patterns of neural activity that can be later recalled. this type of computation has been suggested to take place in the ca3 region of the hippocampus and at several levels in the cortex. one of the weaknesses of these models is their apparent inability to store correlated patterns of activity. we show, however, that a small and biologically plausible modification in the `learning rule' (associating to each neuron a plasticity threshold that reflects its popularity) enables the network to handle correlations. we study the stability properties of the resulting memories (in terms of their resistance to the damage of neurons or synapses), finding a novel property of autoassociative networks: not all memories are equally robust, and the most informative are also the most sensitive to damage. we relate these results to category-specific effects in semantic memory patients, where concepts related to `non-living things' are usually more resistant to brain damage than those related to `living things', a phenomenon suspected to be rooted in the correlation between representations of concepts in the cortex."
"are systems that display topological quantum order (tqo), and have a gap to excitations, hardware fault-tolerant at finite temperatures? we show that in surface code models that display low d-dimensional gauge-like symmetries, such as kitaev's and its generalizations, the expectation value of topological symmetry operators vanishes at any non-zero temperature, a phenomenon that we coined thermal fragility. the autocorrelation time for the non-local topological quantities in these systems may remain finite even in the thermodynamic limit. we provide explicit expressions for the autocorrelation functions in kitaev's model. if temperatures far below the gap may be achieved then these autocorrelation times, albeit finite, can be made large. the physical engine behind the loss of correlations at large spatial and/or temporal distance is the proliferation of topological defects at any finite temperature as a result of a dimensional reduction. this raises an important question: how may we best quantify the degree of protection of quantum information in a topologically ordered system at finite temperature?"
"the family of location and scale mixtures of gaussians has the ability to generate a number of flexible distributional forms. it nests as particular cases several important asymmetric distributions like the generalised hyperbolic distribution. the generalised hyperbolic distribution in turn nests many other well known distributions such as the normal inverse gaussian (nig) whose practical relevance has been widely documented in the literature. in a multivariate setting, we propose to extend the standard location and scale mixture concept into a so called multiple scaled framework which has the advantage of allowing different tail and skewness behaviours in each dimension of the variable space with arbitrary correlation between dimensions. estimation of the parameters is provided via an em algorithm with a particular focus on nig distributions. inference is then extended to cover the case of mixtures of such multiple scaled distributions for application to clustering. assessments on simulated and real data confirm the gain in degrees of freedom and flexibility in modelling data of varying tail behaviour and directional shape."
"we consider an information updating system where an information provider and an information receiver engage in an update process over time. different from the existing literature where updates are countable (hard) and take effect either immediately or after a delay, but $instantaneously$ in both cases, here updates start taking effect right away but $gradually$ over time. we coin this setting $soft$ $updates$. when the updating process starts, the age decreases until the soft update period ends. we constrain the number of times the information provider and the information receiver meet (number of update periods) and the total duration of the update periods. we consider two models for the decrease of age during an update period: in the first model, the rate of decrease of age is proportional to the current age, and in the second model, the rate of decrease of age is constant. the first model results in an exponentially decaying age, and the second model results in a linearly decaying age. in both cases, we determine the optimum updating schemes, by determining the optimum start times and optimum durations of the updates, subject to the constraints on the number of update periods and the total update duration."
"in the present paper, we discuss contra-arguments concerning the use of pareto-lev\'y distributions for modeling in finance. it appears that such probability laws do not provide sufficient number of outliers observed in real data. connection with the classical limit theorem for heavy-tailed distributions with such type of models is also questionable. the idea of alternative modeling is given."
"we study the behaviour of the c60 molecule under very high internal or external pressure using tersoff as well as brenner potentials. as a result, we estimate the critical internal and external pressures that lead to its instability. we also calculate stretching force constant and bulk modulus of this molecule at several pressures under which the molecule remains stable. the values of these estimated here at zero pressure agree closely with those obtained in earlier calculations. we also observe that at high pressures, a finite value of parameter of tersoff potential gives physically acceptable results in contrast to its value zero, which is usually taken for the carbon systems."
"a multistatic radar set-up is considered in which distributed receive antennas are connected to a fusion center (fc) via limited-capacity backhaul links. similar to cloud radio access networks in communications, the receive antennas quantize the received baseband signal before transmitting it to the fc. the problem of maximizing the detection performance at the fc jointly over the code vector used by the transmitting antenna and over the statistics of the noise introduced by backhaul quantization is investigated. specifically, adopting the information-theoretic criterion of the bhattacharyya distance to evaluate the detection performance at the fc and information-theoretic measures of the quantization rate, the problem at hand is addressed via a block coordinate descent (bcd) method coupled with majorization-minimization (mm). numerical results demonstrate the advantages of the proposed joint optimization approach over more conventional solutions that perform separate optimization."
"we consider inference for the parameters of a linear model when the covariates are random and the relationship between response and covariates is possibly non-linear. conventional inference methods such as z-intervals perform poorly in these cases. we propose a double bootstrap-based calibrated percentile method, perc-cal, as a general-purpose ci method which performs very well relative to alternative methods in challenging situations such as these. the superior performance of perc-cal is demonstrated by a thorough, full-factorial design synthetic data study as well as a real data example involving the length of criminal sentences. we also provide theoretical justification for the perc-cal method under mild conditions. the method is implemented in the r package `perccal', available through cran and coded primarily in c++, to make it easier for practitioners to use."
"task-based, rather than vehicle-based, control architectures have been shown to provide superior performance in certain human supervisory control missions. these results motivate the need for the development of robust, reliable usability metrics to aid in creating interfaces for use in this domain. to this end, we conduct a pilot usability study of a particular task-based supervisory control interface called the research environment for supervisory control of heterogenous unmanned vehicles (reschu). in particular, we explore the use of eye-tracking metrics as an objective means of evaluating the reschu interface and providing guidance in improving usability. our main goals for this study are to 1) better understand how eye-tracking can augment standard usability metrics, 2) formulate initial models of operator behavior, and 3) identify interesting areas of future research."
"it is often of interest to decompose a total effect of an exposure into the component that acts on the outcome through some mediator and the component that acts independently through other pathways. said another way, we are interested in the direct and indirect effects of the exposure on the outcome. even if the exposure is randomly assigned, it is often infeasible to randomize the mediator, leaving the mediator-outcome confounding not fully controlled. we develop a sensitivity analysis technique that can bound the direct and indirect effects without parametric assumptions about the unmeasured mediator-outcome confounding."
"the leaderboard in machine learning competitions is a tool to show the performance of various participants and to compare them. however, the leaderboard quickly becomes no longer accurate, due to hack or overfitting. this article gives two pieces of advice to prevent easy hack or overfitting. by following these advice, we reach the conclusion that something like the ladder leaderboard introduced in [blum2015ladder] is inevitable. with this understanding, we naturally simplify ladder by eliminating its redundant computation and explain how to choose the parameter and interpret it. we also prove that the sample complexity is cubic to the desired precision of the leaderboard."
"a new characterization of the multivariate so-called ""quasi-gaussian distribution"" (the authors dared to coin a new term) by means of independence their cartesian and polar coordinates proposed.   the authors try to show that these distributions may essentially differ from the classical gaussian distribution. some properties of these distributions are studied: calculating of moments and of bilateral tail behavior. some potential applications of these distributions and mixtures of the distributions in demography, philology are discussed in the final section."
"a real-space quantum transport simulator for carbon nanoribbon (cnr) mosfets has been developed. using this simulator, the performance of carbon nanoribbon (cnr) mosfets is examined in the ballistic limit. the impact of quantum effects on device performance of cnr mosfets is also studied. we found that 2d semi-infinite graphene contacts provide metal-induced-gap-states (migs) in the cnr channel. these states would provide quantum tunneling in the short channel device and cause fermi level pining. these effects cause device performance degradation both on the on-state and the off-state. pure 1d devices (infinite contacts), however, show no migs. quantum tunneling effects are still playing an important role in the device characteristics. conduction due to band-to-band tunneling is accurately captured in our simulations. it is important in these devices, and found to dominate the off-state current. based on our simulations, both a 1.4nm wide and a 1.8nm wide cnr with channel length of 12.5nm can outperform ultra scaled si devices in terms of drive current capabilities and electrostatic control. although subthreshold slopes in the forward-bias conduction are better than in si transistors, tunneling currents are important and prevent the achievement of the theoretical limit of 60mv/dec."
"this paper introduces a set of algorithms for monte-carlo bayesian reinforcement learning. firstly, monte-carlo estimation of upper bounds on the bayes-optimal value function is employed to construct an optimistic policy. secondly, gradient-based algorithms for approximate upper and lower bounds are introduced. finally, we introduce a new class of gradient algorithms for bayesian bellman error minimisation. we theoretically show that the gradient methods are sound. experimentally, we demonstrate the superiority of the upper bound method in terms of reward obtained. however, we also show that the bayesian bellman error method is a close second, despite its significant computational simplicity."
"we investigate the problem of reconstructing signals from a subsampled convolution of their modulated versions and a known filter. the problem is studied as applies to specific imaging systems relying on spatial phase modulation by randomly coded ""masks."" the diversity induced by the random masks is deemed to improve the conditioning of the deconvolution problem while maintaining sampling efficiency.   we analyze a linear model of the system, where the joint effect of the spatial modulation, blurring, and spatial subsampling is represented by a measurement matrix. we provide a bound on the conditioning of this measurement matrix in terms of the number of masks, the dimension of the image, and certain characteristics of the blurring kernel and subsampling operator. the derived bound shows that stable deconvolution is possible with high probability even if the total number of (scalar) measurements is within a logarithmic factor of the image size. furthermore, beyond a critical number of masks determined by the extent of blurring and subsampling, every additional mask improves the conditioning of the measurement matrix.   we also consider a more interesting scenario where the target image is sparse. we show that under mild conditions on the blurring kernel, with high probability the measurement matrix is a restricted isometry when the number of masks is within a logarithmic factor of the sparsity of the image. therefore, the image can be reconstructed using many sparse recovery algorithms such as the basis pursuit. the bound on the required number of masks is linear in sparsity of the image but it is logarithmic in its dimension. the bound provides a quantitative view of the effect of the blurring and subsampling on the required number of masks, which is critical for designing efficient imaging systems."
"probability distributions can be read as simple expressions of information. each continuous probability distribution describes how information changes with magnitude. once one learns to read a probability distribution as a measurement scale of information, opportunities arise to understand the processes that generate the commonly observed patterns. probability expressions may be parsed into four components: the dissipation of all information, except the preservation of average values, taken over the measurement scale that relates changes in observed values to changes in information, and the transformation from the underlying scale on which information dissipates to alternative scales on which probability pattern may be expressed. information invariances set the commonly observed measurement scales and the relations between them. in particular, a measurement scale for information is defined by its invariance to specific transformations of underlying values into measurable outputs. essentially all common distributions can be understood within this simple framework of information invariance and measurement scale."
"generalized super-cerenkov radiations (scr), as well as their scr-signatures are investigated and classified. two general scr- coherence conditions are found as two natural extremes of the same spontaneous particles decay in (dielectric, nuclear or hadronic) media the main results on the quantum theory of the scr-phenomena as well as the results of the first experimental test of the super-coherence conditions, obtained by using the experimental data from bnl are presented. the new concepts such as: scr-gluons, scr-w-bosons and scr-z-bosons, all three suggested by elementary particle classification, are introduced. the gluonic super-cerenkov-like radiation, first introduced here, is schematically described. the interpretation of some recent rhic results as signature of the scr-gluons is suggested."
"we calculate the tunneling density of states for a tomonaga-luttinger liquid placed under a strong bias voltage. for the tunneling through a side-coupled point contact, one can observe the power law singularities in the tunneling density of states separately for the right- and left-movers despite the point-like tunnel contact. deviations of the nonequilibrium tunneling exponents from the equilibrium case are discussed."
"this paper considers statistical estimation problems where the probability distribution of the observed random variable is invariant with respect to actions of a finite topological group. it is shown that any such distribution must satisfy a restricted finite mixture representation. when specialized to the case of distributions over the sphere that are invariant to the actions of a finite spherical symmetry group $\mathcal g$, a group-invariant extension of the von mises fisher (vmf) distribution is obtained. the $\mathcal g$-invariant vmf is parameterized by location and scale parameters that specify the distribution's mean orientation and its concentration about the mean, respectively. using the restricted finite mixture representation these parameters can be estimated using an expectation maximization (em) maximum likelihood (ml) estimation algorithm. this is illustrated for the problem of mean crystal orientation estimation under the spherically symmetric group associated with the crystal form, e.g., cubic or octahedral or hexahedral. simulations and experiments establish the advantages of the extended vmf em-ml estimator for data acquired by electron backscatter diffraction (ebsd) microscopy of a polycrystalline nickel alloy sample."
"we are concerned with input-to-state stability (iss) of randomly switched systems. we provide preliminary results dealing with sufficient conditions for stochastic versions of iss for randomly switched systems without control inputs, and with the aid of universal formulae we design controllers for iss-disturbance attenuation when control inputs are present. two types of switching signals are considered: the first is characterized by a statistically slow-switching condition, and the second by a class of semi-markov processes."
"in recent years, research on decoding brain activity based on functional magnetic resonance imaging (fmri) has made remarkable achievements. however, constraint-free natural image reconstruction from brain activity is still a challenge. the existing methods simplified the problem by using semantic prior information or just reconstructing simple images such as letters and digitals. without semantic prior information, we present a novel method to reconstruct nature images from fmri signals of human visual cortex based on the computation model of convolutional neural network (cnn). firstly, we extracted the units output of viewed natural images in each layer of a pre-trained cnn as cnn features. secondly, we transformed image reconstruction from fmri signals into the problem of cnn feature visualizations by training a sparse linear regression to map from the fmri patterns to cnn features. by iteratively optimization to find the matched image, whose cnn unit features become most similar to those predicted from the brain activity, we finally achieved the promising results for the challenging constraint-free natural image reconstruction. as there was no use of semantic prior information of the stimuli when training decoding model, any category of images (not constraint by the training set) could be reconstructed theoretically. we found that the reconstructed images resembled the natural stimuli, especially in position and shape. the experimental results suggest that hierarchical visual features can effectively express the visual perception process of human brain."
"topological features of gene regulatory networks can be successfully reproduced by a model population evolving under selection for short dynamical attractors. the evolved population of networks exhibit motif statistics, summarized by significance profiles, which closely match those of {\it e. coli, s. cerevsiae} and {\it b. subtilis}, in such features as the excess of linear motifs and feed-forward loops, and deficiency of feedback loops. the slow relaxation to stasis is a hallmark of a rugged fitness landscape, with independently evolving populations exploring distinct valleys strongly differing in network properties."
"serial section microscopy is an established method for volumetric anatomy reconstruction. section series imaged with electron microscopy are currently vital for the reconstruction of the synaptic connectivity of entire animal brains such as that of drosophila melanogaster. the process of removing ultrathin layers from a solid block containing the specimen, however, is a fragile procedure and has limited precision with respect to section thickness. we have developed a method to estimate the relative z-position of each individual section as a function of signal change across the section series. first experiments show promising results on both serial section transmission electron microscopy (sstem) data and focused ion beam scanning electron microscopy (fib-sem) series. we made our solution available as open source plugins for the trakem2 software and the imagej distribution fiji."
"we study the minimal sample size n=n(n) that suffices to estimate the covariance matrix of an n-dimensional distribution by the sample covariance matrix in the operator norm, with an arbitrary fixed accuracy. we establish the optimal bound n=o(n) for every distribution whose k-dimensional marginals have uniformly bounded $2+\varepsilon$ moments outside the sphere of radius $o(\sqrt{k})$. in the specific case of log-concave distributions, this result provides an alternative approach to the kannan-lovasz-simonovits problem, which was recently solved by adamczak et al. [j. amer. math. soc. 23 (2010) 535-561]. moreover, a lower estimate on the covariance matrix holds under a weaker assumption - uniformly bounded $2+\varepsilon$ moments of one-dimensional marginals. our argument consists of randomizing the spectral sparsifier, a deterministic tool developed recently by batson, spielman and srivastava [siam j. comput. 41 (2012) 1704-1721]. the new randomized method allows one to control the spectral edges of the sample covariance matrix via the stieltjes transform evaluated at carefully chosen random points."
"jayanta kumar ghosh is one of the most extraordinary professors in the field of statistics. his research in numerous areas, especially asymptotics, has been groundbreaking, influential throughout the world, and widely recognized through awards and other honors. his leadership in statistics as director of the indian statistical institute and president of the international statistical institute, among other eminent positions, has been likewise outstanding. in recognition of jayanta's enormous impact, this volume is an effort to honor him by drawing together contributions to the main areas in which he has worked and continues to work. the papers naturally fall into five categories. first, sequential estimation was jayanta's starting point. thus, beginning with that topic, there are two papers, one classical by hall and ding leading to a variant on p-values, and one bayesian by berger and sun extending reference priors to stopping time problems. second, there are five papers in the general area of prior specification. much of jayanta's earlier work involved group families as does sweeting's paper here for instance. there are also two papers dwelling on the link between fuzzy sets and priors, by meeden and by delampady and angers. equally daring is the work by mukerjee with data dependent priors and the pleasing confluence of several prior selection criteria found by ghosh, santra and kim. jayanta himself studied a variety of prior selection criteria including probability matching priors and reference priors."
"we study thermal effects at the depinning transition by numerical simulations of driven one-dimensional elastic interfaces in a disordered medium. we find that the velocity of the interface, evaluated at the critical depinning force, can be correctly described with the power law $v\sim t^\psi$, where $\psi$ is the thermal exponent. using the sample-dependent value of the critical force, we precisely evaluate the value of $\psi$ directly from the temperature dependence of the velocity, obtaining the value $\psi = 0.15 \pm 0.01$. by measuring the structure factor of the interface we show that both the thermally-rounded and the t=0 depinning, display the same large-scale geometry, described by an identical divergence of a characteristic length with the velocity $\xi \propto v^{-\nu/\beta}$, where $\nu$ and $\beta$ are respectively the t=0 correlation and depinning exponents. we discuss the comparison of our results with previous estimates of the thermal exponent and the direct consequences for recent experiments on magnetic domain wall motion in ferromagnetic thin films."
"let $x\in \mathbb{r}^p$ and $y\in \mathbb{r}$ be two random variables. we estimate the conditional covariance matrix $\mathrm{cov}\left(\mathrm{e}\left[\boldsymbol{x}\vert y\right]\right)$ applying a plug-in kernel-based algorithm to its entries. next, we investigate the estimators rate of convergence under smoothness hypotheses on the density function of $(\boldsymbol{x},y)$. in a high-dimensional context, we improve the consistency the whole matrix estimator by providing a decreasing structure over the $\mathrm{cov}\left(\mathrm{e}\left[\boldsymbol{x}\vert y\right]\right)$ entries. we illustrate a sliced inverse regression setting for time series matching the conditions of our estimator"
"we present a mathematical model, based on ordinary differential equations, for the evolution of solid tumors and their response to treatment. specifically the effects of a cytotoxic agent and a monoclonal antibody are included as control term in the equations. the variables considered here are: the number of cancerous cells sensitive to chemotherapy, the number of cancerous cells resistant to chemotherapy, the degree of angiogenesis and the average intensity of vegf. the rules that govern the quantities mentioned above are based on a geometrical argument: we approximate the tumor mass as a sphere and thus derive basic formulae for the normoxic cells and for vegf production. the monoclonal antibody acts on vegf and thus has in uence to the global degree of angiogenesis. numerical estimates on some of the parameters are performed in order to match the main landmark in tumor progression and reaction to treatment in the specific case of colorectal cancer."
"financial portfolios are often optimized for maximum profit while subject to a constraint formulated in terms of the conditional value-at-risk (cvar). this amounts to solving a linear problem. however, in its original formulation this linear problem has a very large number of linear constraints, too many to be enforced in practice. in the literature this is addressed by a reformulation of the problem using so-called dummy variables. this reduces the large number of constraints in the original linear problem at the cost of increasing the number of variables. in the context of reinsurance portfolio optimization we observe that the increase in variable count can lead to situations where solving the reformulated problem takes a long time. therefore we suggest a different approach. we solve the original linear problem with cutting-plane method: the proposed algorithm starts with the solution of a relaxed problem and then iteratively adds cuts until the solution is approximated within a preset threshold. this is a new approach. for a reinsurance case study we show that a significant reduction of necessary computer resources can be achieved."
"does the energy requirements for the human brain give energy constraints that give reason to doubt the feasibility of artificial intelligence? this report will review some relevant estimates of brain bioenergetics and analyze some of the methods of estimating brain emulation energy requirements. turning to ai, there are reasons to believe the energy requirements for de novo ai to have little correlation with brain (emulation) energy requirements since cost could depend merely of the cost of processing higher-level representations rather than billions of neural firings. unless one thinks the human way of thinking is the most optimal or most easily implementable way of achieving software intelligence, we should expect de novo ai to make use of different, potentially very compressed and fast, processes."
"the state-of-art dnn structures involve intensive computation and high memory storage. to mitigate the challenges, the memristor crossbar array has emerged as an intrinsically suitable matrix computation and low-power acceleration framework for dnn applications. however, the high accuracy solution for extreme model compression on memristor crossbar array architecture is still waiting for unraveling. in this paper, we propose a memristor-based dnn framework which combines both structured weight pruning and quantization by incorporating alternating direction method of multipliers (admm) algorithm for better pruning and quantization performance. we also discover the non-optimality of the admm solution in weight pruning and the unused data path in a structured pruned model. motivated by these discoveries, we design a software-hardware co-optimization framework which contains the first proposed network purification and unused path removal algorithms targeting on post-processing a structured pruned model after admm steps. by taking memristor hardware constraints into our whole framework, we achieve extreme high compression ratio on the state-of-art neural network structures with minimum accuracy loss. for quantizing structured pruned model, our framework achieves nearly no accuracy loss after quantizing weights to 8-bit memristor weight representation. we share our models at anonymous link https://bit.ly/2vnmuy0."
"practically measurable quantities resulting from quantum field theory are not described by hermitian operators, contradicting one of the cornerstone axioms of orthodox quantum theory. this could be a sign that some of the axioms of orthodox quantum theory should be reformulated. a non-orthodox reformulation of quantum theory based on integral curves of particle currents is advocated and possible measurable manifestations are discussed. the consistency with particle creation and destruction requires particles to be extended objects, which can be viewed as a new derivation of string theory. within this reformulation, an indirect low-energy test of string theory is also possible."
"this paper revisits the problem of analyzing multiple ratings given by different judges. different from previous work that focuses on distilling the true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic insights into our in-house well-trained judges. we generalize the well-known dawidskene model (dawid & skene, 1979) to a spectrum of probabilistic models under the same ""truelabel + confusion"" paradigm, and show that our proposed hierarchical bayesian model, called hybridconfusion, consistently outperforms dawidskene on both synthetic and real-world data sets."
"inverse probability of treatment weighting (iptw) is a popular propensity score (ps)-based approach to estimate causal effects in observational studies at risk of confounding bias. a major issue when estimating the ps is the presence of partially observed covariates. multiple imputation (mi) is a natural approach to handle missing data on covariates, but its use in the ps context raises three important questions: (i) should we apply rubin's rules to the iptw treatment effect estimates or to the ps estimates themselves? (ii) does the outcome have to be included in the imputation model? (iii) how should we estimate the variance of the iptw estimator after mi? we performed a simulation study focusing on the effect of a binary treatment on a binary outcome with three confounders (two of them partially observed). we used mi with chained equations to create complete datasets and compared three ways of combining the results: combining treatment effect estimates (mite); combining the ps across the imputed datasets (mips); or combining the ps parameters and estimating the ps of the average covariates across the imputed datasets (mipar). we also compared the performance of these methods to complete case (cc) analysis and the missingness pattern (mp) approach, a method which uses a different ps model for each pattern of missingness. we also studied empirically the consistency of these 3 mi estimators. under a missing at random (mar) mechanism, cc and mp analyses were biased in most cases when estimating the marginal treatment effect, whereas mi approaches had good performance in reducing bias as long as the outcome was included in the imputation model. however, only mite was unbiased in all the studied scenarios and rubin's rules provided good variance estimates for mite."
"let a be a compact set in the right-half plane and $\gamma(a)$ the set in $\mathbb{r}^{3}$ obtained by rotating a about the vertical axis. we investigate the support of the limit distribution of minimal energy point charges on $\gamma(a)$ that interact according to the riesz potential 1/r^{s}, 0<s<1, where r is the euclidean distance between points. potential theory yields that this limit distribution coincides with the equilibrium measure on $\gamma(a)$ which is supported on the outer boundary of $\gamma(a)$. we show that there are sets of revolution $\gamma(a)$ such that the support of the equilibrium measure on $\gamma(a)$ is {\bf not} the complete outer boundary, in contrast to the coulomb case s=1. however, the support of the limit distribution on the set of revolution $\gamma(r+a)$ as r goes to infinity, is the full outer boundary for certain sets a, in contrast to the logarithmic case (s=0)."
"recently, many software tools have been developed to perform quantification in lc-ms analyses. however, most of them are specific to either a quantification strategy (e.g. label-free or isotopic labelling) or a mass-spectrometry system (e.g. high or low resolution).   in this context, we have developed masschroq, a versatile software that performs lc-ms data alignment and peptide quantification by peak area integration on extracted ion chromatograms. masschroq is suitable for quantification with or without labelling and is not limited to high resolution systems. peptides of interest (for example all the identified peptides) can be determined automatically or manually by providing targeted m/z and retention time values. it can handle large experiments that include protein or peptide fractionation (as sds-page, 2d-lc). it is fully configurable. every processing step is traceable, the produced data are in open standard format and its modularity allows easy integration into proteomic pipelines. the output results are ready for use in statistical analyses.   evaluation of masschroq on complex label-free data obtained from low and high resolution mass spectrometers showed low cvs for technical reproducibility (1.4%) and high coefficients of correlation to protein quantity (0.98).   masschroq is freely available under the gnu general public licence v3.0 at http://pappso.inra.fr/bioinfo/masschroq/."
"the laser interferometer gravitational wave observatory (ligo) is one of a new generation of detectors of gravitational radiation. the existence of gravitational radiation was first predicted by einstein in 1916, however gravitational waves have not yet been directly observed. one source of gravitation radiation is binary inspiral. two compact bodies orbiting each other, such as a pair of black holes, lose energy to gravitational radiation. as the system loses energy the bodies spiral towards each other. this causes their orbital speed and the amount of gravitational radiation to increase, producing a characteristic ``chirp'' waveform in the ligo sensitive band. in this thesis, matched filtering of ligo science data is used to search for low mass binary systems in the halo of dark matter surrounding the milky way. observations of gravitational microlensing events of stars in the large magellanic cloud suggest that some fraction of the dark matter in the halo may be in the form of massive astrophysical compact halo objects (machos). it has been proposed that low mass black holes formed in the early universe may be a component of the macho population; some fraction of these black hole machos will be in binary systems and detectable by ligo. the inspiral from a macho binary composed of two 0.5 solar mass black holes enters the ligo sensitive band around 40 hz. the chirp signal increases in amplitude and frequency, sweeping through the sensitive band to 4400 hz in 140 seconds. by using evidence from microlensing events and theoretical predictions of the population an upper limit is placed on the rate of black hole macho inspirals in the galactic halo."
"the opportunity to tell a white lie (i.e., a lie that benefits another person) generates a moral conflict between two opposite moral dictates, one pushing towards telling always the truth and the other pushing towards helping others. here we study how people resolve this moral conflict. what does telling a white lie signal about a person's pro-social tendencies? to answer this question, we conducted a two-stage 2x2 experiment. in the first stage, we used a deception game to measure aversion to telling a pareto white lie (i.e., a lie that helps both the liar and the listener), and aversion to telling an altruistic white lie (i.e., a lie that helps the listener at the expense of the liar). in the second stage we measured altruistic tendencies using a dictator game and cooperative tendencies using a prisoner's dilemma. we found three major results: (i) both altruism and cooperation are positively correlated with aversion to telling a pareto white lie; (ii) both altruism and cooperation are negatively correlated with aversion to telling an altruistic white lie; (iii) men are more likely than women to tell an altruistic white lie, but not to tell a pareto white lie. our results shed light on the moral conflit between pro-sociality and truth-telling. in particular, the first finding suggests that a significant proportion of people have non-distributional notions of what the right thing to do is: irrespective of their economic consequences, they tell the truth, they cooperate, they share their money."
"we calculate the semileptonic form factors $f_+^{b\to \eta}(q^2)$ and $f_+^{b\to \eta'}(q^2)$ from qcd sum rules on the light-cone (lcsrs), to nlo in qcd, and for small to moderate q^2, $0\leq q^2\leq 16 {\rm   gev}^2$. we include in particular the so-called singlet contribution, i.e.\ weak annihilation of the b meson with the emission of two gluons which, thanks to the u(1)$_{\rm a}$ anomaly, couple directly to $\etap$. this effect is included to leading-twist accuracy. this contribution has been neglected in previous calculations of the form factors from lcsrs. we find that the singlet contribution to $f_+^{b\to \eta'}$ can be up to 20%, while that to $f_+^{b\to \eta}$ is, as expected, much smaller and below 3%. we also suggest to measure the ratio ${\cal b}(b\to\eta' e \nu)/{\cal   b}(b\to \eta e \nu)$ to better constrain the size of the singlet contribution."
"connected weakly irreducible not irreducible subgroups of $sp(1,n+1)\subset so(4,4n+4)$ that satisfy a certain additional condition are classified. this will be used to classify connected holonomy groups of pseudo-hyper-k\""ahlerian manifolds of index 4."
"one of the main challenges for biomedical research lies in the computer-assisted integrative study of large and increasingly complex combinations of data in order to understand molecular mechanisms. the preservation of the materials and methods of such computational experiments with clear annotations is essential for understanding an experiment, and this is increasingly recognized in the bioinformatics community. our assumption is that offering means of digital, structured aggregation and annotation of the objects of an experiment will provide necessary meta-data for a scientist to understand and recreate the results of an experiment. to support this we explored a model for the semantic description of a workflow-centric research object (ro), where an ro is defined as a resource that aggregates other resources, e.g., datasets, software, spreadsheets, text, etc. we applied this model to a case study where we analysed human metabolite variation by workflows."
"we define atiyah-bott index on stratified manifolds and express it in topological terms. by way of example, we compute this index for geometric operators on manifolds with edges."
"this paper derives new algorithms for signomial programming, a generalization of geometric programming. the algorithms are based on a generic principle for optimization called the mm algorithm. in this setting, one can apply the geometric-arithmetic mean inequality and a supporting hyperplane inequality to create a surrogate function with parameters separated. thus, unconstrained signomial programming reduces to a sequence of one-dimensional minimization problems. simple examples demonstrate that the mm algorithm derived can converge to a boundary point or to one point of a continuum of minimum points. conditions under which the minimum point is unique or occurs in the interior of parameter space are proved for geometric programming. convergence to an interior point occurs at a linear rate. finally, the mm framework easily accommodates equality and inequality constraints of signomial type. for the most important special case, constrained quadratic programming, the mm algorithm involves very simple updates."
"multivariate data that combine binary, categorical, count and continuous outcomes are common in the social and health sciences. we propose a semiparametric bayesian latent variable model for multivariate data of arbitrary type that does not require specification of conditional distributions. drawing on the extended rank likelihood method by hoff [ann. appl. stat. 1 (2007) 265-283], we develop a semiparametric approach for latent variable modeling with mixed outcomes and propose associated markov chain monte carlo estimation methods. motivated by cognitive testing data, we focus on bifactor models, a special case of factor analysis. we employ our semiparametric bayesian latent variable model to investigate the association between cognitive outcomes and mri-measured regional brain volumes."
"we propose new degroot-type social learning models with feedback in a continuous time, to investigate the effect of a noisy information source on consensus formation in a social network. unlike the standard degroot framework, noisy information models destroy consensus formation. on the other hand, the noisy opinion dynamics converge to the equilibrium distribution that encapsulates correlations among agents' opinions. interestingly, such an equilibrium distribution is also a non-equilibrium steady state (ness) with a non-zero probabilistic current loop. thus, noisy information source leads to a ness at long times that encodes persistent correlated opinion dynamics of learning agents."
we consider high spin operators. we give a general argument for the logarithmic scaling of their anomalous dimensions which is based on the symmetries of the problem. by an analytic continuation we can also see the origin of the double logarithmic divergence in the sudakov factor. we show that the cusp anomalous dimension is the energy density for a flux configuration of the gauge theory on $ads_3 \times s^1$. we then focus on operators in ${\cal n}=4$ super yang mills which carry large spin and so(6) charge and show that in a particular limit their properties are described in terms of a bosonic o(6) sigma model. this can be used to make certain all loop computations in the string theory.
"colombian coffee farmers have traditionally focused their efforts on activities including seeding, planting and drying. strategic issues to successfully compete in the industry, such as branding, marketing and consumer research, have been neglected. in this research, we apply a type of sensory analysis, based on several statistical techniques used to investigate the key features of ten different brands of colombian coffee. a panel composed of 32 judges investigated nine different attributes related to flavour, fragrance, sweetness and acidity, among others. the last section presents the conclusions reached regarding customer preference and brands profiles."
"the baire metric induces an ultrametric on a dataset and is of linear computational complexity, contrasted with the standard quadratic time agglomerative hierarchical clustering algorithm. we apply the baire distance to spectrometric and photometric redshifts from the sloan digital sky survey using, in this work, about half a million astronomical objects. we want to know how well the (more cos\ tly to determine) spectrometric redshifts can predict the (more easily obtained) photometric redshifts, i.e. we seek to regress the spectrometric on the photometric redshifts, and we develop a clusterwise nearest neighbor regression procedure for this."
"fluctuations in the physical properties of biological machines are inextricably linked to their functions. distributions of run-lengths and velocities of processive molecular motors, like kinesin-1, are accessible through single molecule techniques, yet there is lack a rigorous theoretical model for these probabilities up to now. we derive exact analytic results for a kinetic model to predict the resistive force ($f$) dependent velocity ($p(v)$) and run-length ($p(n)$) distribution functions of generic finitely processive molecular motors that take forward and backward steps on a track. our theory quantitatively explains the zero force kinesin-1 data for both $p(n)$ and $p(v)$ using the detachment rate as the only parameter, thus allowing us to obtain the variations of these quantities under load. at non-zero $f$, $p(v)$ is non-gaussian, and is bimodal with peaks at positive and negative values of $v$. the prediction that $p(v)$ is bimodal is a consequence of the discrete step-size of kinesin-1, and remains even when the step-size distribution is taken into account. although the predictions are based on analyses of kinesin-1 data, our results are general and should hold for any processive motor, which walks on a track by taking discrete steps."
"the shift from outcrossing to self-fertilization is among the most common transitions in plants. until recently, however, a genome-wide view of this transition has been obscured by a dearth of appropriate data and the lack of appropriate population genomic methods to interpret such data. here, we present novel analyses detailing the origin of the selfing species, capsella rubella, which recently split from its outcrossing sister, capsella grandiflora. due to the recency of the split, most variation within c. rubella is found within c. grandiflora. we can therefore identify genomic regions where two c. rubella individuals have inherited the same or different segments of ancestral diversity (i.e. founding haplotypes) present in c. rubella's founder(s). based on this analysis, we show that c. rubella was founded by multiple individuals drawn from a diverse ancestral population closely related to extant c. grandiflora, that drift and selection have rapidly homogenized most of this ancestral variation since c. rubella's founding, and that little novel variation has accumulated within this time. despite the extensive loss of ancestral variation, the approximately 25% of the genome for which two c. rubella individuals have inherited different founding haplotypes makes up roughly 90% of the genetic variation between them. to extend these findings, we develop a coalescent model that utilizes the inferred frequency of founding haplotypes and variation within founding haplotypes to estimate that c. rubella was founded by a potentially large number of individuals 50-100 kya, and has subsequently experienced a 20x reduction in its effective population size. as population genomic data from an increasing number of outcrossing/selfing pairs are generated, analyses like this here will facilitate a fine-scaled view of the evolutionary and demographic impact of the transition to self-fertilization."
"we consider the classic problem of estimating t, the total number of species in a population, from repeated counts in a simple random sample. we look first at the chao-lee estimator: we initially show that such estimator can be obtained by reconciling two estimators of the unobserved probability, and then develop a sequence of improvements culminating in a dirichlet prior bayesian reinterpretation of the estimation problem. by means of this, we obtain simultaneous estimates of t, of the normalized interspecies variance $\gamma^2$ and of the parameter $\lambda$ of the prior. several simulations show that our estimation method is more flexible than several known methods we used as comparison; the only limitation, apparently shared by all other methods, seems to be that it cannot deal with the rare cases in which $\gamma^2 >1$"
"we present a novel method for controlling the $k$-familywise error rate ($k$-fwer) in the linear regression setting using the knockoffs framework first introduced by barber and cand\`es. our procedure, which we also refer to as knockoffs, can be applied with any design matrix with at least as many observations as variables, and does not require knowing the noise variance. unlike other multiple testing procedures which act directly on $p$-values, knockoffs is specifically tailored to linear regression and implicitly accounts for the statistical relationships between hypothesis tests of different coefficients. we prove that knockoffs controls the $k$-fwer exactly in finite samples and show in simulations that it provides superior power to alternative procedures over a range of linear regression problems. we also discuss extensions to controlling other type i error rates such as the false exceedance rate, and use it to identify candidates for mutations conferring drug-resistance in hiv."
"we study magnetic-field-dependent nonresonant microwave absorption and dispersion in thin la$_{0.7}$sr$_{0.3}$mno$_{3}$ films and show that it originates from the colossal magnetoresistance. we develop the model for magnetoresistance of a thin ferromagnetic film in oblique magnetic field. the model accounts fairly well for our experimental findings, as well as for results of other researchers. we demonstrate that nonresonant microwave absorption is a powerful technique that allows contactless measurement of magnetic properties of thin films, including magnetoresistance, anisotropy field and coercive field."
"we report measurements of the temperature dependence of the magnetic penetration depth \lambda(t) in non-centrosymmetric superconductor re_3w. we employed two experimental techniques: extraction of \lambda(t) from magnetic {\em dc}-susceptibility, measured on a powder sample, and the rf tunnel diode resonator technique, where a bulk polycrystalline sample was used. the results of both techniques agree: the temperature dependence of the penetration depth can be well described by weak-coupling, dirty-limit, s-wave bcs theory where we obtain $\delta(0)/k_bt_c=1.76$. no evidence for unconventional pairing resulting from the absence of the inversion symmetry is found."
although logarithmic conformal field theories (lcfts) are known not to factorise many previous findings have only been formulated on their chiral halves. making only mild and rather general assumptions on the structure of an chiral lcft we deduce statements about its local non-chiral equivalent. two methods are presented how to construct local representations as subrepresentations of the tensor product of chiral and anti-chiral jordan cells. furthermore we explore the assembly of generic non-chiral correlation functions from generic chiral and anti-chiral correlators. the constraint of locality is studied and the generality of our method is discussed.
"this paper presents new theory and methodology for the bayesian estimation of overfitted hidden markov models, with finite state space. the goal is then to achieve posterior emptying of extra states. a prior configuration is constructed which favours configurations where the hidden markov chain remains ergodic although it empties out some of the states. asymptotic posterior convergence rates are proven theoretically, and demonstrated with a large sample simulation. the problem of overfitted hmms is then considered in the context of smaller sample sizes, and due to computational and mixing issues two alternative prior structures are studied, one commonly used in practice, and a mixture of the two priors. the prior parallel tempering approach of van havre (2015) is also extended to hmms to allow mcmc estimation of the complex posterior space. a replicate simulation study and an in-depth exploration is performed to compare the three priors with hyperparameters chosen according to the asymptotic constraints alongside less informative alternatives."
"we propose bayesian extensions of two nonparametric regression methods which are kernel and mutual $k$-nearest neighbor regression methods. derived based on gaussian process models for regression, the extensions provide distributions for target value estimates and the framework to select the hyperparameters. it is shown that both the proposed methods asymptotically converge to kernel and mutual $k$-nearest neighbor regression methods, respectively. the simulation results show that the proposed methods can select proper hyperparameters and are better than or comparable to the former methods for an artificial data set and a real world data set."
"negatively charged dna can be compacted by positively charged dendrimers and the degree of compaction is a delicate balance between the strength of the electrostatic interaction and the elasticity of dna. we report various elastic properties of short double stranded dna (dsdna) and the effect of dendrimer binding using fully atomistic molecular dynamics and numerical simulations. in equilibrium at room temperature, the contour length distribution p(l) and end-to-end distance distribution p(r) are nearly gaussian, the former gives an estimate of the stretch modulus {\gamma}_1 of dsdna in quantitative agreement with the literature value. the bend angle distribution p({\theta}) of the dsdna also has a gaussian form and allows to extract a persistence length, l_p of 43 nm. when the dsdna is compacted by positively charged dendrimer, the stretch modulus stays invariant but the effective bending rigidity estimated from the end-to-end distance distribution decreases dramatically due to backbone charge neutralization of dsdna by dendrimer. we support our observations with numerical solutions of the worm-like-chain (wlc) model as well as using non-equilibrium dsdna stretching simulations. these results are helpful in understanding the dsdna elasticity at short length scales as well as how the elasticity is modulated when dsdna binds to a charged object such as a dendrimer or protein."
"we propose elliptical graphical models based on conditional uncorrelatedness as a general- ization of gaussian graphical models by letting the population distribution be elliptical instead of normal, allowing the fitting of data with arbitrarily heavy tails. we study the class of propor- tionally affine equivariant scatter estimators and show how they can be used to perform elliptical graphical modelling, leading to a new class of partial correlation estimators and analogues of the classical deviance test. general expressions for the asymptotic variance of partial correla- tion estimators, unconstrained and under decomposable models, are given, and the asymptotic chi square approximation of the pseudo-deviance test statistic is proved. the feasibility of our approach is demonstrated by a simulation study, using, among others, tyler's scatter estimator, which is distribution-free within the elliptical model. our approach provides a robustification of gaussian graphical modelling. the latter is likelihood-based and known to be very sensitive to model misspecification and outlying observations."
"we provide a remedy for two concerns that have dogged the use of principal components in regression: (i) principal components are computed from the predictors alone and do not make apparent use of the response, and (ii) principal components are not invariant or equivariant under full rank linear transformation of the predictors. the development begins with principal fitted components [cook, r. d. (2007). fisher lecture: dimension reduction in regression (with discussion). statist. sci. 22 1--26] and uses normal models for the inverse regression of the predictors on the response to gain reductive information for the forward regression of interest. this approach includes methodology for testing hypotheses about the number of components and about conditional independencies among the predictors."
"this paper is an attempt to formalize analytically the question raised in ""world population explained: do dead people outnumber living, or vice versa?"" huffington post, \cite{hj}. we start developing simple deterministic malthusian growth models of the problem (with birth and death rates either constant or time-dependent) before running into both linear birth and death markov chain models and age-structured models."
"we introduce a dbar-formulation of the orthogonal polynomials on the complex plane, and hence of the related normal matrix model, which is expected to play the same role as the riemann-hilbert formalism in the theory of orthogonal polynomials on the line and for the related hermitian model. we propose an analog of deift-kriecherbauer-mclaughlin-venakides-zhou asymptotic method for the analysis of the relevant dbar-problem, and indicate how familiar steps for the hermitian model, e.g. the g-function ``undressing'', might look like in the case of the normal model. we use the particular model considered recently by p. elbau and g. felder as a case study."
"we want to give a construction as simple as possible of a borel subset of a product of two polish spaces. this introduces the notion of potential wadge class. among other things, we study the non-potentially closed sets, by proving hurewicz-like results. this leads to partial uniformization theorems, on big sets, in the sense of cardinality or baire category."
we describe the structure of 0-simple countably compact topological inverse semigroups and the structure of congruence-free countably compact topological inverse semigroups.
"visualizing an outfit is an essential part of shopping for clothes. due to the combinatorial aspect of combining fashion articles, the available images are limited to a pre-determined set of outfits. in this paper, we broaden these visualizations by generating high-resolution images of fashion models wearing a custom outfit under an input body pose. we show that our approach can not only transfer the style and the pose of one generated outfit to another, but also create realistic images of human bodies and garments."
"the black-scholes option pricing model (bsopm) has long been in use for valuation of equity options to find the prices of stocks. in this work, using bsopm, we have come up with a comparative analytical approach and numerical technique to find the price of call option and put option and considered these two prices as buying price and selling price of stocks of frontier markets so that we can predict the stock price (close price). changes have been made to the model to find the parameters strike price and the time of expiration for calculating stock price of frontier markets. to verify the result obtained using modified bsopm we have used machine learning approach using the software rapidminer, where we have adopted different algorithms like the decision tree, ensemble learning method and neural network. it has been observed that, the prediction of close price using machine learning is very similar to the one obtained using bsopm. machine learning approach stands out to be a better predictor over bsopm, because black-scholes-merton equation includes risk and dividend parameter, which changes continuously. we have also numerically calculated volatility. as the prices of the stocks goes high due to overpricing, volatility increases at a tremendous rate and when volatility becomes very high market tends to fall, which can be observed and determined using our modified bsopm. the proposed modified bsopm has also been explained based on the analogy of schrodinger equation (and heat equation) of quantum physics."
"comment on ``understanding or, ps and dr'' [arxiv:0804.2958]"
"we consider the problem of robustly maximizing the growth rate of investor wealth in the presence of model uncertainty. possible models are all those under which the assets' region $e$ and instantaneous covariation $c$ are known, and where additionally the assets are stable in that their occupancy time measures converge to a law with density $p$. this latter assumption is motivated by the observed stability of ranked relative market capitalizations for equity markets. we seek to identify the robust optimal growth rate, as well as a trading strategy which achieves this rate in all models. under minimal assumptions upon $(e,c,p)$, we identify the robust growth rate with the donsker-varadhan rate function from occupancy time large deviations theory. we also prove existence of, and explicitly identify, the optimal trading strategy. we then apply our results in the case of drift uncertainty for ranked relative market capitalizations. assuming regularity under symmetrization for the covariance and limiting density of the ranked capitalizations, we explicitly identify the robust optimal trading strategy in this setting."
"the years 1910-1911 are auspicious years in chinese mathematics with the births of pao-lu hsu, luo-keng hua and shiing-shen chern. these three began the development of modern mathematics in china: hsu in probability and statistics, hua in number theory, and chern in differential geometry. we here review some facts about the life of p.-l. hsu which have been uncovered recently, and then discuss some of his contributions. we have drawn heavily on three papers in the 1979 annals of statistics (volume 7, pages 467-483) by t. w. anderson, k. l. chung and e. l. lehmann, as well as an article by jiang ze-han and duan xue-fu in hsu's collected papers."
"we introduce gaussian process topic models (gptms), a new family of topic models which can leverage a kernel among documents while extracting correlated topics. gptms can be considered a systematic generalization of the correlated topic models (ctms) using ideas from gaussian process (gp) based embedding. since gptms work with both a topic covariance matrix and a document kernel matrix, learning gptms involves a novel component-solving a suitable sylvester equation capturing both topic and document dependencies. the efficacy of gptms is demonstrated with experiments evaluating the quality of both topic modeling and embedding."
"it is investigated hurwitz numbers, that correspond to covering of disk with single non-simple boundary critical value. it is found differential equations, that describe a generating function for these numbers."
"we study the structure of locational marginal prices in day-ahead and real-time wholesale electricity markets. in particular, we consider the case of two north american markets and show that the price correlations contain information on the locational structure of the grid. we study various clustering methods and introduce a type of correlation function based on event synchronization for spiky time series, and another based on string correlations of location names provided by the markets. this allows us to reconstruct aspects of the locational structure of the grid."
"nucleosome core particle is a dynamic structure -- dna may transiently peel off the histone octamer surface due to thermal fluctuations or the action of chromatin remodeling enzymes. partial dna unwrapping enables easier access of dna-binding factors to their target sites and thus may provide a dominant pathway for effecting rapid and robust access to dna packaged into chromatin. indeed, a recent high-resolution map of distances between neighboring nucleosome dyads in \emph{s.cerevisiae} shows that at least 38.7\% of all nucleosomes are partially unwrapped. the extent of unwrapping follows a stereotypical pattern in the vicinity of genes, reminiscent of the canonical pattern of nucleosome occupancy in which nucleosomes are depleted over promoters and well-positioned over coding regions. to explain these observations, we developed a biophysical model which employs a 10-11 base pair periodic nucleosome energy profile. the profile, based on the pattern of histone-dna contacts in nucleosome crystal structures and the idea of linker length discretization, accounts for both nucleosome unwrapping and higher-order chromatin structure. our model reproduces the observed genome-wide distribution of inter-dyad distances, and accounts for patterns of nucleosome occupancy and unwrapping around coding regions. at the same time, our approach explains \emph{in vitro} measurements of accessibility of nucleosome-covered binding sites, and of nucleosome-induced cooperativity between dna-binding factors. we are able to rule out several alternative scenarios of nucleosome unwrapping as inconsistent with the genomic data."
"in this paper, we propose a novel speech enhancement (se) method by exploiting the discrete wavelet transform (dwt). this new method reduces the amount of fast time-varying portion, viz. the dwt-wise detail component, in the spectrogram of speech signals so as to highlight the speech-dominant component and achieves better speech quality. a particularity of this new method is that it is completely unsupervised and requires no prior information about the clean speech and noise in the processed utterance. the presented dwt-based se method with various scaling factors for the detail part is evaluated with a subset of aurora-2 database, and the pesq metric is used to indicate the quality of processed speech signals. the preliminary results show that the processed speech signals reveal a higher pesq score in comparison with the original counterparts. furthermore, we show that this method can still enhance the signal by totally discarding the detail part (setting the respective scaling factor to zero), revealing that the spectrogram can be down-sampled and thus compressed without the cost of lowered quality. in addition, we integrate this new method with conventional speech enhancement algorithms, including spectral subtraction, wiener filtering, and spectral mmse estimation, and show that the resulting integration behaves better than the respective component method. as a result, this new method is quite effective in improving the speech quality and well additive to the other se methods."
"we introduce a methodology to construct parsimonious probabilistic models. this method makes use of information filtering networks to produce a robust estimate of the global sparse inverse covariance from a simple sum of local inverse covariances computed on small sub-parts of the network. being based on local and low-dimensional inversions, this method is computationally very efficient and statistically robust even for the estimation of inverse covariance of high-dimensional, noisy and short time-series. applied to financial data our method results computationally more efficient than state-of-the-art methodologies such as glasso producing, in a fraction of the computation time, models that can have equivalent or better performances but with a sparser inference structure. we also discuss performances with sparse factor models where we notice that relative performances decrease with the number of factors. the local nature of this approach allows us to perform computations in parallel and provides a tool for dynamical adaptation by partial updating when the properties of some variables change without the need of recomputing the whole model. this makes this approach particularly suitable to handle big datasets with large numbers of variables. examples of practical application for forecasting, stress testing and risk allocation in financial systems are also provided."
"within the last two decades, foreign direct investment (fdi) has been observed as one of the prime instruments in the process of restructuring the european economies in transition. many scholars argue that fdi is expected to be a source of valuable technology transfer thus might certainly have positive effects on host country development efforts. nonetheless, there are no clear-cut findings about the fdi genuine performances in supporting the economic growth, productivity and export improvements within the european transition countries. using a large and comprehensive data set, we will therefore analyze the linkage between fdi and above mentioned variables, so as to recommend national policy appropriate measures aimed at averting negative and strengthening the positive fdi spillovers."
"it can be important in bayesian analyses of complex models to construct informative prior distributions which reflect knowledge external to the data at hand. nevertheless, how much prior information an analyst can elicit from an expert will be limited due to constraints of time, cost and other factors. this paper develops effective numerical methods for exploring reasonable choices of a prior distribution from a parametric class, when prior information is specified in the form of some limited constraints on prior predictive distributions, and where these prior predictive distributions are analytically intractable. the methods developed may be thought of as a novel application of the ideas of history matching, a technique developed in the literature on assessment of computer models. we illustrate the approach in the context of logistic regression and sparse signal shrinkage prior distributions for high-dimensional linear models."
we use a finite element approach based on galerkin method to obtain approximate steady state solutions of the thermistor problem with temperature dependent electrical conductivity.
"we propose a procedure for testing the linearity of a scalar-on-function regression relationship. to do so, we use the functional generalized additive model (fgam), a recently developed extension of the functional linear model. for a functional covariate x(t), the fgam models the mean response as the integral with respect to t of f{x(t),t} where f is an unknown bivariate function. the fgam can be viewed as the natural functional extension of generalized additive models. we show how the functional linear model can be represented as a simple mixed model nested within the fgam. using this representation, we then consider restricted likelihood ratio tests for zero variance components in mixed models to test the null hypothesis that the functional linear model holds. the methods are general and can also be applied to testing for interactions in a multivariate additive model or for testing for no effect in the functional linear model. the performance of the proposed tests is assessed on simulated data and in an application to measuring diesel truck emissions, where strong evidence of nonlinearities in the relationship between the functional predictor and the response are found."
"we develop a mixture procedure for multi-sensor systems to monitor data streams for a change-point that causes a gradual degradation to a subset of the streams. observations are assumed to be initially normal random variables with known constant means and variances. after the change-point, observations in the subset will have increasing or decreasing means. the subset and the rate-of-changes are unknown. our procedure uses a mixture statistics, which assumes that each sensor is affected by the change-point with probability $p_0$. analytic expressions are obtained for the average run length (arl) and the expected detection delay (edd) of the mixture procedure, which are demonstrated to be quite accurate numerically. we establish the asymptotic optimality of the mixture procedure. numerical examples demonstrate the good performance of the proposed procedure. we also discuss an adaptive mixture procedure using empirical bayes. this paper extends our earlier work on detecting an abrupt change-point that causes a mean-shift, by tackling the challenges posed by the non-stationarity of the slope-change problem."
"we sequenced genomes from a $\sim$7,000 year old early farmer from stuttgart in germany, an $\sim$8,000 year old hunter-gatherer from luxembourg, and seven $\sim$8,000 year old hunter-gatherers from southern sweden. we analyzed these data together with other ancient genomes and 2,345 contemporary humans to show that the great majority of present-day europeans derive from at least three highly differentiated populations: west european hunter-gatherers (whg), who contributed ancestry to all europeans but not to near easterners; ancient north eurasians (ane), who were most closely related to upper paleolithic siberians and contributed to both europeans and near easterners; and early european farmers (eef), who were mainly of near eastern origin but also harbored whg-related ancestry. we model these populations' deep relationships and show that eef had $\sim$44% ancestry from a ""basal eurasian"" lineage that split prior to the diversification of all other non-african lineages."
"we present an overview of the decision-theoretic framework of statistical causality, which is well-suited for formulating and solving problems of determining the effects of applied causes. the approach is described in detail, and is related to and contrasted with other current formulations, such as structural equation models and potential responses. topics and applications covered include confounding, the effect of treatment on the treated, instrumental variables, and dynamic treatment strategies."
comment: expert elicitation for reliable system design [arxiv:0708.0279]
"time-frequency (tf) analysis is a powerful tool for exploring ultrafast dynamics in atoms and molecules. while some tf methods have demonstrated their usefulness and potential in several of quantum systems, a systematic comparison among these methods is still lacking. to this end, we compare a series of classical and contemporary tf methods by taking hydrogen atom in a strong laser field as a benchmark. in addition, several tf methods such as cohen class distribution other than the wigner-ville distribution, reassignment methods, and the empirical mode decomposition method are first introduced to exploration of ultrafast dynamics. among these tf methods, the synchrosqueezing transform successfully illustrates the physical mechanisms in the multiphoton ionization regime and in the tunneling ionization regime. furthermore, an empirical procedure to analyze an unknown complicated quantum system is provided, indicating the versatility of tf analysis as a new viable venue for exploring quantum dynamics."
"we and others have previously developed brain-machine-interfaces (bmis), which allowed ensembles of cortical neurons to control artificial limbs (1-4). however, it is unclear whether cortical ensembles could operate a bmi for whole-body navigation. here we show that rhesus monkeys can learn to navigate a robotic wheelchair while seated on top of it, and using their cortical activity as the robot control signal. two monkeys were chronically implanted with multichannel electrode arrays which simultaneously sampled activity of roughly 150 premotor and sensorimotor cortex neurons per monkey. this neuronal ensemble activity was transformed by a linear decoder into the robotic wheelchair's translational and rotational velocities. during several weeks of training, monkeys significantly improved their ability to navigate the wheelchair toward the location of a food reward. the navigation was enacted by ensemble modulations attuned to the whole-body displacements, and also to the distance to the food location. these results demonstrate that intracranial bmis could restore whole-body mobility to severely paralyzed patients in the future."
"the interadapt r package is designed to be used by statisticians and clinical investigators to plan randomized trials. it can be used to determine if certain adaptive designs offer tangible benefits compared to standard designs, in the context of investigators' specific trial goals and constraints. specifically, interadapt compares the performance of trial designs with adaptive enrollment criteria versus standard (non-adaptive) group sequential trial designs. performance is compared in terms of power, expected trial duration, and expected sample size. users can either work directly in the r console, or with a user-friendly shiny application that requires no programming experience. several added features are available when using the shiny application. for example, the application allows users to immediately download the results of the performance comparison as a csv-table, or as a printable, html-based report."
"the emergence and maintenance of cooperation within sizable groups of unrelated humans offer many challenges for our understanding. we propose that the humans' capacity of communication, such as how many and how far away the fellows can build up mutual communications, may affect the evolution of cooperation. we study this issue by means of the public goods game (pgg) with a two-layered network of contacts. players obtain payoffs from five-person public goods interactions on a square lattice (the interaction layer). also, they update strategies after communicating with neighbours in learning layer, where two players build up mutual communication with a power law probability depending on their spatial distance. our simulation results indicate that the evolution of cooperation is indeed sensitive to how players choose others to communicate with, including the amount as well as the locations. the tendency of localised communication is proved to be a new mechanism to promote cooperation."
"the correlation coefficient between stocks depends on price history and includes information on hierarchical structure in financial markets. it is useful for portfolio selection and estimation of risk. i introduce the life time of correlation between stocks prices to know how far we should investigate the price history to obtain the optimal durability of correlation. i carry out my research on emerging (poland) and established markets (in the usa, great britain and germany). other methods, including the minimum spanning trees, tree half-life, decomposition of correlations and the epps effect are also discussed."
"gaudin algebras form a family of maximal commutative subalgebras in the tensor product of $n$ copies of the universal enveloping algebra $u(\g)$ of a semisimple lie algebra $\g$. this family is parameterized by collections of pairwise distinct complex numbers $z_1,...,z_n$. we obtain some new commutative subalgebras in $u(\g)^{\otimes n}$ as limit cases of gaudin subalgebras. these commutative subalgebras turn to be related to the hamiltonians of bending flows and to the gelfand--tsetlin bases. we use this to prove the simplicity of spectrum in the gaudin model for some new cases."
"we revisit the ""epsilon-intelligence"" model of toth et al.(2011), that was proposed as a minimal framework to understand the square-root dependence of the impact of meta-orders on volume in financial markets. the basic idea is that most of the daily liquidity is ""latent"" and furthermore vanishes linearly around the current price, as a consequence of the diffusion of the price itself. however, the numerical implementation of toth et al. was criticised as being unrealistic, in particular because all the ""intelligence"" was conferred to market orders, while limit orders were passive and random. in this work, we study various alternative specifications of the model, for example allowing limit orders to react to the order flow, or changing the execution protocols. by and large, our study lends strong support to the idea that the square-root impact law is a very generic and robust property that requires very few ingredients to be valid. we also show that the transition from super-diffusion to sub-diffusion reported in toth et al. is in fact a cross-over, but that the original model can be slightly altered in order to give rise to a genuine phase transition, which is of interest on its own. we finally propose a general theoretical framework to understand how a non-linear impact may appear even in the limit where the bias in the order flow is vanishingly small."
the linear eigenvalue problem governing the stability of the mechanical equilibrium of the fluid in a electrohydrodynamic (ehd) convection problem is investigated. the analytical study is one of bifurcation. this allows us to regain the expression of the neutral surface in the classical case. the method used in the numerical study is a galerkin type spectral method based on polynomials and it provides good results.
"in this paper we discuss a general methodology to compute the market risk measure over long time horizons and at extreme percentiles, which are the typical conditions needed for estimating economic capital. the proposed approach extends the usual market-risk measure, ie, value-at-risk (var) at a short-term horizon and 99% confidence level, by properly applying a scaling on the short-term profit-and-loss (p&l) distribution. besides the standard square-root-of-time scaling, based on normality assumptions, we consider two leptokurtic probability density function classes for fitting empirical p&l datasets and derive accurately their scaling behaviour in light of the central limit theorem, interpreting time scaling as a convolution problem. our analyses result in a range of possible var-scaling approaches depending on the distribution providing the best fit to empirical data, the desired percentile level and the time horizon of the economic capital calculation. after assessing the different approaches on a test equity trading portfolio, it emerges that the choice of the var-scaling approach can affect substantially the economic capital calculation. in particular, the use of a convolution-based approach could lead to significantly larger risk measures (by up to a factor of four) than those calculated using normal assumptions on the p&l distribution."
"in high-dimensional linear regression, the goal pursued here is to estimate an unknown regression function using linear combinations of a suitable set of covariates. one of the key assumptions for the success of any statistical procedure in this setup is to assume that the linear combination is sparse in some sense, for example, that it involves only few covariates. we consider a general, non necessarily linear, regression with gaussian noise and study a related question that is to find a linear combination of approximating functions, which is at the same time sparse and has small mean squared error (mse). we introduce a new estimation procedure, called exponential screening that shows remarkable adaptation properties. it adapts to the linear combination that optimally balances mse and sparsity, whether the latter is measured in terms of the number of non-zero entries in the combination ($\ell_0$ norm) or in terms of the global weight of the combination ($\ell_1$ norm). the power of this adaptation result is illustrated by showing that exponential screening solves optimally and simultaneously all the problems of aggregation in gaussian regression that have been discussed in the literature. moreover, we show that the performance of the exponential screening estimator cannot be improved in a minimax sense, even if the optimal sparsity is known in advance. the theoretical and numerical superiority of exponential screening compared to state-of-the-art sparse procedures is also discussed."
"the noninvasive procedures for neural connectivity are under questioning. theoretical models sustain that the electromagnetic field registered at external sensors is elicited by currents at neural space. nevertheless, what we observe at the sensor space is a superposition of projected fields, from the whole gray-matter. this is the reason for a major pitfall of noninvasive electrophysiology methods: distorted reconstruction of neural activity and its connectivity or leakage. it has been proven that current methods produce incorrect connectomes. somewhat related to the incorrect connectivity modelling, they disregard either systems theory and bayesian information theory. we introduce a new formalism that attains for it, hidden gaussian graphical state-model (higgs). a neural gaussian graphical model (ggm) hidden by the observation equation of magneto-encephalographic (meeg) signals. higgs is equivalent to a frequency domain linear state space model (lssm) but with sparse connectivity prior. the mathematical contribution here is the theory for high-dimensional and frequency-domain higgs solvers. we demonstrate that higgs can attenuate the leakage effect in the most critical case: the distortion eeg signal due to head volume conduction heterogeneities. its application in eeg is illustrated with retrieved connectivity patterns from human steady state visual evoked potentials (ssvep). we provide for the first time confirmatory evidence for noninvasive procedures of neural connectivity: concurrent eeg and electrocorticography (ecog) recordings on monkey. open source packages are freely available online, to reproduce the results presented in this paper and to analyze external meeg databases."
"we study the nonlinear eigenvalue problem $-{\rm div}(a(|\nabla u|)\nabla u)=\lambda|u|^{q(x)-2}u$ in $\omega$, $u=0$ on $\partial\omega$, where $\omega$ is a bounded open set in $\rr^n$ with smooth boundary, $q$ is a continuous function, and $a$ is a nonhomogeneous potential. we establish sufficient conditions on $a$ and $q$ such that the above nonhomogeneous quasilinear problem has continuous families of eigenvalues. the proofs rely on elementary variational arguments. the abstract results of this paper are illustrated by the cases $a(t)=t^{p-2}\log (1+t^r)$ and $a(t)= t^{p-2} [\log (1+t)]^{-1}$."
"we demonstrate a novel way to efficiently and very robust create an entanglement between an atomic and a photonic qubit. a single laser beam is used to excite one atomic ensemble and two different spatial modes of scattered raman fields are collected to generate the atom-photon entanglement. with the help of build-in quantum memory, the entanglement still exists after 20.5 $\mu$s storage time which is further proved by the violation of chsh type bell's inequality. our entanglement procedure is the building block for a novel robust quantum repeater architecture [zhao et al, phys. rev. lett. 98, 240502 (2007)]. our approach can be easily extended to generate high dimensional atom-photon entanglements."
"we show that a simple modification of the 1-nearest neighbor classifier yields a strongly bayes consistent learner. prior to this work, the only strongly bayes consistent proximity-based method was the k-nearest neighbor classifier, for k growing appropriately with sample size. we will argue that a margin-regularized 1-nn enjoys considerable statistical and algorithmic advantages over the k-nn classifier. these include user-friendly finite-sample error bounds, as well as time- and memory-efficient learning and test-point evaluation algorithms with a principled speed-accuracy tradeoff. encouraging empirical results are reported."
"for a given discrete decomposable graphical model, we identify several alternative parametrizations, and construct the corresponding reference priors for suitable groupings of the parameters. specifically, assuming that the cliques of the graph are arranged in a perfect order, the parameters we consider are conditional probabilities of clique-residuals given separators, as well as generalized log-odds-ratios. we also consider a parametrization associated to a collection of variables representing a cut for the statistical model. the reference priors we obtain do not depend on the order of the groupings, belong to a conjugate family, and are proper."
"we assess the impact of a hausman pretest, applied to panel data, on a confidence interval for the slope, conditional on the observed values of the time-varying covariate. this assessment has the advantages that it (a) relates to the values of this covariate at hand, (b) is valid irrespective of how this covariate is generated, (c) uses finite sample results and (d) results in an assessment that is determined by the values of this covariate and only 2 unknown parameters. our conditional analysis shows that the confidence interval constructed after a hausman pretest should not be used."
"there is a growing need for the ability to analyse interval-valued data. however, existing descriptive frameworks to achieve this ignore the process by which interval-valued data are typically constructed; namely by the aggregation of real-valued data generated from some underlying process. in this article we develop the foundations of likelihood based statistical inference for random intervals that directly incorporates the underlying generative procedure into the analysis. that is, it permits the direct fitting of models for the underlying real-valued data given only the random interval-valued summaries. this generative approach overcomes several problems associated with existing methods, including the rarely satisfied assumption of within-interval uniformity. the new methods are illustrated by simulated and real data analyses."
"a comparison between linear stability analysis and observations of pulsation modes in five delta scuti stars, belonging to the same cluster, is presented. the study is based on the work by michel et al. (1999), in which such a comparison was performed for a representative set of model solutions obtained independently for each individual star considered. in this paper we revisit the work by michel et al. (1999) following, however, a new approach which consists in the search for a single, complete, and coherent solution for all the selected stars, in order to constrain and test the assumed physics describing these objects. to do so, refined descriptions for the effects of rotation on the determination of the global stellar parameters and on the adiabatic oscillation frequency computations are used. in addition, a crude attempt is made to study the role of rotation on the prediction of mode instabilities.the present results are found to be comparable with those reported by michel et al. (1999). within the temperature range log t_eff = 3.87-3.88 agreement between observations and model computations of unstable modes is restricted to values for the mixing-length parameter alpha_nl less or equal to 1.50. this indicates that for these stars a smaller value for alpha_nl is required than suggested from a calibrated solar model. we stress the point that the linear stability analysis used in this work still assumes stellar models without rotation and that further developments are required for a proper description of the interaction between rotation and pulsation dynamics."
"the identification of key populations shaping the structure and connectivity of metapopulation systems is a major challenge in population ecology. the use of molecular markers in the theoretical framework of population genetics has allowed great advances in this field, but the prime question of quantifying the role of each population in the system remains unresolved. furthermore, the use and interpretation of classical methods are still bounded by the need for a priori information and underlying assumptions that are seldom respected in natural systems. network theory was applied to map the genetic structure in a metapopulation system using microsatellite data from populations of a threatened seagrass, posidonia oceanica, across its whole geographical range. the network approach, free from a priori assumptions and of usual underlying hypothesis required for the interpretation of classical analysis, allows both the straightforward characterization of hierarchical population structure and the detection of populations acting as hubs critical for relaying gene flow or sustaining the metapopulation system. this development opens major perspectives in ecology and evolution in general, particularly in areas such as conservation biology and epidemiology, where targeting specific populations is crucial."
"we consider a model of matching in trading networks in which firms can enter into bilateral contracts. in trading networks, stable outcomes, which are immune to deviations of arbitrary sets of firms, may not exist. we define a new solution concept called trail stability. trail-stable outcomes are immune to consecutive, pairwise deviations between linked firms. we show that any trading network with bilateral contracts has a trail-stable outcome whenever firms' choice functions satisfy the full substitutability condition. for trail-stable outcomes, we prove results on the lattice structure, the rural hospitals theorem, strategy-proofness, and comparative statics of firm entry and exit. we also introduce weak trail stability which is implied by trail stability under full substitutability. we describe relationships between the solution concepts."
"in a multiple testing problem where one is willing to tolerate a few false rejections, procedure controlling the familywise error rate (fwer) can potentially be improved in terms of its ability to detect false null hypotheses by generalizing it to control the $k$-fwer, the probability of falsely rejecting at least $k$ null hypotheses, for some fixed $k>1$. simes' test for testing the intersection null hypothesis is generalized to control the $k$-fwer weakly, that is, under the intersection null hypothesis, and hochberg's stepup procedure for simultaneous testing of the individual null hypotheses is generalized to control the $k$-fwer strongly, that is, under any configuration of the true and false null hypotheses. the proposed generalizations are developed utilizing joint null distributions of the $k$-dimensional subsets of the $p$-values, assumed to be identical. the generalized simes' test is proved to control the $k$-fwer weakly under the multivariate totally positive of order two (mtp$_2$) condition [j. multivariate analysis 10 (1980) 467--498] of the joint null distribution of the $p$-values by generalizing the original simes' inequality. it is more powerful to detect $k$ or more false null hypotheses than the original simes' test when the $p$-values are independent. a stepdown procedure strongly controlling the $k$-fwer, a version of generalized holm's procedure that is different from and more powerful than [ann. statist. 33 (2005) 1138--1154] with independent $p$-values, is derived before proposing the generalized hochberg's procedure. the strong control of the $k$-fwer for the generalized hochberg's procedure is established in situations where the generalized simes' test is known to control its $k$-fwer weakly."
"we propose an extended spatial evolutionary public goods game (sepgg) model to study the dynamics of individual career choice and the corresponding social output. based on the social value orientation theory, we categorized two classes of work, namely the public work if it serves public interests, and the private work if it serves personal interests. under the context of sepgg, choosing public work is to cooperate and choosing private work is to defect. we then investigate the effects of employee productivity, human capital and external subsidies on individual career choices of the two work types, as well as the overall social welfare. from simulation results, we found that when employee productivity of public work is low, people are more willing to enter the private sector. although this will make both the effort level and human capital of individuals doing private work higher than those engaging in public work, the total outcome of the private sector is still lower than that of the public sector provided a low level of public subsidies. when the employee productivity is higher for public work, a certain amount of subsidy can greatly improve system output. on the contrary, when the employee productivity of public work is low, provisions of subsidy to the public sector can result in a decline in social output."
"in this paper we propose a new approach for constructing \emph{multivariate} gaussian random fields (grfs) with oscillating covariance functions through systems of stochastic partial differential equations (spdes). we discuss how to build systems of spdes that introduces oscillation characteristics in the covariance functions of the multivariate grfs. by choosing different parametrization of the equations, some grfs can be made with oscillating covariance functions but other fields can have mat\'ern covariance functions or close to mat\'ern covariance functions. the multivariate grfs constructed by solving the systems of spdes automatically fulfill the hard requirement of nonnegative definiteness for the covariance functions. the approximate weak solutions to the systems of spdes are used to represent the multivariate grfs by multivariate gaussian \emph{markov} random fields (gmrfs). since the multivariate gmrfs have sparse precision matrices (inverse of the covariance matrices), numerical algorithms for sparse matrices can be applied to the precision matrices for sampling and inference. thus from a computational point of view, the \emph{big-n} problem can be partially solved with these types of models. another advantage of the method is that the oscillation in the covariance function can be controlled directly by the parameters in the system of spdes. we show how to use this proposed approach with simulated data and real data examples."
"we study streaming principal component analysis (pca), that is to find, in $o(dk)$ space, the top $k$ eigenvectors of a $d\times d$ hidden matrix $\bf \sigma$ with online vectors drawn from covariance matrix $\bf \sigma$.   we provide $\textit{global}$ convergence for oja's algorithm which is popularly used in practice but lacks theoretical understanding for $k>1$. we also provide a modified variant $\mathsf{oja}^{++}$ that runs $\textit{even faster}$ than oja's. our results match the information theoretic lower bound in terms of dependency on error, on eigengap, on rank $k$, and on dimension $d$, up to poly-log factors. in addition, our convergence rate can be made gap-free, that is proportional to the approximation error and independent of the eigengap.   in contrast, for general rank $k$, before our work (1) it was open to design any algorithm with efficient global convergence rate; and (2) it was open to design any algorithm with (even local) gap-free convergence rate in $o(dk)$ space."
"kin selection theory is a kind of causal analysis. the initial form of kin selection ascribed cause to costs, benefits, and genetic relatedness. the theory then slowly developed a deeper and more sophisticated approach to partitioning the causes of social evolution. controversy followed because causal analysis inevitably attracts opposing views. it is always possible to separate total effects into different component causes. alternative causal schemes emphasize different aspects of a problem, reflecting the distinct goals, interests, and biases of different perspectives. for example, group selection is a particular causal scheme with certain advantages and significant limitations. ultimately, to use kin selection theory to analyze natural patterns and to understand the history of debates over different approaches, one must follow the underlying history of causal analysis. this article describes the history of kin selection theory, with emphasis on how the causal perspective improved through the study of key patterns of natural history, such as dispersal and sex ratio, and through a unified approach to demographic and social processes. independent historical developments in the multivariate analysis of quantitative traits merged with the causal analysis of social evolution by kin selection."
"we present the results of a search for new particles that lead to a \z boson plus jets in $p\bar{p}$ collisions at $\sqrt{s}=1.96$ tev using the collider detector at fermilab (cdf ii). a data sample with a luminosity of 1.06 \ifb\ collected using \z boson decays to $ee$ and $\mu\mu$ is used. we describe a completely data-based method to predict the dominant background from standard-model \z+jet events. this method can be similarly applied to other analyses requiring background predictions in multi-jet environments, as shown when validating the method by predicting the background from $w$+jets in \ttbar production. no significant excess above the background prediction is observed, and a limit is set using a fourth generation quark model to quantify the acceptance. assuming $br(b' \to b\z) = 100%$ and using a leading-order calculation of the $b'$ cross section, $b'$ quark masses below 268 $\gev/c^2$ are excluded at 95% confidence level."
"we propose a general framework for reduced-rank modeling of matrix-valued data. by applying a generalized nuclear norm penalty we can directly model low-dimensional latent variables associated with rows and columns. our framework flexibly incorporates row and column features, smoothing kernels, and other sources of side information by penalizing deviations from the row and column models. moreover, a large class of these models can be estimated scalably using convex optimization. the computational bottleneck in each case is one singular value decomposition per iteration of a large but easy-to-apply matrix. our framework generalizes traditional convex matrix completion and multi-task learning methods as well as maximum a posteriori estimation under a large class of popular hierarchical bayesian models."
"we present a theoretical study of the superconducting gap function in pros4sb12 using a symmetry-based approach. a three-component order parameter in the triplet channel best describes superconductivity. the gap function is non-degenerate and the lower branch has four cusp nodes at unusual points of the fermi surface, which lead to power law behaviours in the density of states, specific heat and nuclear spin relaxation rate."
"we present two paradigms relating algebraic, topological and quantum computational statistics for the topological model for quantum computation. in particular we suggest correspondences between the computational power of topological quantum computers, computational complexity of link invariants and images of braid group representations. while at least parts of these paradigms are well-known to experts, we provide supporting evidence for them in terms of recent results. we give a fairly comprehensive list of known examples and formulate two conjectures that would further support the paradigms."
"building higher-dimensional copulas is generally recognized as a difficult problem. regular-vines using bivariate copulas provide a flexible class of high-dimensional dependency models. in large dimensions, the drawback of the model is the exponentially increasing complexity. recognizing some of the conditional independences is a possibility for reducing the number of levels of the pair-copula decomposition, and hence to simplify its construction aas et al (2009). the idea of using conditional independences was already performed under elliptical copula assumptions hanea, kurowicka and cooke (2006), kurowicka and cooke (2002) and in the case of dags in a recent work bauer, czado and klein (2011). we provide a method which uses some of the conditional independences encoded by the markov network underlying the variables. we give a theorem which under some graph conditions makes possible to derive pair-copula decomposition of the probability density function associated to a markov network. as the underlying markov network is usually unknown, we first have to discover it from the sample data. using our results published in szantai and kovacs (2008) and kovacs and szantai (2010a) we will show how to derive a multidimensional copula model exploiting the information on conditional independences hidden in the sample data."
"this article shows how to fit reticulate finite and infinite sites sequence spectra to aligned data from five modern human genomes (san, yoruba, french, han and papuan) plus two archaic humans (denisovan and neanderthal), to better infer demographic parameters. these include interbreeding between distinct lineages. major improvements in the fit of the sequence spectrum are made with successively more complicated models. findings include some evidence of a male biased gene flow from the denisova lineage to papuan ancestors and possibly even more archaic gene flow. it is unclear if there is evidence for more than one neanderthal interbreeding, as the evidence suggesting this largely disappears when a finite sites model is fitted."
"particle probability hypothesis density filtering has become a promising means for multi-target tracking due to its capability of handling an unknown and time-varying number of targets in non-linear non-gaussian system. however, its computational complexity grows linearly with the number of measurements and particles assigned to each target, and this can be very time consuming especially when numerous targets and clutter exist in the surveillance region. addressing this issue, we present a distributed computation particle phd filter for target tracking. its framework consists of several local particle phd filters at each processing element and a central unit. each processing element takes responsibility for part particles but full measurements and provides local estimates; central unit controls particle exchange between processing elements and specifies a fusion rule to match and fuse the estimates from different local filters. the proposed framework is suitable for parallel implementation and maintains the tracking accuracy. simulations verify the proposed method can provide comparative accuracy as well as a significant speedup with the standard particle phd filter."
"we establish the first hardness results for the problem of computing the value of one-round games played by a verifier and a team of provers who can share quantum entanglement. in particular, we show that it is np-hard to approximate within an inverse polynomial the value of a one-round game with (i) quantum verifier and two entangled provers or (ii) classical verifier and three entangled provers. previously it was not even known if computing the value exactly is np-hard. we also describe a mathematical conjecture, which, if true, would imply hardness of approximation to within a constant.   we start our proof by describing two ways to modify classical multi-prover games to make them resistant to entangled provers. we then show that a strategy for the modified game that uses entanglement can be ``rounded'' to one that does not. the results then follow from classical inapproximability bounds. our work implies that, unless p=np, the values of entangled-prover games cannot be computed by semidefinite programs that are polynomial in the size of the verifier's system, a method that has been successful for more restricted quantum games."
"we present an experimental investigation on the scaling of resistance in individual single walled carbon nanotube devices with channel lengths that vary four orders of magnitude on the same sample. the electron mean free path is obtained from the linear scaling of resistance with length at various temperatures. the low temperature mean free path is determined by impurity scattering, while at high temperature the mean free path decreases with increasing temperature, indicating that it is limited by electron-phonon scattering. an unusually long mean free path at room temperature has been experimentally confirmed. exponentially increasing resistance with length at extremely long length scales suggests anomalous localization effects."
"we propose a non-parametric statistical procedure for detecting multiple change-points in multidimensional signals. the method is based on a test statistic that generalizes the well-known kruskal-wallis procedure to the multivariate setting. the proposed approach does not require any knowledge about the distribution of the observations and is parameter-free. it is computationally efficient thanks to the use of dynamic programming and can also be applied when the number of change-points is unknown. the method is shown through simulations to be more robust than alternatives, particularly when faced with atypical distributions (e.g., with outliers), high noise levels and/or high-dimensional data."
"many theoretical and experimental studies suggest that range expansions can have severe consequences for the gene pool of the expanding population. due to strongly enhanced genetic drift at the advancing frontier, neutral and weakly deleterious mutations can reach large frequencies in the newly colonized regions, as if they were surfing the front of the range expansion. these findings raise the question of how frequently beneficial mutations successfully surf at shifting range margins, thereby promoting adaptation towards a range-expansion phenotype. here, we use individual-based simulations to study the surfing statistics of recurrent beneficial mutations on wave-like range expansions in linear habitats. we show that the rate of surfing depends on two strongly antagonistic factors, the probability of surfing given the spatial location of a novel mutation and the rate of occurrence of mutations at that location. the surfing probability strongly increases towards the tip of the wave. novel mutations are unlikely to surf unless they enjoy a spatial head start compared to the bulk of the population. the needed head start is shown to be proportional to the inverse fitness of the mutant type, and only weakly dependent on the carrying capacity. the second factor is the mutation occurrence which strongly decreases towards the tip of the wave. thus, most successful mutations arise at an intermediate position in the front of the wave. we present an analytic theory for the tradeoff between these factors that allows to predict how frequently substitutions by beneficial mutations occur at invasion fronts. we find that small amounts of genetic drift increase the fixation rate of beneficial mutations at the advancing front, and thus could be important for adaptation during species invasions."
"we investigate higher topological cyclic homology as an approach to studying chromatic phenomena in homotopy theory. higher topological cyclic homology is constructed from the fixed points of a version of topological hochschild homology based on the n-dimensional torus, and we propose it as a computationally tractable cousin of n-fold iterated algebraic k-theory.   the fixed points of toral topological hochschild homology are related to one another by restriction and frobenius operators. we introduce two additional families of operators on fixed points, the verschiebung, indexed on self-isogenies of the n-torus, and the differentials, indexed on n-vectors. we give a detailed analysis of the relations among the restriction, frobenius, verschiebung, and differentials, producing a higher analog of the structure hesselholt and madsen described for 1-dimensional topological cyclic homology.   we calculate two important pieces of higher topological cyclic homology, namely topological restriction homology and topological frobenius homology, for the sphere spectrum. the latter computation allows us to establish the segal conjecture for the torus, which is to say to completely compute the cohomotopy type of the classifying space of the torus."
"we present measurements of resonant tunneling through discrete energy levels of a silicon double quantum dot formed in a thin silicon-on-insulator layer. in the absence of piezoelectric phonon coupling, spontaneous phonon emission with deformation-potential coupling accounts for inelastic tunneling through the ground states of the two dots. such transport measurements enable us to observe a pauli spin blockade due to effective two-electron spin-triplet correlations, evident in a distinct bias-polarity dependence of resonant tunneling through the ground states. the blockade is lifted by the excited-state resonance by virtue of efficient phonon emission between the ground states. our experiment demonstrates considerable potential for investigating silicon-based spin dynamics and spin-based quantum information processing."
"a simple hawkes model have been developed for the price tick structure dynamics incorporating market microstructure noise and trade clustering. in this paper, the model is extended with random mark to deal with more realistic price tick structures of equities. we examine the impact of jump in price dynamics to the future movements and dependency between the jump sizes and ground intensities. we also derive the volatility formula based on stochastic and statistical methods and compare with realized volatility in simulation and empirical studies. the marked hawkes model is useful to estimate the intraday volatility similarly in the case of simple hawkes model."
"corresponding to fresnel transform there exists a unitary operator in quantum optics theory, which could be named fresnel operator (fo). we show that the multiplication rule of fo naturally leads to the quantum optical abcd law. the canonical operator methods as mapping of ray-transfer abcd matrix is explicitly shown by fo's normally ordered expansion through the coherent state representation and the technique of integration within an ordered product of operators. we show that time evolution of the damping oscillator embodies the quantum optical abcd law."
"we give concentration bounds for martingales that are uniform over finite times and extend classical hoeffding and bernstein inequalities. we also demonstrate our concentration bounds to be optimal with a matching anti-concentration inequality, proved using the same method. together these constitute a finite-time version of the law of the iterated logarithm, and shed light on the relationship between it and the central limit theorem."
"abnormalities in pupillary light reflex can indicate optic nerve disorders that may lead to permanent visual loss if not diagnosed in an early stage. in this study, we focus on relative afferent pupillary defect (rapd), which is based on the difference between the reactions of the eyes when they are exposed to light stimuli. incumbent rapd assessment methods are based on subjective practices that can lead to unreliable measurements. to eliminate subjectivity and obtain reliable measurements, we introduced an automated framework to detect rapd. for validation, we conducted a clinical study with lab-on-a-headset, which can perform automated light reflex test. in addition to benchmarking handcrafted algorithms, we proposed a transfer learning-based approach that transformed a deep learning-based generic object recognition algorithm into a pupil detector. based on the conducted experiments, proposed algorithm rapdnet can achieve a sensitivity and a specificity of 90.6% over 64 test cases in a balanced set, which corresponds to an auc of 0.929 in roc analysis. according to our benchmark with three handcrafted algorithms and nine performance metrics, rapdnet outperforms all other algorithms in every performance category."
"learning the markov network structure from data is a problem that has received considerable attention in machine learning, and in many other application fields. this work focuses on a particular approach for this purpose called independence-based learning. such approach guarantees the learning of the correct structure efficiently, whenever data is sufficient for representing the underlying distribution. however, an important issue of such approach is that the learned structures are encoded in an undirected graph. the problem with graphs is that they cannot encode some types of independence relations, such as the context-specific independences. they are a particular case of conditional independences that is true only for a certain assignment of its conditioning set, in contrast to conditional independences that must hold for all its assignments. in this work we present cspc, an independence-based algorithm for learning structures that encode context-specific independences, and encoding them in a log-linear model, instead of a graph. the central idea of cspc is combining the theoretical guarantees provided by the independence-based approach with the benefits of representing complex structures by using features in a log-linear model. we present experiments in a synthetic case, showing that cspc is more accurate than the state-of-the-art ib algorithms when the underlying distribution contains csis."
"bitcoin is considered the most valuable currency in the world. besides being highly valuable, its value has also experienced a steep increase, from around 1 dollar in 2010 to around 18000 in 2017. then, in recent years, it has attracted considerable attention in a diverse set of fields, including economics and computer science. the former mainly focuses on studying how it affects the market, determining reasons behinds its price fluctuations, and predicting its future prices. the latter mainly focuses on its vulnerabilities, scalability, and other techno-crypto-economic issues. here, we aim at revealing the usefulness of traditional autoregressive integrative moving average (arima) model in predicting the future value of bitcoin by analyzing the price time series in a 3-years-long time period. on the one hand, our empirical studies reveal that this simple scheme is efficient in sub-periods in which the behavior of the time-series is almost unchanged, especially when it is used for short-term prediction, e.g. 1-day. on the other hand, when we try to train the arima model to a 3-years-long period, during which the bitcoin price has experienced different behaviors, or when we try to use it for a long-term prediction, we observe that it introduces large prediction errors. especially, the arima model is unable to capture the sharp fluctuations in the price, e.g. the volatility at the end of 2017. then, it calls for more features to be extracted and used along with the price for a more accurate prediction of the price. we have further investigated the bitcoin price prediction using an arima model, trained over a large dataset, and a limited test window of the bitcoin price, with length $w$, as inputs. our study sheds lights on the interaction of the prediction accuracy, choice of ($p,q,d$), and window size $w$."
"we present a multiwavelength study of the poor cluster rx j1117.4+0743 ([vmf 98] 097) at z=0.485, based on gmos/gemini south g', r' photometry and spectroscopy, and xmm-newton observations. we examine its nature and surroundings by analyzing the projected galaxy distribution, the galaxy velocity distribution, the weak-lensing mass reconstruction, and the x-ray spectroscopy and imaging. the cluster shows a complex morphology. it is composed by at least two structures along the line-of-sight, with velocity dispersions of 592+-82 km s^-1 and 391+-85 km s^-1 respectively. other structures are also detected in x-ray, in the galaxy projected number density map, and by weak-lensing. one of these clumps, could be gravitationally bound and associated to the main cluster. the derived temperature and bolometric x-ray luminosity reveal that [vmf 98] 097 behave like a normal cluster, in agreement with lx-tx correlation found for both local (z=0) and moderate redshift (z~0.4) clusters. we find that the mass determination inferred from weak-lensing is in average 3 to 4.8 times higher (depending on the model assumed) than the x-ray mass. we have two possible explanations for this discrepancy: i) the cluster is in non-equilibrium, then the deviation of the x-ray estimated mass from the true value can be as high as a factor of two; ii) the intervening mass along the line-of-sight of the cluster is producing an over-estimation of the weak-lensing mass. based on the analysis presented, we conclude that [vmf 98] 097 is a perturbed cluster with at least two substructures in velocity space and with other nearby structures at projected distances of about 1 h^-1 mpc. this cluster is an example of a poor cluster caught in the process of accreting sub-structures to become a rich cluster."
"the annihilation of dark matter (dm) in the galaxy could produce specific imprints on the spectra of antimatter species in galactic cosmic rays, which could be detected by upcoming experiments such as pamela and ams02. recent studies show that the presence of substructures can enhance the annihilation signal by a ""boost factor"" that not only depends on energy, but that is intrinsically a statistical property of the distribution of dm substructures inside the milky way. we investigate a scenario in which substructures consist of $\sim 100$ ""mini-spikes"" around intermediate-mass black holes. focusing on primary positrons and antiprotons, we find large boost factors, up to a few thousand, that exhibit a large variance at high energy in the case of positrons and at low energy in the case of antiprotons. as a consequence, an estimate of the dm particle mass based on the observed cut-off in the positron spectrum could lead to a substantial underestimate of its actual value."
"we introduce a price impact model which accounts for finite market depth, tightness and resilience. its coupled bid- and ask-price dynamics induce convex liquidity costs. we provide existence of an optimal solution to the classical problem of maximizing expected utility from terminal liquidation wealth at a finite planning horizon. in the specific case when market uncertainty is generated by an arithmetic brownian motion with drift and the investor exhibits constant absolute risk aversion, we show that the resulting singular optimal stochastic control problem readily reduces to a deterministic optimal tracking problem of the optimal frictionless constant merton portfolio in the presence of convex costs. rather than studying the associated hamilton-jacobi-bellmann pde, we exploit convex analytic and calculus of variations techniques allowing us to construct the solution explicitly and to describe the free boundaries of the action- and non-action regions in the underlying state space. as expected, it is optimal to trade towards the frictionless merton position, taking into account the initial bid-ask spread as well as the optimal liquidation of the accrued position when approaching terminal time. it turns out that this leads to a surprisingly rich phenomenology of possible trajectories for the optimal share holdings."
"we carried out a target survey for lyman break galaxies (lbgs) and lyman alpha emitters (laes) around qso sdss j0211-0009 at z=4.87. the deep and wide broadband and narrowband imaging simultaneously revealed the perspective structure of these two high-z populations. the lbgs without ly-alpha emission form a filamentary structure including the qso, while the laes are distributed around the qso but avoid it within a distance of ~4.5mpc. on the other hand, we serendipitously discovered a protocluster with a significant concentration of lbgs and laes where no strongly uv ionizing source such as a qso or radio galaxy is known to exist. in this cluster field, two populations are spatially cross-correlated with each other. the relative spatial distribution of laes to lbgs is in stark contrast between the qso and the cluster fields. we also found a weak trend showing that the number counts based on ly-alpha and uv continuum fluxes of laes in the qso field are slightly lower than in the cluster field, whereas the number counts of lbgs are almost consistent with each other. the laes avoid the nearby region around the qso where the local uv background radiation could be ~100 times stronger than the average for the epoch. the clustering segregation between lbgs and laes seen in the qso field could be due to either enhanced early galaxy formation in an overdense environment having caused all the laes to evolve into lbgs, or local photoionization due to the strong uv radiation from the qso effectively causing a deficit in low-mass galaxies like laes."
"in the present work we introduce a novel multi-agent model with the aim to reproduce the dynamics of a double auction market at microscopic time scale through a faithful simulation of the matching mechanics in the limit order book. the agents follow a noise decision making process where their actions are related to a stochastic variable, ""the market sentiment"", which we define as a mixture of public and private information. the model, despite making just few basic assumptions over the trading strategies of the agents, is able to reproduce several empirical features of the high-frequency dynamics of the market microstructure not only related to the price movements but also to the deposition of the orders in the book."
"the continuous progress in fabricating low-dimensional systems with large spin-orbit couplings has reached a point in which nowadays materials may display spin-orbit splitting energies ranging from a few to hundreds of mev. this situation calls for a better understanding of the interplay between the spin-orbit coupling and other interactions ubiquitously present in solids, in particular when the spin-orbit splitting is comparable in magnitude with characteristic energy scales such as the fermi energy and the phonon frequency.   in this article, the two-dimensional fr\""ohlich electron-phonon problem is reformulated by introducing the coupling to a spin-orbit rashba potential, allowing for a description of the spin-orbit effects on the electron-phonon interaction. the ground state of the resulting fr\""ohlich-rashba polaron is studied in the weak and strong coupling limits of the electron-phonon interaction for arbitrary values of the spin-orbit splitting. the weak coupling case is studied within the rayleigh-schr\""odinger perturbation theory, while the strong-coupling electron-phonon regime is investigated by means of variational polaron wave functions in the adiabatic limit. it is found that, for both weak and strong coupling polarons, the ground state energy is systematically lowered by the spin-orbit interaction, indicating that the polaronic character is strengthened by the rashba coupling. it is also shown that, consistently with the lowering of the ground state, the polaron effective mass is enhanced compared to the zero spin-orbit limit. finally, it is argued that the crossover between weakly and strongly coupled polarons can be shifted by the spin-orbit interaction."
"epidemics of communicable diseases place a huge burden on public health infrastructures across the world. producing accurate and actionable forecasts of infectious disease incidence at short and long time scales will improve public health response to outbreaks. however, scientists and public health officials face many obstacles in trying to create accurate and actionable real-time forecasts of infectious disease incidence. dengue is a mosquito-borne virus that annually infects over 400 million people worldwide. we developed a real-time forecasting model for dengue hemorrhagic fever in the 77 provinces of thailand. we created an operational and computational infrastructure that generated multi-step predictions of dengue incidence in thai provinces every two weeks throughout 2014. these predictions show mixed performance across provinces, out-performing na\""ive seasonal models in over half of provinces at a 1.5 month horizon. additionally, to assess the degree to which delays in case reporting make long-range prediction a challenging task, we compared the performance of our real-time predictions with predictions made with fully reported data. this paper provides valuable lessons for the implementation of real-time predictions in the context of public health decision making."
"this paper presents the application of a particle filter for data assimilation in the context of puff-based dispersion models. particle filters provide estimates of the higher moments, and are well suited for strongly nonlinear and/or non-gaussian models. the gaussian puff model scipuff, is used in predicting the chemical concentration field after a chemical incident. this model is highly nonlinear and evolves with variable state dimension and, after sufficient time, high dimensionality. while the particle filter formalism naturally supports variable state dimensionality high dimensionality represents a challenge in selecting an adequate number of particles, especially for the bootstrap version. we present an implementation of the bootstrap particle filter and compare its performance with the scipuff predictions. both the model and the particle filter are evaluated on the dipole pride 26 experimental data. since there is no available ground truth, the data has been divided in two sets: training and testing. we show that even with a modest number of particles, the bootstrap particle filter provides better estimates of the concentration field compared with the process model, without excessive increase in computational complexity."
"this paper focuses on the pricing of the variance swap in an incomplete market where the stochastic interest rate and the price of the stock are respectively driven by cox-ingersoll-ross model and heston model with simultaneous l\'{e}vy jumps. by using the equilibrium framework, we obtain the pricing kernel and the equivalent martingale measure. moreover, under the forward measure instead of the risk neural measure, we give the closed-form solution for the fair delivery price of the discretely sampled variance swap by employing the joint moment generating function of the underlying processes. finally, we provide some numerical examples to depict that the values of variance swaps not only depend on the stochastic interest rates but also increase in the presence of jump risks."
"let $f:\omega\to\ir^2$ be a mapping of finite distortion, where $\omega\subset\ir^2 .$ assume that the distortion function $k(x,f)$ satisfies $e^{k(\cdot, f)}\in l^p_{loc}(\omega)$ for some $p>0.$ we establish optimal regularity and area distortion estimates for $f$. especially, we prove that $|df|^2 \log^{\beta -1}(e + |df|) \in l^1_{loc}(\omega) $ for every $\beta <p.$ this answers positively well known conjectures due to iwaniec and martin \cite{imbook} and to iwaniec, koskela and martin \cite{ikm}."
"gaussian concentration graph models and covariance graph models are two classes of graphical models that are useful for uncovering latent dependence structures among multivariate variables. in the bayesian literature, graphs are often determined through the use of priors over the space of positive definite matrices with fixed zeros, but these methods present daunting computational burdens in large problems. motivated by the superior computational efficiency of continuous shrinkage priors for regression analysis, we propose a new framework for structure learning that is based on continuous spike and slab priors and uses latent variables to identify graphs. we discuss model specification, computation, and inference for both concentration and covariance graph models. the new approach produces reliable estimates of graphs and efficiently handles problems with hundreds of variables."
"representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. however, the semantic structure inherent in observations is oftentimes lost in the process. human perception excels at understanding semantics but cannot always be expressed in terms of labels. thus, \emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. in this work we propose to combine \emph{generative unsupervised feature learning} with a \emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear bayesian latent factor models of the observations. we use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. we show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. in addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables."
"postural instability is prevalent in aging and neurodegenerative disease, decreasing quality of life and independence. quantitatively monitoring balance control is important for assessing treatment efficacy and rehabilitation progress. however, existing technologies for assessing postural sway are complex and expensive, limiting their widespread utility. here, we propose a monocular imaging system capable of assessing sub-millimeter 3d sway dynamics. by physically embedding anatomical targets with known \textit{a priori} geometric models, 3d central and upper body kinematic motion was automatically assessed through geometric feature tracking and 3d kinematic motion inverse estimation from a set of 2d frames. sway was tracked in 3d and compared between control and hypoperfusion conditions. the proposed system demonstrated high agreement with a commercial motion capture system (error $4.4 \times 10^{-16} \pm 0.30$~mm, $r^2=0.9773$). significant differences in sway dynamics were observed in early stance central anterior-posterior sway (control: $147.1 \pm 7.43$~mm, hypoperfusion: $177.8 \pm 15.3$~mm; $p=0.039$) and mid stance upper body coronal sway (control: $106.3 \pm 5.80$~mm, hypoperfusion: $128.1 \pm 18.4$~mm; $p=0.040$) commensurate with cerebral blood flow (cbf) perfusion deficit, followed by recovered sway dynamics during late stance governed by cbf recovery. this inexpensive single-camera system enables quantitative 3d sway monitoring for assessing neuromuscular balance control in weakly constrained environments."
"we consider new formulations and methods for sparse quantile regression in the high-dimensional setting. quantile regression plays an important role in many applications, including outlier-robust exploratory analysis in gene selection. in addition, the sparsity consideration in quantile regression enables the exploration of the entire conditional distribution of the response variable given the predictors and therefore yields a more comprehensive view of the important predictors. we propose a generalized omp algorithm for variable selection, taking the misfit loss to be either the traditional quantile loss or a smooth version we call quantile huber, and compare the resulting greedy approaches with convex sparsity-regularized formulations. we apply a recently proposed interior point methodology to efficiently solve all convex formulations as well as convex subproblems in the generalized omp setting, pro- vide theoretical guarantees of consistent estimation, and demonstrate the performance of our approach using empirical studies of simulated and genomic datasets."
"modified gravity theory is known to violate birkhoff's theorem. we explore a key consequence of this violation, the effect of distant matter in the universe on the motion of test particles. we find that when a particle is accelerated, a force is experienced that is proportional to the particle's mass and acceleration and acts in the direction opposite to that of the acceleration. we identify this force with inertia. at very low accelerations, our inertial law deviates slightly from that of newton, yielding a testable prediction that may be verified with relatively simple experiments. our conclusions apply to all gravity theories that reduce to a yukawa-like force in the weak field approximation."
"probability distributions can be read as simple expressions of information. each continuous probability distribution describes how information changes with magnitude. once one learns to read a probability distribution as a measurement scale of information, opportunities arise to understand the processes that generate the commonly observed patterns. probability expressions may be parsed into four components: the dissipation of all information, except the preservation of average values, taken over the measurement scale that relates changes in observed values to changes in information, and the transformation from the underlying scale on which information dissipates to alternative scales on which probability pattern may be expressed. information invariances set the commonly observed measurement scales and the relations between them. in particular, a measurement scale for information is defined by its invariance to specific transformations of underlying values into measurable outputs. essentially all common distributions can be understood within this simple framework of information invariance and measurement scale."
"hybridization networks are representations of evolutionary histories that allow for the inclusion of reticulate events like recombinations, hybridizations, or lateral gene transfers. the recent growth in the number of hybridization network reconstruction algorithms has led to an increasing interest in the definition of metrics for their comparison that can be used to assess the accuracy or robustness of these methods. in this paper we establish some basic results that make it possible the generalization to tree-child time consistent (tctc) hybridization networks of some of the oldest known metrics for phylogenetic trees: those based on the comparison of the vectors of path lengths between leaves. more specifically, we associate to each hybridization network a suitably defined vector of `splitted' path lengths between its leaves, and we prove that if two tctc hybridization networks have the same such vectors, then they must be isomorphic. thus, comparing these vectors by means of a metric for real-valued vectors defines a metric for tctc hybridization networks. we also consider the case of fully resolved hybridization networks, where we prove that simpler, `non-splitted' vectors can be used."
"despite their inherent non-equilibrium nature, living systems can self-organize in highly ordered collective states that share striking similarities with the thermodynamic equilibrium phases of conventional condensed matter and fluid systems. examples range from the liquid-crystal-like arrangements of bacterial colonies, microbial suspensions and tissues to the coherent macro-scale dynamics in schools of fish and flocks of birds. yet, the generic mathematical principles that govern the emergence of structure in such artificial and biological systems are elusive. it is not clear when, or even whether, well-established theoretical concepts describing universal thermostatistics of equilibrium systems can capture and classify ordered states of living matter. here, we connect these two previously disparate regimes: through microfluidic experiments and mathematical modelling, we demonstrate that lattices of hydrodynamically coupled bacterial vortices can spontaneously organize into distinct phases of ferro- and antiferromagnetic order. the preferred phase can be controlled by tuning the vortex coupling through changes of the inter-cavity gap widths. the emergence of opposing order regimes is tightly linked to the existence of geometry-induced edge currents, reminiscent of those in quantum systems. our experimental observations can be rationalized in terms of a generic lattice field theory, suggesting that bacterial spin networks belong to the same universality class as a wide range of equilibrium systems."
"we consider t--shaped, two--dimensional quantum waveguides containing attractive or repulsive impurities with a smooth, realistic shape, and study how the resonance behavior of the total conductance depends upon the strength of the defect potential and the geometry of the device. the resonance parameters are determined locating the relevant s-matrix poles in the riemann energy surface. the total scattering operator is obtained from the s-matrices of the various constituent segments of the device through the *-product composition rule. this allows for a numerically stable evaluation of the scattering matrix and of the resonance parameters."
we study the association between physical appearance and family income using a novel data which has 3-dimensional body scans to mitigate the issue of reporting errors and measurement errors observed in most previous studies. we apply machine learning to obtain intrinsic features consisting of human body and take into account a possible issue of endogenous body shapes. the estimation results show that there is a significant relationship between physical appearance and family income and the associations are different across the gender. this supports the hypothesis on the physical attractiveness premium and its heterogeneity across the gender.
"in this talk, i review the effective theory approach to unstable particle production and present results of a calculation of the process e- e+ ->mu- nubar_mu u dbar x near the w-pair production threshold up to next-to-leading order in gammaw/mw ~ alpha ~ v^2. the remaining theoretical uncertainty and the impact on the measurement of the w mass is discussed."
"peer grading is the process of students reviewing each others' work, such as homework submissions, and has lately become a popular mechanism used in massive open online courses (moocs). intrigued by this idea, we used it in a course on algorithms and data structures at the university of hamburg. throughout the whole semester, students repeatedly handed in submissions to exercises, which were then evaluated both by teaching assistants and by a peer grading mechanism, yielding a large dataset of teacher and peer grades. we applied different statistical and machine learning methods to aggregate the peer grades in order to come up with accurate final grades for the submissions (supervised and unsupervised, methods based on numeric scores and ordinal rankings). surprisingly, none of them improves over the baseline of using the mean peer grade as the final grade. we discuss a number of possible explanations for these results and present a thorough analysis of the generated dataset."
"by applying a standard solution-generating transformation to an arbitrary vacuum bianchi type ii solution, one generates a new solution with spikes commonly observed in numerical simulations. it is conjectured that the spike solution is part of the generalized mixmaster attractor."
"do there exist circular and spherical copulas in $r^d$? that is, do there exist circularly symmetric distributions on the unit disk in $r^2$ and spherically symmetric distributions on the unit ball in $r^d$, $d\ge3$, whose one-dimensional marginal distributions are uniform? the answer is yes for $d=2$ and 3, where the circular and spherical copulas are unique and can be determined explicitly, but no for $d\ge4$. a one-parameter family of elliptical bivariate copulas is obtained from the unique circular copula in $r^2$ by oblique coordinate transformations. copulas obtained by a non-linear transformation of a uniform distribution on the unit ball in $r^d$ are also described, and determined explicitly for $d=2$."
the conventional magnetic induction equation that governs hydromagnetic dynamo action is transformed into an equivalent integral equation system. an advantage of this approach is that the computational domain is restricted to the region occupied by the electrically conducting fluid and to its boundary. this integral equation approach is first employed to simulate kinematic dynamos excited by beltrami-like flows in a finite cylinder. the impact of externally added layers around the cylinder on the onset of dynamo actions is investigated. then it is applied to simulate dynamo experiments within cylindrical geometry including the von karman sodium (vks) experiment and the riga dynamo experiment. a modified version of this approach is utilized to investigate magnetic induction effects under the influence of externally applied magnetic fields which is also important to measure the proximity of a given dynamo facility to the self-excitation threshold.
"we study the stochastic multi-armed bandit problem when one knows the value $\mu^{(\star)}$ of an optimal arm, as a well as a positive lower bound on the smallest positive gap $\delta$. we propose a new randomized policy that attains a regret {\em uniformly bounded over time} in this setting. we also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows $\delta$, and bounded regret of order $1/\delta$ is not possible if one only knows $\mu^{(\star)}$"
"properties of inhomogeneous nuclear matter are evaluated within a relativistic mean field approximation using density dependent coupling constants. a parameterization for these coupling constants is presented, which reproduces the properties of the nucleon self-energy obtained in dirac brueckner hartree fock calculations of asymmetric nuclear matter but also provides a good description for bulk properties of finite nuclei. the inhomogeneous infinite matter is described in terms of cubic wigner-seitz cells, which allows for a microscopic description of the structures in the so-called ``pasta-phase'' of nuclear configurations and provides a smooth transition to the limit of homogeneous matter. the effects of pairing properties and finite temperature are considered. a comparison is made to corresponding results employing the phenomenological skyrme hartree-fock approach and the consequences for the thomas-fermi approximation are discussed."
"numerous studies over the past 30 years have suggested there is a causal connection between the motion of the sun through the galaxy and terrestrial mass extinctions or climate change. proposed mechanisms include comet impacts (via perturbation of the oort cloud), cosmic rays and supernovae, the effects of which are modulated by the passage of the sun through the galactic midplane or spiral arms. supposed periodicities in the fossil record, impact cratering dates or climate proxies over the phanerozoic (past 545 myr) are frequently cited as evidence in support of these hypotheses. this remains a controversial subject, with many refutations and replies having been published. here i review both the mechanisms and the evidence for and against the relevance of astronomical phenomena to climate change and evolution. this necessarily includes a critical assessment of time series analysis techniques and hypothesis testing. some of the studies have suffered from flaws in methodology, in particular drawing incorrect conclusions based on ruling out a null hypothesis. i conclude that there is little evidence for intrinsic periodicities in biodiversity, impact cratering or climate on timescales of tens to hundreds of myr. furthermore, galactic midplane and spiral arm crossings seem to have little or no impact on biological or climate variation above background level. (truncated)"
"we reduce the question whether a given quantum mixed state is separable or entangled to the problem of existence of a certain full family of commuting normal matrices whose matrix elements are partially determined by components of the pure states constituting a decomposition of the considered mixture. the method reproduces many known entanglement and/or separability criteria, and provides yet another geometrical characterization of mixed separable states."
"analyzing multi-layered graphical models provides insight into understanding the conditional relationships among nodes within layers after adjusting for and quantifying the effects of nodes from other layers. we obtain the penalized maximum likelihood estimator for gaussian multi-layered graphical models, based on a computational approach involving screening of variables, iterative estimation of the directed edges between layers and undirected edges within layers and a final refitting and stability selection step that provides improved performance in finite sample settings. we establish the consistency of the estimator in a high-dimensional setting. to obtain this result, we develop a strategy that leverages the biconvexity of the likelihood function to ensure convergence of the developed iterative algorithm to a stationary point, as well as careful uniform error control of the estimates over iterations. the performance of the maximum likelihood estimator is illustrated on synthetic data."
"how to define and use the concept of inclusive fitness is a contentious topic in evolutionary theory. inclusive fitness can be used to calculate selection on a focal gene, but it is also applied to whole organisms. individuals are then predicted to appear designed as if to maximise their inclusive fitness, provided that certain conditions are met (formally when interactions between individuals are 'additive'). here we argue that applying the concept of inclusive fitness to organisms is justified under far broader conditions than previously shown, but only if it is appropriately defined. specifically, we propose that organisms should maximise the sum of their offspring (including any accrued due to the behaviour/phenotype of relatives), plus any effects on their relatives' offspring production, weighted by relatedness. in contrast, most theoreticians have argued that a focal individual's inclusive fitness should exclude any offspring accrued due to the behaviour of relatives. our approach is based on the notion that long-term evolution follows the genome's 'majority interest' of building coherent bodies that are efficient 'vehicles' for gene propagation. a gene favoured by selection that reduces the propagation of unlinked genes at other loci (e.g. meiotic segregation distorters that lower sperm production) is eventually neutralised by counter-selection throughout the rest of the genome. most phenotypes will therefore appear as if designed to maximise the propagation of any given gene in a focal individual and its relatives."
"markov state models (msms) and master equation models are popular approaches to approximate molecular kinetics, equilibria, metastable states, and reaction coordinates in terms of a state space discretization usually obtained by clustering. recently, a powerful generalization of msms has been introduced, the variational approach (va) of molecular kinetics and its special case the time-lagged independent component analysis (tica), which allow us to approximate slow collective variables and molecular kinetics by linear combinations of smooth basis functions or order parameters. while it is known how to estimate msms from trajectories whose starting points are not sampled from an equilibrium ensemble, this has not yet been the case for tica and the va. previous estimates from short trajectories, have been strongly biased and thus not variationally optimal. here, we employ koopman operator theory and ideas from dynamic mode decomposition (dmd) to extend the va and tica to non-equilibrium data. the main insight is that the va and tica provide a coefficient matrix that we call koopman model, as it approximates the underlying dynamical (koopman) operator in conjunction with the basis set used. this koopman model can be used to compute a stationary vector to reweight the data to equilibrium. from such a koopman-reweighted sample, equilibrium expectation values and variationally optimal reversible koopman models can be constructed even with short simulations. the koopman model can be used to propagate densities, and its eigenvalue decomposition provide estimates of relaxation timescales and slow collective variables for dimension reduction. koopman models are generalizations of markov state models, tica and the linear va and allow molecular kinetics to be described without a cluster discretization."
"we consider the problem of solving mixed random linear equations with $k$ components. this is the noiseless setting of mixed linear regression. the goal is to estimate multiple linear models from mixed samples in the case where the labels (which sample corresponds to which model) are not observed. we give a tractable algorithm for the mixed linear equation problem, and show that under some technical conditions, our algorithm is guaranteed to solve the problem exactly with sample complexity linear in the dimension, and polynomial in $k$, the number of components. previous approaches have required either exponential dependence on $k$, or super-linear dependence on the dimension. the proposed algorithm is a combination of tensor decomposition and alternating minimization. our analysis involves proving that the initialization provided by the tensor method allows alternating minimization, which is equivalent to em in our setting, to converge to the global optimum at a linear rate."
"we establish a spectral duality for certain unbounded operators in hilbert space. the class of operators includes discrete graph laplacians arising from infinite weighted graphs. the problem in this context is to establish a practical approximation of infinite models with suitable sequences of finite models which in turn allow (relatively) easy computations.   let $x$ be an infinite set and let $\h$ be a hilbert space of functions on $x$ with inner product $\ip{\cdot}{\cdot}=\ip{\cdot}{\cdot}_{\h}$. we will be assuming that the dirac masses $\delta_x$, for $x\in x$, are contained in $\h$. and we then define an associated operator $\delta$ in $\h$ given by $$(\delta v)(x):=\ip{\delta_x}{v}_{\h}.$$ similarly, for every finite subset $f\subset x$, we get an operator $\delta_f$.   if $f_1\subset f_2\subset...$ is an ascending sequence of finite subsets such that $\cup_{k\in\bn}f_k=x$, we are interested in the following two problems:   (a) obtaining an approximation formula $$\lim_{k\to\infty}\delta_{f_k}=\delta;$$ and   (b) establish a computational spectral analysis for the truncated operators $\delta_f$ in (a)."
"balance of excitation and inhibition is a fundamental feature of in vivo network activity and is important for its computations. however, its presence in the neocortex of higher mammals is not well established. we investigated the dynamics of excitation and inhibition using dense multielectrode recordings in humans and monkeys. we found that in all states of the wake-sleep cycle, excitatory and inhibitory ensembles are well balanced, and co-fluctuate with slight instantaneous deviations from perfect balance, mostly in slow-wave sleep. remarkably, these correlated fluctuations are seen for many different temporal scales. the similarity of these computational features with a network model of self-generated balanced states suggests that such balanced activity is essentially generated by recurrent activity in the local network and is not due to external inputs. finally, we find that this balance breaks down during seizures, where the temporal correlation of excitatory and inhibitory populations is disrupted. these results show that balanced activity is a feature of normal brain activity, and break down of the balance could be an important factor to define pathological states."
"in a mouse intercross with more than 500 animals and genome-wide gene expression data on six tissues, we identified a high proportion (18%) of sample mix-ups in the genotype data. local expression quantitative trait loci (eqtl; genetic loci influencing gene expression) with extremely large effect were used to form a classifier to predict an individual's eqtl genotype based on expression data alone. by considering multiple eqtl and their related transcripts, we identified numerous individuals whose predicted eqtl genotypes (based on their expression data) did not match their observed genotypes, and then went on to identify other individuals whose genotypes did match the predicted eqtl genotypes. the concordance of predictions across six tissues indicated that the problem was due to mix-ups in the genotypes (though we further identified a small number of sample mix-ups in each of the six panels of gene expression microarrays). consideration of the plate positions of the dna samples indicated a number of off-by-one and off-by-two errors, likely the result of pipetting errors. such sample mix-ups can be a problem in any genetic study, but eqtl data allow us to identify, and even correct, such problems. our methods have been implemented in an r package, r/lineup."
"we report the observation of shubnikov-de haas oscillations in the underdoped cuprate superconductor yba$_2$cu$_4$o$_8$ (y124). for field aligned along the c-axis, the frequency of the oscillations is $660\pm 30$ t, which corresponds to $\sim 2.4$ % of the total area of the first brillouin zone. the effective mass of the quasiparticles on this orbit is measured to be $2.7\pm0.3$ times the free electron mass. both the frequency and mass are comparable to those recently observed for ortho-ii yba$_2$cu$_3$o$_{6.5}$ (y123-ii). we show that although small fermi surface pockets may be expected from band structure calculations in y123-ii, no such pockets are predicted for y124. our results therefore imply that these small pockets are a generic feature of the copper oxide plane in underdoped cuprates."
"we propose a latent self-exciting point process model that describes geographically distributed interactions between pairs of entities. in contrast to most existing approaches that assume fully observable interactions, here we consider a scenario where certain interaction events lack information about participants. instead, this information needs to be inferred from the available observations. we develop an efficient approximate algorithm based on variational expectation-maximization to infer unknown participants in an event given the location and the time of the event. we validate the model on synthetic as well as real-world data, and obtain very promising results on the identity-inference task. we also use our model to predict the timing and participants of future events, and demonstrate that it compares favorably with baseline approaches."
"we investigate the possibility of discriminating between modified newtonian dynamics (mond) and newtonian gravity with dark matter, by studying the vertical dynamics of disk galaxies. we consider models with the same circular velocity in the equatorial plane (purely baryonic disks in mond and the same disks in newtonian gravity embedded in spherical dark matter haloes), and we construct their intrinsic and projected kinematical fields by solving the jeans equations under the assumption of a two-integral distribution function. we found that the vertical velocity dispersion of deep-mond disks can be much larger than in the equivalent spherical newtonian models. however, in the more realistic case of high-surface density disks this effect is significantly reduced, casting doubts on the possibility of discriminating between mond and newtonian gravity with dark matter by using current observations."
"integrated light from distant galaxies is often compared to stellar population models via the equivalent widths of spectral features--spectral indices--whose strengths rely on the abundances of one or more elements. such comparisons hinge not only on the overall metal abundance but also on relative abundances. studies have examined the influence of individual elements on synthetic spectra but little has been done to address similar issues in the stellar evolution models that underlie most stellar population models. stellar evolution models will primarily be influenced by changes in opacities. in order to explore this issue in detail, twelve sets of stellar evolution tracks and isochrones have been created at constant heavy element mass fraction z that self-consistently account for varying heavy element mixtures. these sets include scaled-solar, alpha-enhanced, and individual cases where the elements c, n, o, ne, mg, si, s, ca, ti, and fe have been enhanced above their scaled-solar values. the variations that arise between scaled-solar and the other cases are examined with respect to the h-r diagram and main sequence lifetimes."
"in this paper we develop proximal methods for statistical learning. proximal point algorithms are useful in statistics and machine learning for obtaining optimization solutions for composite functions. our approach exploits closed-form solutions of proximal operators and envelope representations based on the moreau, forward-backward, douglas-rachford and half-quadratic envelopes. envelope representations lead to novel proximal algorithms for statistical optimisation of composite objective functions which include both non-smooth and non-convex objectives. we illustrate our methodology with regularized logistic and poisson regression and non-convex bridge penalties with a fused lasso norm. we provide a discussion of convergence of non-descent algorithms with acceleration and for non-convex functions. finally, we provide directions for future research."
"we focus on the problem of how wealth is distributed among the units of a networked economic system. we first review the empirical results documenting that in many economies the wealth distribution is described by a combination of log--normal and power--law behaviours. we then focus on the bouchaud--m\'ezard model of wealth exchange, describing an economy of interacting agents connected through an exchange network. we report analytical and numerical results showing that the system self--organises towards a stationary state whose associated wealth distribution depends crucially on the underlying interaction network. in particular we show that if the network displays a homogeneous density of links, the wealth distribution displays either the log--normal or the power--law form. this means that the first--order topological properties alone (such as the scale--free property) are not enough to explain the emergence of the empirically observed \emph{mixed} form of the wealth distribution. in order to reproduce this nontrivial pattern, the network has to be heterogeneously divided into regions with variable density of links. we show new results detailing how this effect is related to the higher--order correlation properties of the underlying network. in particular, we analyse assortativity by degree and the pairwise wealth correlations, and discuss the effects that these properties have on each other."
"we have calculated the atomic electric dipole moments (edms) d of ^3he and ^{171}yb induced by their respective nuclear schiff moments s. our results are d(he)= 8.3x10^{-5} and d(yb)= -1.9 in units 10^{-17}s/(e{fm}^3)e cm. by considering the nuclear schiff moments induced by the parity and time-reversal violating nucleon-nucleon interaction we find d(^{171}yb)~0.6d(^{199}hg). for ^3he the nuclear edm coupled with the hyperfine interaction gives a larger atomic edm than the schiff moment. the result for ^3he is required for a neutron edm experiment that is under development, where ^3he is used as a comagnetometer. we find that the edm for he is orders of magnitude smaller than the neutron edm. the result for yb is needed for the planning and interpretation of experiments that have been proposed to measure the edm of this atom."
the convergence of the algorithm for solving convex feasibility problem is studied by the method of sequential averaged and relaxed projections. some results of h. h. bauschke and j. m. borwein are generalized by introducing new methods. examples illustrating these generalizations are given.
"transformation models are a very important tool for applied statisticians and econometricians. in many applications, the dependent variable is transformed so that homogeneity or normal distribution of the error holds. in this paper, we analyze transformation models in a high-dimensional setting, where the set of potential covariates is large. we propose an estimator for the transformation parameter and we show that it is asymptotically normally distributed using an orthogonalized moment condition where the nuisance functions depend on the target parameter. in a simulation study, we show that the proposed estimator works well in small samples. a common practice in labor economics is to transform wage with the log-function. in this study, we test if this transformation holds in cps data from the united states."
"spatial big data have the ""velocity,"" ""volume,"" and ""variety"" of big data sources and additional geographic information about the record. digital data sources, such as medical claims, mobile phone call data records, and geo-tagged tweets, have entered infectious disease epidemiology as novel sources of data to complement traditional infectious disease surveillance. in this work, we provide examples of how spatial big data have been used thus far in epidemiological analyses and describe opportunities for these sources to improve public health coordination and disease mitigation strategies. in addition, we consider the technical, practical, and ethical challenges with the use of spatial big data in infectious disease surveillance and inference. finally, we discuss the implications of the rising use of spatial big data in epidemiology to health risk communications, across-scale public health coordination, and public health policy recommendation."
these are lecture notes that are based on the lectures from a class i taught on the topic of randomized linear algebra (rla) at uc berkeley during the fall 2013 semester.
we study coxeter racks over $\mathbb{z}_n$ and the knot and link invariants they define. we exploit the module structure of these racks to enhance the rack counting invariants and give examples showing that these enhanced invariants are stronger than the unenhanced rack counting invariants.
"the spectrum and scale of fluctuations in protein structures affect the range of cell phenomena, including stability of protein structures or their fragments, allosteric transitions and energy transfer. the study presents a statistical-thermodynamic analysis of relationship between the sequence composition and the distribution of residue fluctuations in protein-protein complexes. a one-node-per residue elastic network model accounting for the nonhomogeneous protein mass distribution and the inter-atomic interactions through the renormalized inter-residue potential is developed. two factors, a protein mass distribution and a residue environment, were found to determine the scale of residue fluctuations. surface residues undergo larger fluctuations than core residues, showing agreement with experimental observations. ranking residues over the normalized scale of fluctuations yields a distinct classification of amino acids into three groups. the structural instability in proteins possibly relates to the high content of the highly fluctuating residues and a deficiency of the weakly fluctuating residues in irregular secondary structure elements (loops), chameleon sequences and disordered proteins. strong correlation between residue fluctuations and the sequence composition of protein loops supports this hypothesis. comparing fluctuations of binding site residues (interface residues) with other surface residues shows that, on average, the interface is more rigid than the rest of the protein surface and gly, ala, ser, cys, leu and trp have a propensity to form more stable docking patches on the interface. the findings have broad implications for understanding mechanisms of protein association and stability of protein structures."
"it has been shown that aic-type criteria are asymptotically efficient selectors of the tuning parameter in non-concave penalized regression methods under the assumption that the population variance is known or that a consistent estimator is available. we relax this assumption to prove that aic itself is asymptotically efficient and we study its performance in finite samples. in classical regression, it is known that aic tends to select overly complex models when the dimension of the maximum candidate model is large relative to the sample size. simulation studies suggest that aic suffers from the same shortcomings when used in penalized regression. we therefore propose the use of the classical corrected aic (aicc) as an alternative and prove that it maintains the desired asymptotic properties. to broaden our results, we further prove the efficiency of aic for penalized likelihood methods in the context of generalized linear models with no dispersion parameter. similar results exist in the literature but only for a restricted set of candidate models. by employing results from the classical literature on maximum-likelihood estimation in misspecified models, we are able to establish this result for a general set of candidate models. we use simulations to assess the performance of aic and aicc, as well as that of other selectors, in finite samples for both scad-penalized and lasso regressions and a real data example is considered."
"fiber graphs of gr\""obner bases from contingency tables are important in statistical hypothesis testing, where one studies random walks on these graphs using the metropolis-hastings algorithm. the connectivity of the graphs has implications on how fast the algorithm converges. in this paper, we study a class of fiber graphs with elementary combinatorial techniques and provide results that support a recent conjecture of engstr\""om: the connectivity is given by the minimum vertex degree."
"this paper proposes a new bayesian multiple change-point model which is based on the hidden markov approach. the dirichlet process hidden markov model does not require the specification of the number of change-points a priori. hence our model is robust to model specification in contrast to the fully parametric bayesian model. we propose a general markov chain monte carlo algorithm which only needs to sample the states around change-points. simulations for a normal mean-shift model with known and unknown variance demonstrate advantages of our approach. two applications, namely the coal-mining disaster data and the real united states gross domestic product growth, are provided. we detect a single change-point for both the disaster data and us gdp growth. all the change-point locations and posterior inferences of the two applications are in line with existing methods."
"we prove that, for any choice of parameters, the kazhdan-lusztig cells of a weyl group of type $b$ are unions of combinatorial cells (defined using the domino insertion algorithm)."
"this work explores the role of the intrinsic fluctuations in finite parameter controller configurations characterizing an ensemble of arbitrary irregular filter circuits. our analysis illustrates that the parametric intrinsic geometric description exhibits a set of exact pair correction functions and global correlation volume with and without the variation of the mismatch factor. the present consideration shows that the canonical fluctuations can precisely be depicted without any approximation. the intrinsic geometric notion offers a clear picture of the fluctuating controllers, which as the limit of the ensemble averaging reduce to the specified controller. for the constant mismatch factor controllers, the gaussian fluctuations over equilibrium basis accomplish a well-defined, non-degenerate, flat regular intrinsic riemannian surface. an explicit computation further demonstrates that the underlying power correlations involve ordinary summations, even if we consider the variable mismatch factor controllers. our intrinsic geometric framework describes a definite character to the canonical power fluctuations of the controllers and constitutes a stable design strategy for the parameters."
"we examine brane induced gravity on codimension-1 branes, a.k.a dgp gravity, as a theory of five-dimensional gravity containing a certain class four-dimensional branes. from this perspective, the model suffers from a number of pathologies which went unnoticed before. by generalizing the 5d geometry from minkowski to schwarzschild, we find that when the bulk mass is large enough, the brane hits a pressure singularity at finite radius. further, on the self-accelerating branch, the five-dimensional energy is unbounded from below, implying that the self-accelerating backgrounds are unstable. even in an empty minkowski bulk, standard euclidean techniques suggest that the spontaneous nucleation of self-accelerating branes is unsuppressed. if so, quantum effects will strongly modify any classical intuition about the theory. we also note that unless considered as z_2-orbifold boundaries, self-accelerating branes correspond to `wormhole' configurations, which introduces the usual problematic issues associated with wormholes. altogether these pathologies present a serious challenge that any proposed uv completion of the dgp model must overcome."
"the sequence of nucleotide bases occurring in an organism's dna is often regarded as a codescript for its construction. however, information in a dna sequence can only be regarded as a codescript relative to an operational biochemical machine, which the information constrains in such a way as to direct the process of construction. in reality, any biochemical machine for which a dna codescript is efficacious is itself produced through the mechanical interpretation of an identical or very similar codescript. in these terms the origin of life can be described as a bootstrap process involving the simultaneous accumulation of genetic information and the generation of a machine that interprets it as instructions for its own construction. this problem is discussed within the theoretical frameworks of thermodynamics, informatics and self-reproducing automata, paying special attention to the physico-chemical origin of genetic coding and the conditions, both thermodynamic and informatic, which a system must fulfil in order for it to sustain semiosis. the origin of life is equated with biosemiosis"
"in recent years, the availability of highly pure stable isotopes has made possible the investigation of the dependence of the physical properties of crystals, in particular semiconductors, on their isotopic composition. following the investigation of the specific heat ($c_p$, $c_v$) of monatomic crystals such as diamond, silicon, and germanium, similar investigations have been undertaken for the tetrahedral diatomic systems zno and gan (wurtzite structure), for which the effect of the mass of the cation differs from that of the anion. in this article we present measurements for a semiconductor with rock salt structure, namely lead sulfide. because of the large difference in the atomic mass of both constituents ($m_{\rm pb}$= 207.21 and ($m_{\rm s}$=32.06 a.m.u., for the natural isotopic abundance) the effects of varying the cation and that of the anion mass are very different for this canonical semiconductor. we compare the measured temperature dependence of $c_p \approx c_v$, and the corresponding derivatives with respect to ($m_{\rm pb}$ and $m_{\rm s}$), with \textit{\textit{ab initio}} calculations based on the lattice dynamics obtained from the local density approximation (lda) electronic band structure. quantitative deviations between theory and experiment are attributed to the absence of spin-orbit interaction in the abinit program used for the electronic band structure calculations."
"infectious diseases spread through human networks. susceptible-infected-removed (sir) model is one of the epidemic models to describe infection dynamics on a complex network connecting individuals. in the metapopulation sir model, each node represents a population (group) which has many individuals. in this paper, we propose a modified metapopulation sir model in which a latent period is taken into account. we call it siir model. we divide the infection period into two stages: an infected stage, which is the same as the previous model, and a seriously ill stage, in which individuals are infected and cannot move to the other populations. the two infectious stages in our modified metapopulation sir model produce a discontinuous final size distribution. individuals in the infected stage spread the disease like individuals in the seriously ill stage and never recover directly, which makes an effective recovery rate smaller than the given recovery rate."
"the data of f1000 provide us with the unique opportunity to investigate the relationship between peers' ratings and bibliometric metrics on a broad and comprehensive data set with high-quality ratings. f1000 is a post-publication peer review system of the biomedical literature. the comparison of metrics with peer evaluation has been widely acknowledged as a way of validating metrics. based on the seven indicators offered by incites, we analyzed the validity of raw citation counts (times cited, 2nd generation citations, and 2nd generation citations per citing document), normalized indicators (journal actual/expected citations, category actual/expected citations, and percentile in subject area), and a journal based indicator (journal impact factor). the data set consists of 125 papers published in 2008 and belonging to the subject category cell biology or immunology. as the results show, percentile in subject area achieves the highest correlation with f1000 ratings; we can assert that for further three other indicators (times cited, 2nd generation citations, and category actual/expected citations) the 'true' correlation with the ratings reaches at least a medium effect size."
"the problem of voodoo correlations is recognized in neuroimaging as the problem of estimating quantities of interest from the same data that was used to select them as interesting. in statistical terminology, the problem of inference following selection from the same data is that of selective inference. motivated by the unwelcome side-effects of the recommended remedy- splitting the data. a method for constructing confidence intervals based on the correct post-selection distribution of the observations has been suggested recently. we utilize a similar approach in order to provide point estimates that account for a large part of the selection bias. we show via extensive simulations that the proposed estimator has favorable properties, namely, that it is likely to reduce estimation bias and the mean squared error compared to the direct estimator without sacrificing power to detect non-zero correlation as in the case of the data splitting approach. we show that both point estimates and confidence intervals are needed in order to get a full assessment of the uncertainty in the point estimates as both are integrated into the confidence calibration plots proposed recently.   the computation of the estimators is implemented in an accompanying software package."
"decoding, ie prediction from brain images or signals, calls for empirical evaluation of its predictive power. such evaluation is achieved via cross-validation, a method also used to tune decoders' hyper-parameters. this paper is a review on cross-validation procedures for decoding in neuroimaging. it includes a didactic overview of the relevant theoretical considerations. practical aspects are highlighted with an extensive empirical study of the common decoders in within-and across-subject predictions, on multiple datasets --anatomical and functional mri and meg-- and simulations. theory and experiments outline that the popular "" leave-one-out "" strategy leads to unstable and biased estimates, and a repeated random splits method should be preferred. experiments outline the large error bars of cross-validation in neuroimaging settings: typical confidence intervals of 10%. nested cross-validation can tune decoders' parameters while avoiding circularity bias. however we find that it can be more favorable to use sane defaults, in particular for non-sparse decoders."
"because of the advance in technologies, modern statistical studies often encounter linear models with the number of explanatory variables much larger than the sample size. estimation and variable selection in these high-dimensional problems with deterministic design points is very different from those in the case of random covariates, due to the identifiability of the high-dimensional regression parameter vector. we show that a reasonable approach is to focus on the projection of the regression parameter vector onto the linear space generated by the design matrix. in this work, we consider the ridge regression estimator of the projection vector and propose to threshold the ridge regression estimator when the projection vector is sparse in the sense that many of its components are small. the proposed estimator has an explicit form and is easy to use in application. asymptotic properties such as the consistency of variable selection and estimation and the convergence rate of the prediction mean squared error are established under some sparsity conditions on the projection vector. a simulation study is also conducted to examine the performance of the proposed estimator."
"this paper introduces a popular dimension reduction method, sliced inverse regression (sir), into multivariate statistical process monitoring. provides an extension of sir for the single-index model by adopting the idea from partial least squares (pls). our partial sliced inverse regression (psir) method has the merit of incorporating information from both predictors (x) and responses (y), and it has capability of handling large, nonlinear, or ""n<p"" dataset. two statistics with their corresponding distributions and control limits are given based on the x-space decomposition of psir for the purpose of fault detection in process monitoring. simulations showed psir outperformed over pls and sir for both linear and nonlinear model."
"this paper addresses the issue of model selection for hidden markov models (hmms). we generalize factorized asymptotic bayesian inference (fab), which has been recently developed for model selection on independent hidden variables (i.e., mixture models), for time-dependent hidden variables. as with fab in mixture models, fab for hmms is derived as an iterative lower bound maximization algorithm of a factorized information criterion (fic). it inherits, from fab for mixture models, several desirable properties for learning hmms, such as asymptotic consistency of fic with marginal log-likelihood, a shrinkage effect for hidden state selection, monotonic increase of the lower fic bound through the iterative optimization. further, it does not have a tunable hyper-parameter, and thus its model selection process can be fully automated. experimental results shows that fab outperforms states-of-the-art variational bayesian hmm and non-parametric bayesian hmm in terms of model selection accuracy and computational efficiency."
"in this paper we consider a multivariate model-based approach to measure the dynamic evolution of tail risk interdependence among us banks, financial services and insurance sectors. to deeply investigate the risk contribution of insurers we consider separately life and non-life companies. to achieve this goal we apply the multivariate student-t markov switching model and the multiple-covar (coes) risk measures introduced in bernardi et. al. (2013b) to account for both the known stylised characteristics of the data and the contemporaneous joint distress events affecting financial sectors. our empirical investigation finds that banks appear to be the major source of risk for all the remaining sectors, followed by the financial services and the insurance sectors, showing that insurance sector significantly contributes as well to the overall risk. moreover, we find that the role of each sector in contributing to other sectors distress evolves over time accordingly to the current predominant financial condition, implying different interconnection strength."
"we report the first ex post study of the economic impact of sea level rise. we apply two econometric approaches to estimate the past effects of sea level rise on the economy of the usa, viz. barro type growth regressions adjusted for spatial patterns and a matching estimator. unit of analysis is 3063 counties of the usa. we fit growth regressions for 13 time periods and we estimated numerous varieties and robustness tests for both growth regressions and matching estimator. although there is some evidence that sea level rise has a positive effect on economic growth, in most specifications the estimated effects are insignificant. we therefore conclude that there is no stable, significant effect of sea level rise on economic growth. this finding contradicts previous ex ante studies."
"life depends as much on the flow of information as on the flow of energy. here we review the many efforts to make this intuition precise. starting with the building blocks of information theory, we explore examples where it has been possible to measure, directly, the flow of information in biological networks, or more generally where information theoretic ideas have been used to guide the analysis of experiments. systems of interest range from single molecules (the sequence diversity in families of proteins) to groups of organisms (the distribution of velocities in flocks of birds), and all scales in between. many of these analyses are motivated by the idea that biological systems may have evolved to optimize the gathering and representation of information, and we review the experimental evidence for this optimization, again across a wide range of scales."
"we study the correlations between the maxima $m$ and $m$ of a brownian motion (bm) on the time intervals $[0,t_1]$ and $[0,t_2]$, with $t_2>t_1$. we determine exact forms of the distribution functions $p(m,m)$ and $p(g = m - m)$, and calculate the moments $\mathbb{e}\{\left(m - m\right)^k\}$ and the cross-moments $\mathbb{e}\{m^l m^k\}$ with arbitrary integers $l$ and $k$. we show that correlations between $m$ and $m$ decay as $\sqrt{t_1/t_2}$ when $t_2/t_1 \to \infty$, revealing strong memory effects in the statistics of the bm maxima. we also compute the pearson correlation coefficient $\rho(m,m)$, the power spectrum of $m_t$, and we discuss a possibility of extracting the ensemble-averaged diffusion coefficient in single-trajectory experiments using a single realization of the maximum process."
"optimum lifetime of a series (resp. parallel) system with general standby component(s) always depends on allocation strategy of standby component(s) into the system. here, we discuss three different models of one or more standby compo- nents. in each model, we compare different series (resp. parallel) systems (which are formed through different allocation strategies of standby component(s)) with respect to the usual stochastic and the stochastic precedence orders."
"the aim of this work is to develop a systematic manner to close overdetermined systems arising from conformal killing tensors (ckt). the research performs this action for 1-tensor and 2-tensors. this research makes it possible to develop a new general method for any rank of ckt. this method can also be applied to other types of killing equations, as well as to overdetermined systems constrained by some other conditions.   the major methodological apparatus of the research is a decomposition of the section bundles where the covariant derivatives of the ckt land via generalized gradients. this decomposition generates a tree in which each row represents a higher derivative. after using the conformal killing equation, just a few components (branches) survive, which means that most of them can be expressed in terms of lower order terms. this results in a finite number of independent jets. thus, any higher covariant derivative can be written in terms of these jets.   the findings of this work are significant methodologically and, more specifically, in the potential for the discovery of symmetries. first, this work has uncovered a new method that could be used to close overdetermined systems arising from conformal killing tensors (ckt). second, through an application of this method, this research finds higher symmetry operators of first and second degree, which are known by other means, for the laplace operator. the findings also reveal the first order symmetry operators for the yamabe case. moreover, the research leads to conjectures about the second order symmetries of the yamabe operator."
"this paper is concerned with a robust estimator of the intensity of a stationary spatial point process. the estimator corresponds to the median of a jittered sample of the number of points, computed from a tessellation of the observation domain. we show that this median-based estimator satisfies a bahadur representation from which we deduce its consistency and asymptotic normality under mild assumptions on the spatial point process. through a simulation study, we compare the new estimator with the standard one counting the mean number of points per unit volume. the empirical study verifies the asymptotic properties established and shows that the median-based estimator is more robust to outliers than the standard estimator."
"this study presents an efficient incremental/decremental approach for big streams based on kernel ridge regression (krr), a frequently used data analysis in cloud centers. to avoid reanalyzing the whole dataset whenever sensors receive new training data, typical incremental krr used a single-instance mechanism for updating an existing system. however, this inevitably increased redundant computational time, not to mention applicability to big streams. to this end, the proposed mechanism supports incremental/decremental processing for both single and multiple samples (i.e., batch processing). a large scale of data can be divided into batches, processed by a machine, without sacrificing the accuracy. moreover, incremental/decremental analyses in empirical and intrinsic space are also proposed in this study to handle different types of data either with a large number of samples or high feature dimensions, whereas typical methods focused only on one type. at the end of this study, we further the proposed mechanism to statistical kernelized bayesian regression, so that uncertainty modeling with incremental/decremental computation becomes applicable. experimental results showed that computational time was significantly reduced, better than the original nonincremental design and the typical single incremental method. furthermore, the accuracy of the proposed method remained the same as the baselines. this implied that the system enhanced efficiency without sacrificing the accuracy. these findings proved that the proposed method was appropriate for variable streaming data analysis, thereby demonstrating the effectiveness of the proposed method."
"we have obtained a ""hierarchical regionalization"" of 3,107 county-level units of the united states based upon census-recorded 1995-2000 intercounty migration flows. the methodology employed was the two-stage (double-standardization and strong component [directed graph] hierarchical clustering) algorithm described in the 2009 pnas (106 [26], e66) letter (arxiv:0904.4863). various features (e. g., cosmopolitan vs. provincial aspects, and indices of isolation) of the regionalization have been previously discussed in arxiv:0907.2393, arxiv:0903.3623 and arxiv:0809.2768. however, due to the lengthy (38-page) nature of the associated dendrogram, the detailed tree structure itself was not readily available for inspection. here, we do present this (county-searchable) dendrogram--and invite readers to explore it, based on their particular interests/locations. an ordinal scale--rather than the originally-derived cardinal scale of the doubly-standardized values--in which groupings/features were more immediately apparent, was originally presented. now, we append the cardinal-scale dendrogram."
we show both theoretically and experimentally that the magnetization density accompanying ultrafast excitation of a semiconductor with circular polarized light varies rapidly enough to produce a detectable thz field.
"decentralized consensus-based optimization is a general computational framework where a network of nodes cooperatively minimizes a sum of locally available cost functions via only local computation and communication. in this article, we survey recent advances on this topic, particularly focusing on decentralized, consensus-based, first-order gradient methods for large-scale stochastic optimization. the class of consensus-based stochastic optimization algorithms is communication-efficient, able to exploit data parallelism, robust in random and adversarial environments, and simple to implement, thus providing scalable solutions to a wide range of large-scale machine learning problems. we review different state-of-the-art decentralized stochastic optimization formulations, different variants of consensus-based procedures, and demonstrate how to obtain decentralized counterparts of centralized stochastic first-order methods. we provide several intuitive illustrations of the main technical ideas as well as applications of the algorithms in the context of decentralized training of machine learning models."
"the twistor space of representations on an open variety maps to a weight two space of local monodromy transformations around a divisor component at infinty. the space of $\sigma$-invariant sections of this slope-two bundle over the twistor line is a real 3 dimensional space whose parameters correspond to the complex residue of the higgs field, and the real parabolic weight of a harmonic bundle."
"in this paper, we consider the problem of sparse signal detection based on partial support set estimation with compressive measurements in a distributed network. multiple nodes in the network are assumed to observe sparse signals which share a common but unknown support. while in the traditional compressive sensing (cs) framework, the goal is to recover the complete sparse signal, in sparse signal detection, complete signal recovery may not be necessary to make a reliable detection decision. in particular, detection can be performed based on partially or inaccurately estimated signals which requires less computational burden than that is required for complete signal recovery. to that end, we investigate the problem of sparse signal detection based on partially estimated support set. first, we discuss how to determine the minimum fraction of the support set to be known so that a desired detection performance is achieved in a centralized setting. second, we develop two distributed algorithms for sparse signal detection when the raw compressed observations are not available at the central fusion center. in these algorithms, the final decision statistic is computed based on locally estimated partial support sets via orthogonal matching pursuit (omp) at individual nodes. the proposed distributed algorithms with less communication overhead are shown to provide comparable performance (sometimes better) to the centralized approach when the size of the estimated partial support set is very small."
"we derive sharp bounds for the prices of vix futures using the full information of s&p 500 smiles. to that end, we formulate the model-free sub/superreplication of the vix by trading in the s&p 500 and its vanilla options as well as the forward-starting log-contracts. a dual problem of minimizing/maximizing certain risk-neutral expectations is introduced and shown to yield the same value.   the classical bounds for vix futures given the smiles only use a calendar spread of log-contracts on the s&p 500. we analyze for which smiles the classical bounds are sharp and how they can be improved when they are not. in particular, we introduce a family of functionally generated portfolios which often improves the classical bounds while still being tractable; more precisely, determined by a single concave/convex function on the line. numerical experiments on market data and sabr smiles show that the classical lower bound can be improved dramatically, whereas the upper bound is often close to optimal."
"recently, several experiments were reported using ambient vibration surveys in buildings to estimate the modal parameters of buildings. their modal properties are full of relevant information concerning its dynamic behaviour in its elastic domain. the main scope of this paper is to determine relevant, though simple, beam modelling whose validity could be easily checked with experimental data. in this study, we recorded ambient vibrations in 3 buildings in grenoble selected because of their vertical structural homogeneity. first, a set of recordings was done using a 18 channels digital acquisition system (cityshark) connected to six 3c lennartz 5s sensors. we used the frequency domain decomposition (fdd) technique to extract the modal parameters of these buildings. second, it is shown in the following that the experimental quasi-elastic behaviour of such structure can be reduced to the behaviour of a vertical continuous timoshenko beam. a parametric study of this beam shows that a bijective relation exists between the beam parameters and its eigenfrequencies distribution. consequently, the timoshenko beam parameters can be estimated from the experimental sequence of eigenfrequencies. having the beam parameters calibrated by the in situ data, the reliability of the modelling is checked by complementary comparisons. for this purpose, the mode shapes and eigenfrequencies of higher modes are calculated and compared to the experimental data. a good agreement is also obtained. in addition, the beam model integrates in a very synthetic way the essential parameters of the dynamic behaviour."
"computing moments of various parameter estimators related to an autoregressive model of statistics, one needs to evaluate several expressions of the type mentioned in the title of this article. we proceed to derive the corresponding formulas."
"here we present a new method of estimating global variations in outdoor pm$_{2.5}$ concentrations using satellite images combined with ground-level measurements and deep convolutional neural networks. specifically, new deep learning models were trained over the global pm$_{2.5}$ concentration range ($<$1-436 $\mu$g/m$^3$) using a large database of satellite images paired with ground level pm$_{2.5}$ measurements available from the world health organization. final model selection was based on a systematic evaluation of well-known architectures for the convolutional base including inceptionv3, xception, and vgg16. the xception architecture performed best and the final global model had a root mean square error (rmse) value of 13.01 $\mu$g/m$^3$ (r$^2$=0.75) in the disjoint test set. the predictive performance of our new global model (called image-pm$_{2.5}$) is similar to the current state-of-the-art model used in the global burden of disease study but relies only on satellite images as input. as a result, the image-pm$_{2.5}$ model offers a fast, cost-effective means of estimating global variations in long-term average pm$_{2.5}$ concentrations and may be particularly useful for regions without ground monitoring data or detailed emissions inventories. the image-pm$_{2.5}$ model can be used as a stand-alone method of global exposure estimation or incorporated into more complex hierarchical model structures."
"we consider discrete time heath-jarrow-morton type interest rate models, where the interest rate curves are driven by a geometric spatial autoregression field. strong consistency and asymptotic normality of the maximum likelihood estimators of the parameters are proved for stable no-arbitrage models containing a general stochastic discounting factor, where explicit form of the ml estimators is not available given a non-i.i.d. sample. the results form the basis of further statistical problems in such models."
we explain how to find the kk-theoretic counterpart of extremal k-set defined by larry brown and gert pedersen.
"when targeting a distribution that is artificially invariant under some permutations, markov chain monte carlo (mcmc) algorithms face the label-switching problem, rendering marginal inference particularly cumbersome. such a situation arises, for example, in the bayesian analysis of finite mixture models. adaptive mcmc algorithms such as adaptive metropolis (am), which self-calibrates its proposal distribution using an online estimate of the covariance matrix of the target, are no exception. to address the label-switching issue, relabeling algorithms associate a permutation to each mcmc sample, trying to obtain reasonable marginals. in the case of adaptive metropolis (bernoulli 7 (2001) 223-242), an online relabeling strategy is required. this paper is devoted to the amor algorithm, a provably consistent variant of am that can cope with the label-switching problem. the idea is to nest relabeling steps within the mcmc algorithm based on the estimation of a single covariance matrix that is used both for adapting the covariance of the proposal distribution in the metropolis algorithm step and for online relabeling. we compare the behavior of amor to similar relabeling methods. in the case of compactly supported target distributions, we prove a strong law of large numbers for amor and its ergodicity. these are the first results on the consistency of an online relabeling algorithm to our knowledge. the proof underlines latent relations between relabeling and vector quantization."
"in the present paper, we derive lower bounds for the risk of the nonparametric empirical bayes estimators. in order to attain the optimal convergence rate, we propose generalization of the linear empirical bayes estimation method which takes advantage of the flexibility of the wavelet techniques. we present an empirical bayes estimator as a wavelet series expansion and estimate coefficients by minimizing the prior risk of the estimator. as a result, estimation of wavelet coefficients requires solution of a well-posed low-dimensional sparse system of linear equations. the dimension of the system depends on the size of wavelet support and smoothness of the bayes estimator. an adaptive choice of the resolution level is carried out using lepski (1997) method. the method is computationally efficient and provides asymptotically optimal adaptive eb estimators. the theory is supplemented by numerous examples."
we present general analytical expressions of stokes and anti-stokes spectral photon-flux densities that are spontaneously generated by a single monochromatic pump wave propagating in a single-mode optical fiber. we validate our results by comparing them with experimental data. limiting cases of the general expressions corresponding to interesting physical situations are discussed.
"the magnetic resonance (mr) analysis of brain tumors is widely used for diagnosis and examination of tumor subregions. the overlapping area among the intensity distribution of healthy, enhancing, non-enhancing, and edema region makes the automatic segmentation a challenging task. here, we show that a convolutional neural network trained on high-contrast images can transform intensity distribution of brain lesion in its internal subregions. specifically, generative adversarial network (gan) is extended to synthesize high-contrast images. a comparison of these synthetic images and real images of brain tumor tissue in mr scans showed significant segmentation improvement and decreased the number of real channels for segmentation. the synthetic images are used as a substitute for real channels and can bypass real modalities in the multimodal brain tumor segmentation framework. segmentation results on brats 2019 dataset demonstrate that our proposed approach can efficiently segment the tumor areas."
"we calculate potentials of the mean force for twenty amino acids in the vicinity of the (111) surface of gold, for several dipeptides, and for some analogs of the side chains, using molecular dynamics simulations and the umbrella sampling method. we compare results obtained within three different force fields: one hydrophobic (for a contaminated surface) and two hydrophilic. all of these fields lead to good binding with very different specificities and different patterns in the density and polarization of water. the covalent bond with the sulfur atom on cysteine is modeled by the morse potential. we demonstrate that binding energies of dipeptides are different than the combined binding energies of their amino-acidic components. for the hydrophobic gold, adsorption events of a small protein are driven by attraction to the strongest binding amino acids. this is not so in the hydrophilic cases - a result of smaller specificities combined with the difficulty for proteins, but not for single amino acids, to penetrate the first layer of water. the properties of water near the surface sensitively depend on the force field."
"the calculation of multivariate normal probabilities is of great importance in many statistical and economic applications. this paper proposes a spherical monte carlo method with both theoretical analysis and numerical simulation. first, the multivariate normal probability is rewritten via an inner radial integral and an outer spherical integral by the spherical transformation. for the outer spherical integral, we apply an integration rule by randomly rotating a predetermined set of well-located points. to find the desired set, we derive an upper bound for the variance of the monte carlo estimator and propose a set which is related to the kissing number problem in sphere packings. for the inner radial integral, we employ the idea of antithetic variates and identify certain conditions so that variance reduction is guaranteed. extensive monte carlo experiments on some probabilities calculation confirm these claims."
"we give a simple axiomatic definition of a rational-valued invariant s(w,v,e) of triples (w,v,e), where w is a (smooth, oriented, closed) 6-manifold and v is a 3-submanifold of w, and where e is a second rational cohomology class of the complement of v satisfying a certain condition. the definition is stated in terms of cobordisms of such triples and the signature of 4-manifolds. when w = s^6 and v is a smoothly embedded 3-sphere, and when e/2 is the poincare dual of a seifert surface of v, the invariant coincides with -8 times haefliger's embedding invariant of (s^6,v). our definition recovers a more general invariant due to takase, and contains a new definition for milnor's triple linking number of algebraically split 3-component links in r^3 that is close to the one given by the perturbative series expansion of the chern-simons theory of links in r^3."
"in a conformational nonequilibrium steady state (cness), enzyme turnover is modulated by the underlying conformational dynamics. based on a discrete kinetic network model, we use the integrated probability flux balance method to derive the cness turnover rate for a conformation-modulated enzymatic reaction. the traditional michaelis-menten (mm) rate equation is extended to a generalized form, which includes non-mm corrections induced by conformational population currents within combined cyclic kinetic loops. when conformational detailed balance is satisfied, the turnover rate reduces to the mm functional form, explaining its validity for many enzymatic systems. for the first time, a one-to-one correspondence is established between non-mm terms and combined cyclic loops with unbalanced conformational currents. cooperativity resulting from nonequilibrium conformational dynamics has been observed in enzymatic reactions, and we provide a novel, rigorous means of predicting and characterizing such behavior. our generalized mm equation affords a systematic approach for exploring cness enzyme kinetics."
"estimating and assessing the risk of a large portfolio is an important topic in financial econometrics and risk management. the risk is often estimated by a substitution of a good estimator of the volatility matrix. however, the accuracy of such a risk estimator for large portfolios is largely unknown, and a simple inequality in the previous literature gives an infeasible upper bound for the estimation error. in addition, numerical studies illustrate that this upper bound is very crude. in this paper, we propose factor-based risk estimators under a large amount of assets, and introduce a high-confidence level upper bound (h-club) to assess the accuracy of the risk estimation. the h-club is constructed based on three different estimates of the volatility matrix: sample covariance, approximate factor model with known factors, and unknown factors (poet, fan, liao and mincheva, 2013). for the first time in the literature, we derive the limiting distribution of the estimated risks in high dimensionality. our numerical results demonstrate that the proposed upper bounds significantly outperform the traditional crude bounds, and provide insightful assessment of the estimation of the portfolio risks. in addition, our simulated results quantify the relative error in the risk estimation, which is usually negligible using 3-month daily data. finally, the proposed methods are applied to an empirical study."
"general relativity does not allow one to specify the topology of space, leaving the possibility that space is multiply rather than simply connected. we review the main mathematical properties of multiply connected spaces, and the different tools to classify them and to analyse their properties. following their mathematical classification, we describe the different possible muticonnected spaces which may be used to construct friedmann-lemaitre universe models. observational tests concern the distribution of images of discrete cosmic objects or more global effects, mainly those concerning the cosmic microwave background. according to the 2003-2006 wmap data releases, various deviations from the flat infinite universe model predictions hint at a possible non-trivial topology for the shape of space. in particular, a finite universe with the topology of the poincar\'e dodecahedral spherical space fits remarkably well the data and is a good candidate for explaining both the local curvature of space and the large angle anomalies in the temperature power spectrum. such a model of a small universe, whose volume would represent only about 80% the volume of the observable universe, offers an observational signature in the form of a predictable topological lens effect on one hand, and rises new issues on the physics of the early universe on the other hand."
"by the nature of their construction, many statistical models for extremes result in likelihood functions that are computationally prohibitive to evaluate. this is consequently problematic for the purposes of likelihood-based inference. with a focus on the bayesian framework, this chapter examines the use of approximate bayesian computation (abc) techniques for the fitting and analysis of statistical models for extremes. after introducing the ideas behind abc algorithms and methods, we demonstrate their application to extremal models in stereology and spatial extremes."
"we generalise dwork's theory of $p$-adic formal congruences from the univariate to a multi-variate setting. we apply our results to prove integrality assertions on the taylor coefficients of (multi-variable) mirror maps. more precisely, with $\mathbf z=(z_1,z_2,...,z_d)$, we show that the taylor coefficients of the multi-variable series $q(\mathbf z)=z_i\exp(g(\mathbf z)/f(\mathbf z))$ are integers, where $f(\mathbf z)$ and $g(\mathbf z)+\log(z_i) f(\mathbf z)$, $i=1,2,...,d$, are specific solutions of certain gkz systems. this result implies the integrality of the taylor coefficients of numerous families of multi-variable mirror maps of calabi-yau complete intersections in weighted projective spaces, as well as of many one-variable mirror maps in the ""tables of calabi-yau equations"" [arxiv:math/0507430] of almkvist, van enckevort, van straten and zudilin. in particular, our results prove a conjecture of batyrev and van straten in [comm. math. phys. 168 (1995), 493-533] on the integrality of the taylor coefficients of canonical coordinates for a large family of such coordinates in several variables."
"an explicit procedure to construct a family of martingales generated by a process with independent increments is presented. the main tools are the polynomials that give the relationship between the moments and cumulants, and a set of martingales related to the jumps of the process called teugels martingales"
"gene products (rnas, proteins) often occur at low molecular counts inside individual cells, and hence are subject to considerable random fluctuations (noise) in copy number over time. not surprisingly, cells encode diverse regulatory mechanisms to buffer noise. one such mechanism is the incoherent feedforward circuit. we analyze a simplistic version of this circuit, where an upstream regulator x affects both the production and degradation of a protein y. thus, any random increase in x's copy numbers would increase both production and degradation, keeping y levels unchanged. to study its stochastic dynamics, we formulate this network into a mathematical model using the chemical master equation formulation. we prove that if the functional dependence of y's production and degradation on x is similar, then the steady-distribution of y's copy numbers is independent of x. to investigate how fluctuations in y propagate downstream, a protein z whose production rate only depend on y is introduced. intriguingly, results show that the extent of noise in z increases with noise in x, in spite of the fact that the magnitude of noise in y is invariant of x. such counter intuitive results arise because x enhances the time-scale of fluctuations in y, which amplifies fluctuations in downstream processes. in summary, while feedforward systems can buffer a protein from noise in its upstream regulators, noise can propagate downstream due to changes in the time-scale of fluctuations."
"we investigate what a snapshot of a quantum evolution - a quantum channel reflecting open system dynamics - reveals about the underlying continuous time evolution. remarkably, from such a snapshot, and without imposing additional assumptions, it can be decided whether or not a channel is consistent with a time (in)dependent markovian evolution, for which we provide computable necessary and sufficient criteria. based on these, a computable measure of `markovianity' is introduced. we discuss how the consistency with markovian dynamics can be checked in quantum process tomography. the results also clarify the geometry of the set of quantum channels with respect to being solutions of time (in)dependent master equations."
"the rate of recombination affects the mode of molecular evolution. in high-recombining sequence, the targets of selection are individual genetic loci; under low recombination, selection collectively acts on large, genetically linked genomic segments. selection under linkage can induce clonal interference, a specific mode of evolution by competition of genetic clades within a population. this mode is well known in asexually evolving microbes, but has not been traced systematically in an obligate sexual organism. here we show that the drosophila genome is partitioned into two modes of evolution: a local interference regime with limited effects of genetic linkage, and an interference condensate with clonal competition. we map these modes by differences in mutation frequency spectra, and we show that the transition between them occurs at a threshold recombination rate that is predictable from genomic summary statistics. we find the interference condensate in segments of low-recombining sequence that are located primarily in chromosomal regions flanking the centromeres and cover about 20% of the drosophila genome. condensate regions have characteristics of asexual evolution that impact gene function: the efficacy of selection and the speed of evolution are lower and the genetic load is higher than in regions of local interference. our results suggest that multicellular eukaryotes can harbor heterogeneous modes and tempi of evolution within one genome. we argue that this variation generates selection on genome architecture."
"we consider continuous-time diffusion models driven by fractional brownian motion. observations are assumed to possess a non-trivial likelihood given the latent path. due to the non-markovianity and high-dimensionality of the latent paths, estimating posterior expectations is a computationally challenging undertaking. we present a reparameterization framework based on the davies and harte method for sampling stationary gaussian processes and use this framework to construct a markov chain monte carlo algorithm that allows computationally efficient bayesian inference. the markov chain monte carlo algorithm is based on a version of hybrid monte carlo that delivers increased efficiency when applied on the high-dimensional latent variables arising in this context. we specify the methodology on a stochastic volatility model allowing for memory in the volatility increments through a fractional specification. the methodology is illustrated on simulated data and on the s&p500/vix time series and is shown to be effective. contrary to a long range dependence attribute of such models often assumed in the literature, with hurst parameter larger than 1/2, the posterior distribution favours values smaller than 1/2, pointing towards medium range dependence."
"many plants, such as mimosa pudica (the sensitive plant), employ electrochemical signals known as action potentials (aps) for rapid intercellular communication. in this paper, we consider a reaction diffusion model of individual ap signals to analyze aps from a communication and information theoretic perspective. we use concepts from molecular communication to explain the underlying process of information transfer in a plant for a single ap pulse that is shared with one or more receiver cells. we also use the chemical langevin equation to accommodate the deterministic as well as stochastic component of the system. finally we present an information theoretic analysis of single action potentials, obtaining achievable information rates for these signals. we show that, in general, the presence of an ap signal can increase the mutual information and information propagation speed among neighboring cells with receivers in different settings."
"for the sum process $x=x^1+x^2$ of a bivariate l\'evy process $(x^1,x^2)$ with possibly dependent components, we derive a quintuple law describing the first upwards passage event of $x$ over a fixed barrier, caused by a jump, by the joint distribution of five quantities: the time relative to the time of the previous maximum, the time of the previous maximum, the overshoot, the undershoot and the undershoot of the previous maximum. the dependence between the jumps of $x^1$ and $x^2$ is modeled by a l\'evy copula. we calculate these quantities for some examples, where we pay particular attention to the influence of the dependence structure. we apply our findings to the ruin event of an insurance risk process."
"the increasing interest in complex networks research has been a consequence of several intrinsic features of this area, such as the generality of the approach to represent and model virtually any discrete system, and the incorporation of concepts and methods deriving from many areas, from statistical physics to sociology, which are often used in an independent way. yet, for this same reason, it would be desirable to integrate these various aspects into a more coherent and organic framework, which would imply in several benefits normally allowed by the systematization in science, including the identification of new types of problems and the cross-fertilization between fields. more specifically, the identification of the main areas to which the concepts frequently used in complex networks can be applied paves the way to adopting and applying a larger set of concepts and methods deriving from those respective areas. among the several areas that have been used in complex networks research, pattern recognition, optimization, linear algebra, and time series analysis seem to play a more basic and recurrent role. in the present manuscript, we propose a systematic way to integrate the concepts from these diverse areas regarding complex networks research. in order to do so, we start by grouping the multidisciplinary concepts into three main groups, namely features, similarity, and network connectivity. then we show that several of the analysis and modeling approaches to complex networks can be thought as a composition of maps between these three groups, with emphasis on nine main types of mappings, which are presented and illustrated. such a systematization of principles and approaches also provides an opportunity to review some of the most closely related works in the literature, which is also developed in this article."
"a mean function in reproducing kernel hilbert space, or a kernel mean, is an important part of many applications ranging from kernel principal component analysis to hilbert-space embedding of distributions. given finite samples, an empirical average is the standard estimate for the true kernel mean. we show that this estimator can be improved via a well-known phenomenon in statistics called stein's phenomenon. after consideration, our theoretical analysis reveals the existence of a wide class of estimators that are better than the standard. focusing on a subset of this class, we propose efficient shrinkage estimators for the kernel mean. empirical evaluations on several benchmark applications clearly demonstrate that the proposed estimators outperform the standard kernel mean estimator."
"we propose in this contribution a method for l one regularization in prototype based relevance learning vector quantization (lvq) for sparse relevance profiles. sparse relevance profiles in hyperspectral data analysis fade down those spectral bands which are not necessary for classification. in particular, we consider the sparsity in the relevance profile enforced by lasso optimization. the latter one is obtained by a gradient learning scheme using a differentiable parametrized approximation of the $l_{1}$-norm, which has an upper error bound. we extend this regularization idea also to the matrix learning variant of lvq as the natural generalization of relevance learning."
"we study the quantum corrections to the moduli space of the quiver gauge theory corresponding to regular and fractional d3-branes at the dp_1 singularity. we find that besides the known runaway behavior at the lowest step of the duality cascade, there is a runaway direction along a mesonic branch at every higher step of the cascade. moreover, the algebra of the chiral operators which obtain the large expectation values is such that we reproduce altmann's first order deformation of the dp_1 cone."
"in this paper, we discuss delayed periodic dynamical systems, compare capability of criteria of global exponential stability in terms of various $l^{p}$ ($1\le p<\infty$) norms. a general approach to investigate global exponential stability in terms of various $l^{p}$ ($1\le p<\infty$) norms is given. sufficient conditions ensuring global exponential stability are given, too. comparisons of various stability criteria are given. more importantly, it is pointed out that sufficient conditions in terms of $l^{1}$ norm are enough and easy to implement in practice."
"we propose an accurate and efficient method to compute vibrational spectra of molecules, based on exact diagonalization of an algebraically calculated matrix based on powers of morse coordinate. the present work focuses on the 1d potential of diatomic molecules: as typical examples, we apply this method to the standard lennard-jones oscillator, and to the ab initio potential of the h2 molecule. global cm-1 accuracy is exhibited through the h2 spectrum, obtained through the diagonalization of a 30 x 30 matrix. this theory is at the root of a new method to obtain globally accurate vibrational spectral data in the context of the multi-dimensional potential of polyatomic molecules, at an affordable computational cost."
"in this paper we reconsider a series expansion for a dark matter distribution function in the spherically symmetric anisotropic limit. we show here that the expansion may be renormalized so that the series does converge in time to an estimate of the steady state distribution function in the central regions. subsequently we use this distribution function to discuss the nature of the central equilibrium and, by invoking stationarity of boltzmann's h function as a measure of (thermodynamic) relaxation, we calculate the adiabatic variation in the local logarithmic slope of the mass density. similarly the pseudo (phase-space) density variation with radius is calculated. these are compared to empirical fitting functions. there is general agreement on the inner part of the logarithmic slope of the density and of the inner profile of the pseudo-density power law, but coincident continuity with the outer power-laws is not yet achieved. finally some suggestions are made regarding the actual microphysics acting during the non-equilibrium approach to relaxation. in particular a cascade regime is identified."
we study a 1d transport equation with nonlocal velocity and show the formation of singularities in finite time for a generic family of initial data. by adding a diffusion term the finite time singularity is prevented and the solutions exist globally in time.
"in most natural sciences there is currently the insight that it is necessary to bridge gaps between different processes which can be observed on different scales. this is especially true in the field of chemical reactions where the abilities to form bonds between different types of atoms and molecules create much of the properties we experience in our everyday life, especially in all biological activity. there are essentially two types of processes related to biochemical reaction networks, the interactions among molecules and interactions involving their conformational changes, so in a sense, their internal state. the first type of processes can be conveniently approximated by the so-called mass-action kinetics, but this is not necessarily so for the second kind where molecular states do not define any kind of density or concentration. in this paper we demonstrate the necessity to study reaction networks in a stochastic formulation for which we can construct a coherent approximation in terms of specific space-time scales and the number of particles. the continuum limit procedure naturally creates equations of fokker-planck type where the evolution of the concentration occurs on a slower time scale when compared to the evolution of the conformational changes, for example triggered by binding or unbinding events with other (typically smaller) molecules. we apply the asymptotic theory to derive the effective, i.e. macroscopic dynamics of the biochemical reaction system. the theory can also be applied to other processes where entities can be described by finitely many internal states, with changes of states occuring by arrival of other entities described by a birth-death process."
"in a recent work by charpin, helleseth, and zinoviev kloosterman sums $k(a)$ over a finite field $\f_{2^m}$ were evaluated modulo 24 in the case $m$ odd, and the number of those $a$ giving the same value for $k(a)$ modulo 24 was given. in this paper the same is done in the case $m$ even. the key techniques used in this paper are different from those used in the aforementioned work. in particular, we exploit recent results on the number of irreducible polynomials with prescribed coefficients."
we successfully demonstrate coexistence of record-high 11.2 tb/s (56x200gb/s) classical channels with a discrete-variable-qkd channel over a multicore fibre. continuous secret key generation is confirmed together with classical channel performance below the sdfec limit and a minimum quantum channel spacing of 17nm in the c-band.
"the effect of the pauli exclusion principle on double ionization of he atoms by strong, linearly polarized laser pulses is analyzed. we show that correlated electron escape, with electron momenta symmetric with respect to the field polarization axis, is suppressed if atoms are initially prepared in the metastable state 3s. the effect is a consequence of selection rules for the transition to the appropriate outgoing two-electron states. we illustrate the suppression in numerical calculations of electron and ion momentum distributions within a reduced dimensionality model."
"the regression discontinuity (rd) design is a popular approach to causal inference in non-randomized studies. this is because it can be used to identify and estimate causal effects under mild conditions. specifically, for each subject, the rd design assigns a treatment or non-treatment, depending on whether or not an observed value of an assignment variable exceeds a fixed and known cutoff value.   in this paper, we propose a bayesian nonparametric regression modeling approach to rd designs, which exploits a local randomization feature. in this approach, the assignment variable is treated as a covariate, and a scalar-valued confounding variable is treated as a dependent variable (which may be a multivariate confounder score). then, over the model's posterior distribution of locally-randomized subjects that cluster around the cutoff of the assignment variable, inference for causal effects are made within this random cluster, via two-group statistical comparisons of treatment outcomes and non-treatment outcomes.   we illustrate the bayesian nonparametric approach through the analysis of a real educational data set, to investigate the causal link between basic skills and teaching ability."
distance covariance and distance correlation have been widely adopted in measuring dependence of a pair of random variables or random vectors. if the computation of distance covariance and distance correlation is implemented directly accordingly to its definition then its computational complexity is o($n^2$) which is a disadvantage compared to other faster methods. in this paper we show that the computation of distance covariance and distance correlation of real valued random variables can be implemented by an o(n log n) algorithm and this is comparable to other computationally efficient algorithms. the new formula we derive for an unbiased estimator for squared distance covariance turns out to be a u-statistic. this fact implies some nice asymptotic properties that were derived before via more complex methods. we apply the fast computing algorithm to some synthetic data. our work will make distance correlation applicable to a much wider class of applications.
"optical/near-infrared (optical/nir; oir) light from low-mass neutron star x-ray binaries (nsxbs) in outburst is traditionally thought to be thermal emission from the accretion disc. here we present a comprehensive collection of quasi-simultaneous oir and x-ray data from 19 low-magnetic field nsxbs, including new observations of three sources: 4u 0614+09, lmc x-2 and gx 349+2. the average radio-oir spectrum for nsxbs is alpha ~ +0.2 (where l_nu propto nu^alpha) at least at high luminosities when the radio jet is detected. this is comparable to, but slightly more inverted than the alpha ~ 0.0 found for black hole x-ray binaries. the oir spectra and relations between oir and x-ray fluxes are compared to those expected if the oir emission is dominated by thermal emission from an x-ray or viscously heated disc, or synchrotron emission from the inner regions of the jets. we find that thermal emission due to x-ray reprocessing can explain all the data except at high luminosities for some nsxbs, namely the atolls and millisecond x-ray pulsars (msxps). optically thin synchrotron emission from the jets (with an observed oir spectral index of alpha_thin < 0) dominate the nir light above l_x ~ 10^36 erg/s and the optical above l_x ~ 10^37 erg/s in these systems. for nsxb z-sources, the oir observations can be explained by x-ray reprocessing alone, although synchrotron emission may make a low level contribution to the nir, and could dominate the oir in one or two cases."
"automated label-free quantitative imaging of biological samples can greatly benefit high throughput diseases diagnosis. digital holographic microscopy (dhm) is a powerful quantitative label-free imaging tool that retrieves structural details of cellular samples non-invasively. in off-axis dhm, a proper spatial filtering window in fourier space is crucial to the quality of reconstructed phase image. here we describe a region-recognition approach that combines shape recognition with an iterative thresholding to extracts the optimal shape of frequency components. the region recognition technique offers fully automated adaptive filtering that can operate with a variety of samples and imaging conditions. when imaging through optically scattering biological hydrogel matrix, the technique surpasses previous histogram thresholding techniques without requiring any manual intervention. finally, we automate the extraction of the statistical difference of optical height between malaria parasite infected and uninfected red blood cells. the method described here pave way to greater autonomy in automated dhm imaging for imaging live cell in thick cell cultures."
"this paper examines the role and efficiency of the non-convex loss functions for binary classification problems. in particular, we investigate how to design a simple and effective boosting algorithm that is robust to the outliers in the data. the analysis of the role of a particular non-convex loss for prediction accuracy varies depending on the diminishing tail properties of the gradient of the loss -- the ability of the loss to efficiently adapt to the outlying data, the local convex properties of the loss and the proportion of the contaminated data. in order to use these properties efficiently, we propose a new family of non-convex losses named $\gamma$-robust losses. moreover, we present a new boosting framework, {\it arch boost}, designed for augmenting the existing work such that its corresponding classification algorithm is significantly more adaptable to the unknown data contamination. along with the arch boosting framework, the non-convex losses lead to the new class of boosting algorithms, named adaptive, robust, boosting (arb). furthermore, we present theoretical examples that demonstrate the robustness properties of the proposed algorithms. in particular, we develop a new breakdown point analysis and a new influence function analysis that demonstrate gains in robustness. moreover, we present new theoretical results, based only on local curvatures, which may be used to establish statistical and optimization properties of the proposed arch boosting algorithms with highly non-convex loss functions. extensive numerical calculations are used to illustrate these theoretical properties and reveal advantages over the existing boosting methods when data exhibits a number of outliers."
"the overwhelming amount of available scholarly literature in the life sciences poses significant challenges to scientists wishing to keep up with important developments related to their research, but also provides a useful resource for the discovery of recent information concerning genes, diseases, compounds and the interactions between them. in this paper, we describe an algorithm called bio-lda that uses extracted biological terminology to automatically identify latent topics, and provides a variety of measures to uncover putative relations among topics and bio-terms. relationships identified using those approaches are combined with existing data in life science datasets to provide additional insight. three case studies demonstrate the utility of the bio-lda model, including association predication, association search and connectivity map generation. this combined approach offers new opportunities for knowledge discovery in many areas of biology including target identification, lead hopping and drug repurposing."
we introduce two types of ordinal pattern dependence between time series. positive (resp. negative) ordinal pattern dependence can be seen as a non-paramatric and in particular non-linear counterpart to positive (resp. negative) correlation. we show in an explorative study that both types of this dependence show up in real world financial data.
"recently, mike and farmer have constructed a very powerful and realistic behavioral model to mimick the dynamic process of stock price formation based on the empirical regularities of order placement and cancelation in a purely order-driven market, which can successfully reproduce the whole distribution of returns, not only the well-known power-law tails, together with several other important stylized facts. there are three key ingredients in the mike-farmer (mf) model: the long memory of order signs characterized by the hurst index $h_s$, the distribution of relative order prices $x$ in reference to the same best price described by a student distribution (or tsallis' $q$-gaussian), and the dynamics of order cancelation. they showed that different values of the hurst index $h_s$ and the freedom degree $\alpha_x$ of the student distribution can always produce power-law tails in the return distribution $f(r)$ with different tail exponent $\alpha_r$. in this paper, we study the origin of the power-law tails of the return distribution $f(r)$ in the mf model, based on extensive simulations with different combinations of the left part $f_l(x)$ for $x<0$ and the right part $f_r(x)$ for $x>0$ of $f(x)$. we find that power-law tails appear only when $f_l(x)$ has a power-law tail, no matter $f_r(x)$ has a power-law tail or not. in addition, we find that the distributions of returns in the mf model at different timescales can be well modeled by the student distributions, whose tail exponents are close to the well-known cubic law and increase with the timescale."
"the bandstructure was calculated by the full-potential linearized augmented plane wave method. the result reveals two important insights to the novel second harmonic generation (shg) of alpha-phase lithium iodate ($\alpha-liio_{3}$) crystal: the existence of finite intra-band momentum matrix elements due to the non-inversion symmetry of the crystal illuminating the potential of the intra-band transition, and the strong covalent bonding between the $i$-atoms and the ligand $o$-atoms resulting the condition of the double-resonance. an inter-band transition scenario in shg as $\alpha-liio_{3}$ in nano-structure is proposed. the optical properties were calculated within the theoretical framework of the time-dependent perturbation of the independent-particle model. the dielectric tensors and the refractive index were evaluated. comparisons between the predictions and the results were made: the x-ray near edge absorption spectra; the refractive index at the static limit, and at finite frequencies. possible factors attributing the calculation errors is discussed."
"state-space models (ssms) are increasingly used in ecology to model time-series such as animal movement paths and population dynamics. this type of hierarchical model is often structured to account for two levels of variability: biological stochasticity and measurement error. ssms are flexible. they can model linear and nonlinear processes using a variety of statistical distributions. recent ecological ssms are often complex, with a large number of parameters to estimate. through a simulation study, we show that even simple linear gaussian ssms can suffer from parameter- and state-estimation problems. we demonstrate that these problems occur primarily when measurement error is larger than biological stochasticity, the condition that often drives ecologists to use ssms. using an animal movement example, we show how these estimation problems can affect ecological inference. biased parameter estimates of a ssm describing the movement of polar bears (\textit{ursus maritimus}) result in overestimating their energy expenditure. we suggest potential solutions, but show that it often remains difficult to estimate parameters. while ssms are powerful tools, they can give misleading results and we urge ecologists to assess whether the parameters can be estimated accurately before drawing ecological conclusions from their results."
"symbolic data analysis is based on special descriptions of data - symbolic objects (so). such descriptions preserve more detailed information about units and their clusters than the usual representations with mean values. a special kind of symbolic object is a representation with frequency or probability distributions (modal values). this representation enables us to consider in the clustering process the variables of all measurement types at the same time. in the paper a clustering criterion function for sos is proposed such that the representative of each cluster is again composed of distributions of variables' values over the cluster. the corresponding leaders clustering method is based on this result. it is also shown that for the corresponding agglomerative hierarchical method a generalized ward's formula holds. both methods are compatible - they are solving the same clustering optimization problem. the leaders method efficiently solves clustering problems with large number of units; while the agglomerative method can be applied alone on the smaller data set, or it could be applied on leaders, obtained with compatible nonhierarchical clustering method. such a combination of two compatible methods enables us to decide upon the right number of clusters on the basis of the corresponding dendrogram. the proposed methods were applied on different data sets. in the paper, some results of clustering of ess data are presented."
"we study the problem of fitting circles (or circular arcs) to data points observed with errors in both variables. a detailed error analysis for all popular circle fitting methods -- geometric fit, kasa fit, pratt fit, and taubin fit -- is presented. our error analysis goes deeper than the traditional expansion to the leading order. we obtain higher order terms, which show exactly why and by how much circle fits differ from each other. our analysis allows us to construct a new algebraic (non-iterative) circle fitting algorithm that outperforms all the existing methods, including the (previously regarded as unbeatable) geometric fit."
"we present new observations of the fundamental ro-vibrational co spectrum of v1647 ori, the young star whose recent outburst illuminated mcneil's nebula. previous spectra, acquired during outburst in 2004 february and july, had shown the co emission lines to be broad and centrally peaked-similar to the co spectrum of a typical classical t tauri star. in this paper, we present co spectra acquired shortly after the luminosity of the source returned to its pre-outburst level (2006 february) and roughly one year later (2006 december and 2007 february). the spectrum taken in 2006 february revealed blue-shifted co absorption lines superimposed on the previously observed co emission lines. the projected velocity, column density, and temperature of this outflowing gas was 30 km/s, 3^{+2}_{-1}e18 cm^{-2$, and 700^{+300}_{-100} k, respectively. the absorption lines were not observed in the 2006 december and 2007 february data, and so their strengths must have decreased in the interim by a factor of 9 or more. we discuss three mechanisms that could give rise to this unusual outflow."
"we consider the problem of how to construct a physical process over a finite state space $x$ that applies some desired conditional distribution $p$ to initial states to produce final states. this problem arises often in the thermodynamics of computation and nonequilibrium statistical physics more generally (e.g., when designing processes to implement some desired computation, feedback controller, or maxwell demon). it was previously known that some conditional distributions cannot be implemented using any master equation that involves just the states in $x$. however, here we show that any conditional distribution $p$ can in fact be implemented---if additional ""hidden"" states not in $x$ are available. moreover, we show that it is always possible to implement $p$ in a thermodynamically reversible manner. we then investigate a novel cost of the physical resources needed to implement a given distribution $p$: the minimal number of hidden states needed to do so. we calculate this cost exactly for the special case where $p$ represents a single-valued function, and provide an upper bound for the general case, in terms of the nonnegative rank of $p$. these results show that having access to one extra binary degree of freedom, thus doubling the total number of states, is sufficient to implement any $p$ with a master equation in a thermodynamically reversible way, if there are no constraints on the allowed form of the master equation. (such constraints can greatly increase the minimal needed number of hidden states.) our results also imply that for certain $p$ that can be implemented without hidden states, having hidden states permits an implementation that generates less heat."
"the perturbative effective potential v in the massless $\lambda\phi^4$ model with a global o(n) symmetry is uniquely determined to all orders by the renormalization group functions alone when the coleman-weinberg renormalization condition $\frac{d^4v}{d\phi^4}|_{\phi = \mu} = \lambda$ is used, where $\mu$ represents the renormalization scale. systematic methods are developed to express the n-loop effective potential in the coleman-weinberg scheme in terms of the known n-loop minimal subtraction (ms) renormalization-group functions. moreover, it also proves possible to sum the leading- and subsequent-to-leading-logarithm contributions to v. an essential element of this analysis is a conversion of the renormalization group functions in the coleman-weinberg scheme to the renormalization group functions in the ms scheme. as an example, the explicit five-loop effective potential is obtained from the known five-loop ms renormalization group functions and we explicitly sum the leading logarithm (ll), next-to-leading (nll), and further subleading-logarithm contributions to v. extensions of these results to massless scalar qed are also presented. because massless scalar qed has two couplings, conversion of the rg functions from the ms scheme to the cw scheme requires the use of multi-scale renormalization group methods."
"motivated by the need for accurate frequency information, a novel algorithm for estimating the fundamental frequency and its rate of change in three-phase power systems is developed. this is achieved through two stages of kalman filtering. in the first stage a quaternion extended kalman filter, which provides a unified framework for joint modeling of voltage measurements from all the phases, is used to estimate the instantaneous phase increment of the three-phase voltages. the phase increment estimates are then used as observations of the extended kalman filter in the second stage that accounts for the dynamic behavior of the system frequency and simultaneously estimates the fundamental frequency and its rate of change. the framework is then extended to account for the presence of harmonics. finally, the concept is validated through simulation on both synthetic and real-world data."
"animal behaviors are sometimes decomposable into discrete, stereotyped elements. in one model, such behaviors are triggered by specific commands; in the extreme case, the discreteness of behavior is traced to the discreteness of action potentials in the individual command neurons. we use the crawling behavior of the nematode c. elegans to explore the opposite extreme, in which discreteness and stereotypy emerges from the dynamics of the entire behavior. a simple stochastic model for the worm's continuously changing body shape during crawling has attractors corresponding to forward and backward motion; noise-driven transitions between these attractors correspond to abrupt reversals. we show that, with no free parameters, this model generates reversals at a rate within error bars of that observed experimentally, and the relatively stereotyped trajectories in the neighborhood of the reversal also are predicted correctly."
"we develop a theory of local asymptotic normality in the quantum domain based on a novel quantum analogue of the log-likelihood ratio. this formulation is applicable to any quantum statistical model satisfying a mild smoothness condition. as an application, we prove the asymptotic achievability of the holevo bound for the local shift parameter."
"humans can imagine a scene from a sound. we want machines to do so by using conditional generative adversarial networks (gans). by applying the techniques including spectral norm, projection discriminator and auxiliary classifier, compared with naive conditional gan, the model can generate images with better quality in terms of both subjective and objective evaluations. almost three-fourth of people agree that our model have the ability to generate images related to sounds. by inputting different volumes of the same sound, our model output different scales of changes based on the volumes, showing that our model truly knows the relationship between sounds and images to some extent."
"in this paper we study mean-variance hedging under the g-expectation framework. our analysis is carried out by exploiting the g-martingale representation theorem and the related probabilistic tools, in a contin- uous financial market with two assets, where the discounted risky one is modeled as a symmetric g-martingale. by tackling progressively larger classes of contingent claims, we are able to explicitly compute the optimal strategy under general assumptions on the form of the contingent claim."
"individualized treatment rules (itrs) tailor treatments according to individual patient characteristics. they can significantly improve patient care and are thus becoming increasingly popular. the data collected during randomized clinical trials are often used to estimate the optimal itrs. however, these trials are generally expensive to run, and, moreover, they are not designed to efficiently estimate itrs. in this paper, we propose a cost-effective estimation method from an active learning perspective. in particular, our method recruits only the ""most informative"" patients (in terms of learning the optimal itrs) from an ongoing clinical trial. simulation studies and real-data examples show that our active clinical trial method significantly improves on competing methods. we derive risk bounds and show that they support these observed empirical advantages."
"using the recently introduced parametric representation of non-commutative quantum field theory, we implement here the dimensional regularization and renormalization of the vulcanized $\phi^{\star 4}_4$ model on the moyal space."
"motion imaging phantoms are expensive, bulky and difficult to transport and set-up. the purpose of this paper is to demonstrate a simple approach to the design of multi-modality motion imaging phantoms that use mechanically stored energy to produce motion. we propose two phantom designs that use mainsprings and elastic bands to store energy. a rectangular piece was attached to an axle at the end of the transmission chain of each phantom, and underwent a rotary motion upon release of the mechanical motor. the phantoms were imaged with mri and us, and the image sequences were embedded in a 1d non linear manifold (laplacian eigenmap) and the spectrogram of the embedding was used to derive the angular velocity over time. the derived velocities were consistent and reproducible within a small error. the proposed motion phantom concept showed great potential for the construction of simple and affordable motion phantoms"
"let y be a normal projective variety and p a morphism from x to y, which is a projective holomorphic symplectic resolution. namikawa proved that the kuranishi deformation spaces def(x) and def(y) are both smooth, of the same dimension, and p induces a finite branched cover f from def(x) to def(y). we prove that f is galois. we proceed to calculate the galois group g, when x is simply connected, and its holomorphic symplectic structure is unique, up to a scalar factor. the singularity of y is generically of ade-type, along every codimension 2 irreducible component b of the singular locus, by namikawa's work. the modular galois group g is the product of weyl groups of finite type, indexed by such irreducible components b. each weyl group factor w_b is that of a dynkin diagram, obtained as a quotient of the dynkin diagram of the singularity-type of b, by a group of dynkin diagram automorphisms.   finally we consider generalizations of the above set-up, where y is affine symplectic, or a calabi-yau threefold with a curve of ade-singularities. we prove that the morphism f from def(x) to def(y) is a galois cover of its image. this explains the analogy between the above results and related work of nakajima, on quiver varieties, and of szendroi on enhanced gauge symmetries for calabi-yau threefolds."
"we present a simple model to describe the dark matter density, the gas density, and the gas temperature profiles of galaxy clusters. analytical expressions for these quantities are given in terms of only five free parameters with a clear physical meaning: the mass m of the dark matter halo (or the characteristic temperature t_0), the characteristic scale radius a, the cooling radius in units of a (0<alpha<1), the central temperature in units of t_0 (0<t<1), and the asymptotic baryon fraction in units of the cosmic value (f~1). it is shown that our model is able to reproduce the three-dimensional density and temperature profiles inferred from x-ray observations of real clusters within a 20 per cent accuracy over most of the radial range. some possible applications are briefly discussed."
"leading ebola subtypes exhibit a wide mortality range, here explained at the molecular level by using fractal hydropathic scaling of amino acid sequences based on protein self-organized criticality. specific hydrophobic features in the hydrophilic mucin-like domain suffice to account for the wide mortality range. significance statement: ebola virus is spreading rapidly in africa. the connection between protein amino acid sequence and mortality is identified here."
"we present a new bayesian non-parametric deprojection algorithm doping (deprojection of observed photometry using and inverse gambit), that is designed to extract 3-d luminosity density distributions $\rho$ from observed surface brightness maps $i$, in generalised geometries, while taking into account changes in intrinsic shape with radius, using a penalised likelihood approach and an mcmc optimiser. we provide the most likely solution to the integral equation that represents deprojection of the measured $i$ to $\rho$. in order to keep the solution modular, we choose to express $\rho$ as a function of the line-of-sight (los) coordinate $z$. we calculate the extent of the system along the ${\bf z}$-axis, for a given point on the image that lies within an identified isophotal annulus. the extent along the los is binned and density is held a constant over each such $z$-bin. the code begins with a seed density and at the beginning of an iterative step, the trial $\rho$ is updated. comparison of the projection of the current choice of $\rho$ and the observed $i$ defines the likelihood function (which is supplemented by laplacian regularisation), the maximal region of which is sought by the optimiser (metropolis hastings). the algorithm is successfully tested on a set of test galaxies, the morphology of which ranges from an elliptical galaxy with varying eccentricity to an infinitesimally thin disk galaxy marked by an abruptly varying eccentricity profile. applications are made to faint dwarf elliptical galaxy ic~3019 and another dwarf elliptical that is characterised by a central spheroidal nuclear component superimposed upon a more extended flattened component. the result of deprojection of the x-ray image of triaxial cluster a1413 is also presented."
this submission has been withdrawn by the author and superseded by arxiv:0804.0744.
"the inviscid limit of the stochastic burgers equation is discussed in terms of the level surfaces of the minimising hamilton-jacobi function, the classical mechanical caustic and the maxwell set and their algebraic pre-images under the classical mechanical flow map. the problem is analysed in terms of a reduced (one dimensional) action function. we demonstrate that the geometry of the caustic, level surfaces and maxwell set can change infinitely rapidly causing turbulent behaviour which is stochastic in nature. the intermittence of this turbulence is demonstrated in terms of the recurrence of two processes."
"we observe $(x_i,y_i)_{i=1}^n$ where the $y_i$'s are real valued outputs and the $x_i$'s are $m\times t$ matrices. we observe a new entry $x$ and we want to predict the output $y$ associated with it. we focus on the high-dimensional setting, where $m t \gg n$. this includes the matrix completion problem with noise, as well as other problems. we consider linear prediction procedures based on different penalizations, involving a mixture of several norms: the nuclear norm, the frobenius norm and the $\ell_1$-norm. for these procedures, we prove sharp oracle inequalities, using a statistical learning theory point of view. a surprising fact in our results is that the rates of convergence do not depend on $m$ and $t$ directly. the analysis is conducted without the usually considered incoherency condition on the unknown matrix or restricted isometry condition on the sampling operator. moreover, our results are the first to give for this problem an analysis of penalization (such nuclear norm penalization) as a regularization algorithm: our oracle inequalities prove that these procedures have a prediction accuracy close to the deterministic oracle one, given that the reguralization parameters are well-chosen."
"we study fermi edge singularities in photo-absorption spectra of generic mesoscopic systems such as quantum dots or nanoparticles. we predict deviations from macroscopic-metallic behavior and propose experimental setups for the observation of these effects. the theory is based on the model of a localized, or rank one, perturbation caused by the (core) hole left behind after the photo-excitation of an electron into the conduction band. the photo-absorption spectra result from the competition between two many-body responses, anderson's orthogonality catastrophe and the mahan-nozieres-dedominicis contribution. both mechanisms depend on the system size through the number of particles and, more importantly, fluctuations produced by the coherence characteristic of mesoscopic samples. the latter lead to a modification of the dipole matrix element and trigger one of our key results: a rounded k-edge typically found in metals will turn into a (slightly) peaked edge on average in the mesoscopic regime. we consider in detail the effect of the ""bound state"" produced by the core hole."
"in this paper we address the problem of finding the most probable state of a discrete markov random field (mrf), also known as the mrf energy minimization problem. the task is known to be np-hard in general and its practical importance motivates numerous approximate algorithms. we propose a submodular relaxation approach (smr) based on a lagrangian relaxation of the initial problem. unlike the dual decomposition approach of komodakis et al., 2011 smr does not decompose the graph structure of the initial problem but constructs a submodular energy that is minimized within the lagrangian relaxation. our approach is applicable to both pairwise and high-order mrfs and allows to take into account global potentials of certain types. we study theoretical properties of the proposed approach and evaluate it experimentally."
"online and stochastic learning has emerged as powerful tool in large scale optimization. in this work, we generalize the douglas-rachford splitting (drs) method for minimizing composite functions to online and stochastic settings (to our best knowledge this is the first time drs been generalized to sequential version). we first establish an $o(1/\sqrt{t})$ regret bound for batch drs method. then we proved that the online drs splitting method enjoy an $o(1)$ regret bound and stochastic drs splitting has a convergence rate of $o(1/\sqrt{t})$. the proof is simple and intuitive, and the results and technique can be served as a initiate for the research on the large scale machine learning employ the drs method. numerical experiments of the proposed method demonstrate the effectiveness of the online and stochastic update rule, and further confirm our regret and convergence analysis."
"cortical slow oscillations occur in the mammalian brain during deep sleep and have been shown to contribute to memory consolidation, an effect that can be enhanced by electrical stimulation. as the precise underlying working mechanisms are not known it is desired to develop and analyze computational models of slow oscillations and to study the response to electrical stimuli. in this paper we employ the conductance based model of compte et al. [j neurophysiol 89, 2707] to study the effect of electrical stimulation. the population response to electrical stimulation depends on the timing of the stimulus with respect to the state of the slow oscillation. first, we reproduce the experimental results of electrical stimulation in ferret brain slices by shu et al. [nature 423, 288] from the conductance based model. we then numerically obtain the phase response curve for the conductance based network model to quantify the network's response to weak stimuli. our results agree with experiments in vivo and in vitro that show that sensitivity to stimulation is weaker in the up than in the down state. however, we also find that within the up state stimulation leads to a shortening of the up state, or phase advance, whereas during the up-down transition a prolongation of up states is possible, resulting in a phase delay. finally, we compute the phase response curve for the simple mean-field model by ngo et al. [europhys lett 89, 68002] and find that the qualitative shape of the prc is preserved, despite its different mechanism for the generation of slow oscillations."
"the malliavin derivative for a l\'evy process $(x_t)$ can be defined on the space $\dd_{1,2}$ using a chaos expansion or in the case of a pure jump process also via an increment quotient operator \cite{sole-utzet-vives}. in this paper we define the malliavin derivative operator $\d$ on the class $\mathcal{s}$ of smooth random variables $f(x_{t_1}, ..., x_{t_n}),$ where $f$ is a smooth function with compact support. we show that the closure of $l_2(\om) \supseteq \mathcal{s} \stackrel{\d}{\to} l_2(\m\otimes \mass)$ yields to the space $\dd_{1,2}.$ as an application we conclude that lipschitz functions map from $\dd_{1,2}$ into $\dd_{1,2}.$"
"we provide explicit solutions of certain forward-backward stochastic differential equations (fbsdes) with quadratic growth. these particular fbsdes are associated with quadratic term structure models of interest rates and characterize the zero-coupon bond price. the results of this paper are naturally related to similar results on affine term structure models of hyndman (math. financ. econ. 2(2):107-128, 2009) due to the relationship between quadratic functionals of gaussian processes and linear functionals of affine processes. similar to the affine case a sufficient condition for the explicit solutions to hold is the solvability in a fixed interval of riccati-type ordinary differential equations. however, in contrast to the affine case, these riccati equations are easily associated with those occurring in linear-quadratic control problems. we also consider quadratic models for a risky asset price and characterize the futures price and forward price of the asset in terms of similar fbsdes. an example is considered, using an approach based on stochastic flows that is related to the fbsde approach, to further emphasize the parallels between the affine and quadratic models. an appendix discusses solvability and explicit solutions of the riccati equations."
"capture-recapture data are often collected when abundance estimation is of interest. in the presence of unobserved individual heterogeneity, specified on a continuous scale for the capture probabilities, the likelihood is not generally available in closed form, but expressible only as an analytically intractable integral. model-fitting algorithms to estimate abundance most notably include a numerical approximation for the likelihood or use of a bayesian data augmentation technique considering the complete data likelihood. we consider a bayesian hybrid approach, defining a ""semi-complete"" data likelihood, composed of the product of a complete data likelihood component for individuals seen at least once within the study and a marginal data likelihood component for the individuals not seen within the study, approximated using numerical integration. this approach combines the advantages of the two different approaches, with the semi-complete likelihood component specified as a single integral (over the dimension of the individual heterogeneity component). in addition, the models can be fitted within bugs/jags (commonly used for the bayesian complete data likelihood approach) but with significantly improved computational efficiency compared to the commonly used super-population data augmentation approaches (between about 10 and 77 times more efficient in the two examples we consider). the semi-complete likelihood approach is flexible and applicable to a range of models, including spatially explicit capture-recapture models. the model-fitting approach is applied to two different datasets corresponding to the closed population model $m_h$ for snowshoe hare data and a spatially explicit capture-recapture model applied to gibbon data."
"in this note, we initiate the systematic study of the lie algebra structure of the necklace lie algebra n of a free algebra in 2d variables. we begin by giving a description of n as an sp(2d)-module. specializing to d = 1, we decompose n into a direct sum of highest weight modules for sl_2, the coefficients of which are given by a closed formula. next, we observe that n has a nontrivial center, which we link through the center c of the trace ring of couples of generic 2x2 matrices to the poisson center of s(sl_2). the lie algebra structure of n induces a poisson structure on c, the symplectic leaves of which we are able to describe as coadjoint orbits for the lie group of the semidirect product sl_2\rtimes h of sl_2 with the heisenberg lie algebra h. finally, we provide a link between double poisson algebras on one hand and poisson orders on the other hand, showing that all trace rings of a double poisson algebra are poisson orders over their center."
"this paper studies game-type credit default swaps that allow the protection buyer and seller to raise or reduce their respective positions once prior to default. this leads to the study of an optimal stopping game subject to early default termination. under a structural credit risk model based on spectrally negative levy processes, we apply the principles of smooth and continuous fit to identify the equilibrium exercise strategies for the buyer and the seller. we then rigorously prove the existence of the nash equilibrium and compute the contract value at equilibrium. numerical examples are provided to illustrate the impacts of default risk and other contractual features on the players' exercise timing at equilibrium."
"we study the maximal mutual information about a random variable $y$ (representing non-private information) displayed through an additive gaussian channel when guaranteeing that only $\epsilon$ bits of information is leaked about a random variable $x$ (representing private information) that is correlated with $y$. denoting this quantity by $g_\epsilon(x,y)$, we show that for perfect privacy, i.e., $\epsilon=0$, one has $g_0(x,y)=0$ for any pair of absolutely continuous random variables $(x,y)$ and then derive a second-order approximation for $g_\epsilon(x,y)$ for small $\epsilon$. this approximation is shown to be related to the strong data processing inequality for mutual information under suitable conditions on the joint distribution $p_{xy}$. next, motivated by an operational interpretation of data privacy, we formulate the privacy-utility tradeoff in the same setup using estimation-theoretic quantities and obtain explicit bounds for this tradeoff when $\epsilon$ is sufficiently small using the approximation formula derived for $g_\epsilon(x,y)$."
"an assumption-free automatic check of medical images for potentially overseen anomalies would be a valuable assistance for a radiologist. deep learning and especially variational auto-encoders (vaes) have shown great potential in the unsupervised learning of data distributions. in principle, this allows for such a check and even the localization of parts in the image that are most suspicious. currently, however, the reconstruction-based localization by design requires adjusting the model architecture to the specific problem looked at during evaluation. this contradicts the principle of building assumption-free models. we propose complementing the localization part with a term derived from the kullback-leibler (kl)-divergence. for validation, we perform a series of experiments on fashionmnist as well as on a medical task including >1000 healthy and >250 brain tumor patients. results show that the proposed formalism outperforms the state of the art vae-based localization of anomalies across many hyperparameter settings and also shows a competitive max performance."
"in automatic speech recognition (asr) what a user says depends on the particular context she is in. typically, this context is represented as a set of word n-grams. in this work, we present a novel, all-neural, end-to-end (e2e) asr sys- tem that utilizes such context. our approach, which we re- fer to as contextual listen, attend and spell (clas) jointly- optimizes the asr components along with embeddings of the context n-grams. during inference, the clas system can be presented with context phrases which might contain out-of- vocabulary (oov) terms not seen during training. we com- pare our proposed system to a more traditional contextualiza- tion approach, which performs shallow-fusion between inde- pendently trained las and contextual n-gram models during beam search. across a number of tasks, we find that the pro- posed clas system outperforms the baseline method by as much as 68% relative wer, indicating the advantage of joint optimization over individually trained components. index terms: speech recognition, sequence-to-sequence models, listen attend and spell, las, attention, embedded speech recognition."
"we study factorization schemes for parton shower models in hadron-hadron collisions. as an example, we calculate lepton pair production mediated by a virtual photon in quark--anti-quark annihilation, and we compare factorized cross sections obtained in the conventional $\bar{\rm ms}$ scheme with those obtained in a factorization scheme in which a kinematical constraint due to parton radiation is taken into account. we discuss some properties of factorized cross sections."
"when deploying a chinese neural text-to-speech (tts) synthesis system, one of the challenges is to synthesize chinese utterances with english phrases or words embedded. this paper looks into the problem in the encoder-decoder framework when only monolingual data from a target speaker is available. specifically, we view the problem from two aspects: speaker consistency within an utterance and naturalness. we start the investigation with an average voice model which is built from multi-speaker monolingual data, i.e. mandarin and english data. on the basis of that, we look into speaker embedding for speaker consistency within an utterance and phoneme embedding for naturalness and intelligibility and study the choice of data for model training. we report the findings and discuss the challenges to build a mixed-lingual tts system with only monolingual data."
"we give existence results for solutions of the prescribed scalar curvature equation on $s^3$, when the curvature function is a positive morse function and satisfies an index-count condition."
"sampling from gaussian markov random fields (gmrfs), that is multivariate gaussian ran- dom vectors that are parameterised by the inverse of their covariance matrix, is a fundamental problem in computational statistics. in this paper, we show how we can exploit arbitrarily accu- rate approximations to a gmrf to speed up krylov subspace sampling methods. we also show that these methods can be used when computing the normalising constant of a large multivariate gaussian distribution, which is needed for both any likelihood-based inference method. the method we derive is also applicable to other structured gaussian random vectors and, in particu- lar, we show that when the precision matrix is a perturbation of a (block) circulant matrix, it is still possible to derive o(n log n) sampling schemes."
"mixtures of gaussian factors are powerful tools for modeling an unobserved heterogeneous population, offering - at the same time - dimension reduction and model-based clustering. unfortunately, the high prevalence of spurious solutions and the disturbing effects of outlying observations, along maximum likelihood estimation, open serious issues. in this paper we consider restrictions for the component covariances, to avoid spurious solutions, and trimming, to provide robustness against violations of normality assumptions of the underlying latent factors. a detailed aecm algorithm for this new approach is presented. simulation results and an application to the ais dataset show the aim and effectiveness of the proposed methodology."
"the combination of multiple classifiers using ensemble methods is increasingly important for making progress in a variety of difficult prediction problems. we present a comparative analysis of several ensemble methods through two case studies in genomics, namely the prediction of genetic interactions and protein functions, to demonstrate their efficacy on real-world datasets and draw useful conclusions about their behavior. these methods include simple aggregation, meta-learning, cluster-based meta-learning, and ensemble selection using heterogeneous classifiers trained on resampled data to improve the diversity of their predictions. we present a detailed analysis of these methods across 4 genomics datasets and find the best of these methods offer statistically significant improvements over the state of the art in their respective domains. in addition, we establish a novel connection between ensemble selection and meta-learning, demonstrating how both of these disparate methods establish a balance between ensemble diversity and performance."
"the study of good nonregular fractional factorial designs has received significant attention over the last two decades. recent research indicates that designs constructed from quaternary codes (qc) are very promising in this regard. the present paper aims at exploring the fundamental structure and developing a theory to characterize the wordlengths and aliasing indexes for a general $(1/4)^p$th-fraction qc design. then the theory is applied to (1/64)th-fraction qc designs. examples are given, indicating that there exist some qc designs that have better design properties, and are thus more cost-efficient, than the regular fractional factorial designs of the same size. in addition, a result about the periodic structure of (1/64)th-fraction qc designs regarding resolution is stated."
"using three different representations of the bicomplex numbers $t\cong cl_{c}(1,0) \cong cl_{c}(0,1)$, which is a commutative ring with zero divisors defined by $t={w_0+w_1 {i_1}+w_2{i_2}+w_3 {j} | w_0,w_1,w_2,w_3 \in{r}}$ where ${i_1^{2}}=-1, {i_2^{2}}=-1, {j^{2}}=1 and {i_1}{i_2}={j}={i_2}{i_1}$, we construct three classes of bicomplex pseudoanalytic functions. in particular, we obtain some specific systems of vekua equations of two complex variables and we established some connections between one of these systems and the classical vekua equations. we consider also the complexification of the real stationary two-dimensional schr{\""o}dinger equation. with the aid of any of its particular solutions, we construct a specific bicomplex vekua equation possessing the following special property. the scalar parts of its solutions are solutions of the original complexified schr{\""o}dinger equation and the vectorial parts are solutions of another complexified schr{\""o}dinger equation."
designs of experiments for multivariate case are reviewed. fast algorithm of construction of good latin hypercube designs is developed.
"we give a statement on extension with estimates of convex functions defined on a linear subspace, inspired by similar extension results concerning metrics on positive line bundles"
"in this article, i will prove a recurrence theorem which says that any $h^s(\mathbb{t}^2)$ (s>2) solution to the 2d euler equation returns repeatedly to an arbitrarily small $h^0(\mathbb{t}^2)$ neighborhood."
"we consider calculation of capital requirements when the underlying economic scenarios are determined by simulatable risk factors. in the respective nested simulation framework, the goal is to estimate portfolio tail risk, quantified via var or tvar of a given collection of future economic scenarios representing factor levels at the risk horizon. traditionally, evaluating portfolio losses of an outer scenario is done by computing a conditional expectation via inner-level monte carlo and is computationally expensive. we introduce several inter-related machine learning techniques to speed up this computation, in particular by properly accounting for the simulation noise. our main workhorse is an advanced gaussian process (gp) regression approach which uses nonparametric spatial modeling to efficiently learn the relationship between the stochastic factors defining scenarios and corresponding portfolio value. leveraging this emulator, we develop sequential algorithms that adaptively allocate inner simulation budgets to target the quantile region. the gp framework also yields better uncertainty quantification for the resulting var/tvar estimators that reduces bias and variance compared to existing methods. we illustrate the proposed strategies with two case-studies in two and six dimensions."
we consider two c^2 anosov diffeomorphisms in a c^1 neighborhood of a linear hyperbolic automorphism of three dimensional torus with real spectrum. we prove that they are c^1+ conjugate if and only if the differentials of the return maps at corresponding periodic points have the same eigenvalues.
"we introduce concepts of intermediate rank for countable groups that ""interpolate"" between consecutive values of the classical (integer-valued) rank. various classes of groups are proved to have intermediate rank behaviors. we are especially interested in interpolation between rank 1 and rank 2. for instance, we construct groups ""of rank 7/4"". our setting is essentially that of non positively curved spaces, where concepts of intermediate rank include polynomial rank, local rank, and mesoscopic rank. the resulting framework has interesting connections to operator algebras. we prove property rd in many cases where intermediate rank occurs. this gives a new family of groups satisfying the baum-connes conjecture. we prove that the reduced $c^*$-algebras of groups of rank 7/4 have stable rank 1."
"in this paper we present large-scale correlated calculations of linear optical absorption spectrum of oligo-acenes containing up to seven benzene rings. for the calculations we used the pariser-parr-pople (p-p-p) hamiltonian, along with the configuration interaction (ci) technique at various levels such as the full ci (fci), the quadruple ci (qci) and multi-reference singles-doubles ci (mrsdci). the role of coulomb parameters used in the p-p-p hamiltonian was examined by considering standard ohno parameters, as well as a screened set of parameters. a detailed analysis of the many-body character of the important excited states contributing to the linear absorption has also been performed. the results of our calculations have been compared extensively with the theoretical work of other authors, as well as with the experiments."
"the problem of quantile hedging for basket derivatives in the black-scholes model with correlation is considered. explicit formulas for the probability maximizing function and the cost reduction function are derived. applicability of the results for the widely traded derivatives as digital, quantos, outperformance and spread options is shown."
"compressive sensing (cs) exploits sparsity to recover sparse or compressible signals from dimensionality reducing, non-adaptive sensing mechanisms. sparsity is also used to enhance interpretability in machine learning and statistics applications: while the ambient dimension is vast in modern data analysis problems, the relevant information therein typically resides in a much lower dimensional space. however, many solutions proposed nowadays do not leverage the true underlying structure. recent results in cs extend the simple sparsity idea to more sophisticated {\em structured} sparsity models, which describe the interdependency between the nonzero components of a signal, allowing to increase the interpretability of the results and lead to better recovery performance. in order to better understand the impact of structured sparsity, in this chapter we analyze the connections between the discrete models and their convex relaxations, highlighting their relative advantages. we start with the general group sparse model and then elaborate on two important special cases: the dispersive and the hierarchical models. for each, we present the models in their discrete nature, discuss how to solve the ensuing discrete problems and then describe convex relaxations. we also consider more general structures as defined by set functions and present their convex proxies. further, we discuss efficient optimization solutions for structured sparsity problems and illustrate structured sparsity in action via three applications."
"despite the greater functional importance of protein levels, our knowledge of gene expression evolution is based almost entirely on studies of mrna levels. in contrast, our understanding of how translational regulation evolves has lagged far behind. here we have applied ribosome profiling - which measures both global mrna levels and their translation rates - to two species of saccharomyces yeast and their interspecific hybrid in order to assess the relative contributions of changes in mrna abundance and translation to regulatory evolution. we report that both cis and trans-acting regulatory divergence in translation are abundant, affecting at least 35% of genes. the majority of translational divergence acts to buffer changes in mrna abundance, suggesting a widespread role for stabilizing selection acting across regulatory levels. nevertheless, we observe evidence of lineage-specific selection acting on a number of yeast functional modules, including instances of reinforcing selection acting at both levels of regulation. finally, we also uncover multiple instances of stop-codon readthrough that are conserved between species. our analysis reveals the under-appreciated complexity of post-transcriptional regulatory divergence and indicates that partitioning the search for the locus of selection into the binary categories of 'coding' vs. 'regulatory' may overlook a significant source of selection, acting at multiple regulatory levels along the path from genotype to phenotype."
"with an eye towards human-centered automation, we contribute to the development of a systematic means to infer features of human decision-making from behavioral data. motivated by the common use of softmax selection in models of human decision-making, we study the maximum likelihood parameter estimation problem for softmax decision-making models with linear objective functions. we present conditions under which the likelihood function is convex. these allow us to provide sufficient conditions for convergence of the resulting maximum likelihood estimator and to construct its asymptotic distribution. in the case of models with nonlinear objective functions, we show how the estimator can be applied by linearizing about a nominal parameter value. we apply the estimator to fit the stochastic ucl (upper credible limit) model of human decision-making to human subject data. we show statistically significant differences in behavior across related, but distinct, tasks."
"the anatomically layered structure of a human brain results in leveled functions. in all these levels of different functions, comparison, feedback and imitation are the universal and crucial mechanisms. languages, symbols and tools play key roles in the development of human brain and entire civilization."
"this paper investigates and extends the computationally attractive nonparametric random coefficients estimator of fox, kim, ryan, and bajari (2011). we show that their estimator is a special case of the nonnegative lasso, explaining its sparse nature observed in many applications. recognizing this link, we extend the estimator, transforming it to a special case of the nonnegative elastic net. the extension improves the estimator's recovery of the true support and allows for more accurate estimates of the random coefficients' distribution. our estimator is a generalization of the original estimator and therefore, is guaranteed to have a model fit at least as good as the original one. a theoretical analysis of both estimators' properties shows that, under conditions, our generalized estimator approximates the true distribution more accurately. two monte carlo experiments and an application to a travel mode data set illustrate the improved performance of the generalized estimator."
"we propose a general adaptive lasso method for a quantile regression model. our method is very interesting when we know nothing about the first two moments of the model error. we first prove that the obtained estimators satisfy the oracle properties, which involves the relevant variable selection without using hypothesis test. next, we study the proposed method when the (multiphase) model changes to unknown observations called change-points. convergence rates of the change-points and of the regression parameters estimators in each phase are found. the sparsity of the adaptive lasso quantile estimators of the regression parameters is not affected by the change-points estimation. if the phases number is unknown, a consistent criterion is proposed. numerical studies by monte carlo simulations show the performance of the proposed method, compared to other existing methods in the literature, for models with a single phase or for multiphase models. the adaptive lasso quantile method performs better than known variable selection methods, as the least squared method with adaptive lasso penalty, $l_1$-method with lasso-type penalty and quantile method with scad penalty."
"this paper develops theoretical results regarding noisy 1-bit compressed sensing and sparse binomial regression. we show that a single convex program gives an accurate estimate of the signal, or coefficient vector, for both of these models. we demonstrate that an s-sparse signal in r^n can be accurately estimated from m = o(slog(n/s)) single-bit measurements using a simple convex program. this remains true even if each measurement bit is flipped with probability nearly 1/2. worst-case (adversarial) noise can also be accounted for, and uniform results that hold for all sparse inputs are derived as well. in the terminology of sparse logistic regression, we show that o(slog(n/s)) bernoulli trials are sufficient to estimate a coefficient vector in r^n which is approximately s-sparse. moreover, the same convex program works for virtually all generalized linear models, in which the link function may be unknown. to our knowledge, these are the first results that tie together the theory of sparse logistic regression to 1-bit compressed sensing. our results apply to general signal structures aside from sparsity; one only needs to know the size of the set k where signals reside. the size is given by the mean width of k, a computable quantity whose square serves as a robust extension of the dimension."
"the purpose of the present article is to show the multilinearity for symbols in goodwillie-lichtenbaum complex in two cases.   the first case shown is where the degree is equal to the weight. in this case, the motivic cohomology groups of a field are isomorphic to the milnor's k-groups as shown by nesterenko-suslin, totaro and suslin-voevodsky for various motivic complexes, but we give an explicit isomorphism for goodwillie-lichtenbaum complex in a form which visibly carries multilinearity of milnor's symbols to our multilinearity of motivic symbols. next, we establish multilinearity and skew-symmetry for irreducible goodwillie-lichtenbaum symbols in h^{l-1} (spec k, z(l)). these properties have been expected to hold from the author's construction of a bilinear form of dilogarithm in case k is a subfield of the field of complex numbers and l=2.   next, we establish multilinearity and skew-symmetry for goodwillie-lichtenbaum symbols in h^{l-1} (spec k, z(l)). these properties have been expected to hold from the author's construction of a bilinear form of dilogarithm in case k is a subfield of the field of complex numbers and l=2.   the multilinearity of symbols may be viewed as a generalization of the well-known formula det(ab) = det(a) det(b) for tuples of commuting matrices."
"3d human motion indexing and retrieval is an interesting problem due to the rise of several data-driven applications aimed at analyzing and/or re-utilizing 3d human skeletal data, such as data-driven animation, analysis of sports bio-mechanics, human surveillance etc. spatio-temporal articulations of humans, noisy/missing data, different speeds of the same motion etc. make it challenging and several of the existing state of the art methods use hand-craft features along with optimization based or histogram based comparison in order to perform retrieval. further, they demonstrate it only for very small datasets and few classes. we make a case for using a learned representation that should recognize the motion as well as enforce a discriminative ranking. to that end, we propose, a 3d human motion descriptor learned using a deep network. our learned embedding is generalizable and applicable to real-world data - addressing the aforementioned challenges and further enables sub-motion searching in its embedding space using another network. our model exploits the inter-class similarity using trajectory cues, and performs far superior in a self-supervised setting. state of the art results on all these fronts is shown on two large scale 3d human motion datasets - ntu rgb+d and hdm05."
"dimensional reduction of high dimensional data can be achieved by keeping only the relevant eigenmodes after principal component analysis. however, differentiating relevant eigenmodes from the random noise eigenmodes is problematic. a new method based on the random matrix theory and a statistical goodness-of-fit test is proposed in this paper. it is validated by numerical simulations and applied to real-time magnetic resonance cardiac cine images."
"many discrete mathematics problems in phylogenetics are defined in terms of the relative labeling of pairs of leaf-labeled trees. these relative labelings are naturally formalized as tanglegrams, which have previously been an object of study in coevolutionary analysis. although there has been considerable work on planar drawings of tanglegrams, they have not been fully explored as combinatorial objects until recently. in this paper, we describe how many discrete mathematical questions on trees ""factor"" through a problem on tanglegrams, and how understanding that factoring can simplify analysis. depending on the problem, it may be useful to consider a unordered version of tanglegrams, and/or their unrooted counterparts. for all of these definitions, we show how the isomorphism types of tanglegrams can be understood in terms of double cosets of the symmetric group, and we investigate their automorphisms. understanding tanglegrams better will isolate the distinct problems on leaf-labeled pairs of trees and reveal natural symmetries of spaces associated with such problems."
"we introduce the entropic measure transform (emt) problem for a general process and prove the existence of a unique optimal measure characterizing the solution. the density process of the optimal measure is characterized using a semimartingale bsde under general conditions. the emt is used to reinterpret the conditional entropic risk-measure and to obtain a convenient formula for the conditional expectation of a process which admits an affine representation under a related measure. the entropic measure transform is then used provide a new characterization of defaultable bond prices, forward prices, and futures prices when the asset is driven by a jump diffusion. the characterization of these pricing problems in terms of the emt provides economic interpretations as a maximization of returns subject to a penalty for removing financial risk as expressed through the aggregate relative entropy. the emt is shown to extend the optimal stochastic control characterization of default-free bond prices of gombani and runggaldier (math. financ. 23(4):659-686, 2013). these methods are illustrated numerically with an example in the defaultable bond setting."
we consider a probabilistic cellular automaton to analyze the stochastic dynamics of a predator-prey system. the local rules are markovian and are based in the lotka-volterra model. the individuals of each species reside on the sites of a lattice and interact with an unsymmetrical neighborhood. we look for the effect of the space anisotropy in the characterization of the oscillations of the species population densities. our study of the probabilistic cellular automaton is based on simple and pair mean-field approximations and explicitly takes into account spatial anisotropy.
"the issue of model risk in default modeling has been known since inception of the academic literature in the field. however, a rigorous treatment requires a description of all the possible models, and a measure of the distance between a single model and the alternatives, consistent with the applications. this is the purpose of the current paper. we first analytically describe all possible joint models for default, in the class of finite sequences of exchangeable bernoulli random variables. we then measure how the model risk of choosing or calibrating one of them affects the portfolio loss from default, using two popular and economically sensible metrics, value-at-risk (var) and expected shortfall (es)."
"the restricted boltzmann machine is a graphical model for binary random variables. based on a complete bipartite graph separating hidden and observed variables, it is the binary analog to the factor analysis model. we study this graphical model from the perspectives of algebraic statistics and tropical geometry, starting with the observation that its zariski closure is a hadamard power of the first secant variety of the segre variety of projective lines. we derive a dimension formula for the tropicalized model, and we use it to show that the restricted boltzmann machine is identifiable in many cases. our methods include coding theory and geometry of linear threshold functions."
"by providing a framework of accounting for the shared ancestry inherent to all life, phylogenetics is becoming the statistical foundation of biology. the importance of model choice continues to grow as phylogenetic models continue to increase in complexity to better capture micro and macroevolutionary processes. in a bayesian framework, the marginal likelihood is how data update our prior beliefs about models, which gives us an intuitive measure of comparing model fit that is grounded in probability theory. given the rapid increase in the number and complexity of phylogenetic models, methods for approximating marginal likelihoods are increasingly important. here we try to provide an intuitive description of marginal likelihoods and why they are important in bayesian model testing. we also categorize and review methods for estimating marginal likelihoods of phylogenetic models, highlighting several recent methods that provide well-behaved estimates. furthermore, we review some empirical studies that demonstrate how marginal likelihoods can be used to learn about models of evolution from biological data. we discuss promising alternatives that can complement marginal likelihoods for bayesian model choice, including posterior-predictive methods. using simulations, we find one alternative method based on approximate-bayesian computation (abc) to be biased. we conclude by discussing the challenges of bayesian model choice and future directions that promise to improve the approximation of marginal likelihoods and bayesian phylogenetics as a whole."
"we explore the probabilistic structure of dna in a number of bacterial genomes and conclude that a form of markovianness is present at the boundaries between coding and non-coding regions, that is, the sequence of start and stop codons annotated for the bacterial genome. this sequence is shown to satisfy a conditional independence property which allows its governing markov chain to be uniquely identified from the abundances of start and stop codons. furthermore, the annotated sequence is shown to comply with chargaff's second parity rule at the codon level."
"we apply the cyclic coordinate descent algorithm of friedman, hastie and tibshirani (2010) to the fitting of a conditional logistic regression model with lasso ($\ell_1$) and elastic net penalties. the sequential strong rules of tibshirani et al (2012) are also used in the algorithm and it is shown that these offer a considerable speed up over the standard coordinate descent algorithm with warm starts.   once implemented, the algorithm is used in simulation studies to compare the variable selection and prediction performance of the conditional logistic regression model against that of its unconditional (standard) counterpart. we find that the conditional model performs admirably on datasets drawn from a suitable conditional distribution, outperforming its unconditional counterpart at variable selection. the conditional model is also fit to a small real world dataset, demonstrating how we obtain regularisation paths for the parameters of the model and how we apply cross validation for this method where natural unconditional prediction rules are hard to come by."
"we decompose the k-theory space of a waldhausen category in terms of its dwyer-kan simplicial localization. this leads to a criterion for functors to induce equivalences of k-theory spectra that generalizes and explains many of the criteria appearing in the literature. we show that under mild hypotheses, a weakly exact functor that induces an equivalence of homotopy categories induces an equivalence of k-theory spectra."
new lower bounds on the minimum average hamming distance of binary codes are derived. the bounds are obtained using linear programming approach.
"the author studied the growth of the amplitude in a mathieu-like equation with multiplicative white noise. the approximate value of the exponent at the extremum on parametric resonance regions was obtained theoretically by introducing the width of time interval, and the exponents were calculated numerically by solving the stochastic differential equations by a symplectic numerical method. the mathieu-like equation contains a parameter $\alpha$ that is determined by the intensity of noise and the strength of the coupling between the variable and the noise. the value of $\alpha$ was restricted not to be negative without loss of generality. it was shown that the exponent decreases with $\alpha$, reaches a minimum and increases after that. it was also found that the exponent as a function of $\alpha$ has only one minimum at $\alpha \neq 0$ on parametric resonance regions of $\alpha = 0$. this minimum value is obtained theoretically and numerically. the existence of the minimum at $\alpha \neq 0$ indicates the suppression of the growth by multiplicative white noise."
"this work explores a simple approximation to describe isolated impurity scattering in a strongly correlated metal. the approximation combines conventional one electron scattering theory and the dynamic mean field theory to describe strong correlations in the host. it becomes exact in several limits, including those of very weak and very strong impurity potentials. original electronic structure appears at the impurity site when the impurity potential strength is moderate and the host is close to the mott transition. our results may provide useful guidance for interpretation of scanning tunneling microscopy experiments in strongly correlated systems."
"in biological tissues, it is now well-understood that mechanical cues are a powerful mechanism for pattern regulation. while much work has focused on interactions between cells and external substrates, recent experiments suggest that cell polarization and motility might be governed by the internal shear stiffness of nearby tissue, deemed ""plithotaxis"". meanwhile, other work has demonstrated that there is a direct relationship between cell shapes and tissue shear modulus in confluent tissues. joining these two ideas, we develop a hydrodynamic model that couples cell shape, and therefore tissue stiffness, to cell motility and polarization. using linear stability analysis and numerical simulations, we find that tissue behavior can be tuned between largely homogeneous states and patterned states such as asters, controlled by a composite ""morphotaxis"" parameter that encapsulates the nature of the coupling between shape and polarization. the control parameter is in principle experimentally accessible, and depends both on whether a cell tends to move in the direction of lower or higher shear modulus, and whether sinks or sources of polarization tend to fluidize the system."
"we consider a poisson process $\eta$ on a measurable space $(\by,\mathcal{y})$ equipped with a partial ordering, assumed to be strict almost everwhwere with respect to the intensity measure $\lambda$ of $\eta$. we give a clark-ocone type formula providing an explicit representation of square integrable martingales (defined with respect to the natural filtration associated with $\eta$), which was previously known only in the special case, when $\lambda$ is the product of lebesgue measure on $\r_+$ and a $\sigma$-finite measure on another space $\bx$. our proof is new and based on only a few basic properties of poisson processes and stochastic integrals. we also consider the more general case of an independent random measure in the sense of it\^o of pure jump type and show that the clark-ocone type representation leads to an explicit version of the kunita-watanabe decomposition of square integrable martingales. we also find the explicit minimal variance hedge in a quite general financial market driven by an independent random measure."
"these are the lecture notes for the summer course given for 2018 mathematical finance summer school at shandong unversity. it contains a brief introduction to the kyle model and the related topics in filtering, enlargement of filtrations and markov bridges."
"it is shown that delta hedging provides the optimal trading strategy in terms of minimal required initial capital to replicate a given terminal payoff in a continuous-time markovian context. this holds true in market models where no equivalent local martingale measure exists but only a square-integrable market price of risk. a new probability measure is constructed, which takes the place of an equivalent local martingale measure. in order to ensure the existence of the delta hedge, sufficient conditions are derived for the necessary differentiability of expectations indexed over the initial market configuration. the recently often discussed phenomenon of ""bubbles"" is a special case of the setting in this paper. several examples at the end illustrate the techniques described in this work."
"we consider the problem of maximum a posteriori (map) inference in discrete graphical models. we present a parallel map inference algorithm called bethe-admm based on two ideas: tree-decomposition of the graph and the alternating direction method of multipliers (admm). however, unlike the standard admm, we use an inexact admm augmented with a bethe-divergence based proximal function, which makes each subproblem in admm easy to solve in parallel using the sum-product algorithm. we rigorously prove global convergence of bethe-admm. the proposed algorithm is extensively evaluated on both synthetic and real datasets to illustrate its effectiveness. further, the parallel bethe-admm is shown to scale almost linearly with increasing number of cores."
"we propose a new formulation of the fluctuating lattice boltzmann equation that is consistent with both equilibrium statististical mechanics and fluctuating hydrodynamics. the formalism is based on a generalized lattice-gas model, with each velocity direction occupied by many particles. we show that the most probable state of this model corresponds to the usual equilibrium distribution of the lattice boltzmann equation. thermal fluctuations about this equilibrium are controlled by the mean number of particles at a lattice site. stochastic collision rules are described by a monte carlo process satisfying detailed balance. this allows for a straightforward derivation of discrete langevin equations for the fluctuating modes. it is shown that all non-conserved modes should be thermalized, as first pointed out by adhikari et al.; any other choice violates the condition of detailed balance. a chapman-enskog analysis is used to derive the equations of fluctuating hydrodynamics on large length and time scales; the level of fluctuations is shown to be thermodynamically consistent with the equation of state of an isothermal, ideal gas. we believe this formalism will be useful in developing new algorithms for thermal and multiphase flows."
"we treat a discrete-time asset allocation problem in an arbitrage-free, generically incomplete financial market, where the investor has a possibly non-concave utility function and wealth is restricted to remain non-negative. under easily verifiable conditions, we establish the existence of optimal portfolios."
"experience is an important asset in almost any professional activity. in basketball, there is believed to be a positive association between coaching experience and effective use of team timeouts. here, we analyze both the extent to which a team's change in scoring margin per possession after timeouts deviate from the team's average scoring margin per possession---what we called timeout factor, and the extent to which this performance measure is associated with coaching experience across all teams in the national basketball association over the 2009-2012 seasons. we find that timeout factor plays a minor role in the scoring dynamics of basketball. surprisingly, we find that timeout factor is negatively associated with coaching experience. our findings support empirical studies showing that, under certain conditions, mentors early in their careers can have a stronger positive impact on their teams than later in their careers."
"triangular lie algebras are the lie algebras which can be faithfully represented by triangular matrices of any finite size over the real/complex number field. in the paper invariants ('generalized casimir operators') are found for three classes of lie algebras, namely those which are either strictly or non-strictly triangular, and for so-called special upper triangular lie algebras. algebraic algorithm of [j. phys. a: math. gen., 2006, v.39, 5749; math-ph/0602046], developed further in [j. phys. a: math. theor., 2007, v.40, 113; math-ph/0606045], is used to determine the invariants. a conjecture of [j. phys. a: math. gen., 2001, v.34, 9085], concerning the number of independent invariants and their form, is corroborated."
"several indices that measure the degree of balance of a rooted phylogenetic tree have been proposed so far in the literature. in this work we define and study a new index of this kind, which we call the total cophenetic index: the sum, over all pairs of different leaves, of the depth of their least common ancestor. this index makes sense for arbitrary trees, can be computed in linear time and it has a larger range of values and a greater resolution power than other indices like colless' or sackin's. we compute its maximum and minimum values for arbitrary and binary trees, as well as exact formulas for its expected value for binary trees under the yule and the uniform models of evolution. as a byproduct of this study, we obtain an exact formula for the expected value of the sackin index under the uniform model, a result that seems to be new in the literature."
an adaptive nonparametric estimation procedure is constructed for the estimation problem of heteroscedastic regression when the noise variance depends on the unknown regression. a non-asymptotic upper bound for a quadratic risk (an oracle inequality) is constructed.
"general models of gibbs delaunay-voronoi tessellations, which can be viewed as extensions of ord's process, are considered. the interaction may occur on each cell of the tessellation and between neighbour cells. the tessellation may also be subjected to a geometric hardcore interaction, forcing the cells not to be too large, too small, or too flat. this setting, natural for applications, introduces some theoretical difficulties since the interaction is not necessarily hereditary. mathematical results available for studying these models are reviewed and further outcomes are provided. they concern the existence, the simulation and the estimation of such tessellations. based on these results, tools to handle these objects in practice are presented: how to simulate them, estimate their parameters and validate the fitted model. some examples of simulated tessellations are studied in details."
"a local bootstrap method is proposed for the analysis of electoral vote-count first-digit frequencies, complementing the benford's law limit. the method is calibrated on five presidential-election first rounds (2002--2006) and applied to the 2009 iranian presidential-election first round. candidate k has a highly significant (p< 0.15%) excess of vote counts starting with the digit 7. this leads to other anomalies, two of which are individually significant at p\sim 0.1%, and one at p\sim 1%. independently, iranian pre-election opinion polls significantly reject the official results unless the five polls favouring candidate a are considered alone. if the latter represent normalised data and a linear, least-squares, equal-weighted fit is used, then either candidates r and k suffered a sudden, dramatic (70%\pm 15%) loss of electoral support just prior to the election, or the official results are rejected (p\sim 0.01%)."
"matrix completion and robust principal component analysis have been widely used for the recovery of data suffering from missing entries or outliers. in many real-world applications however, the data is also time-varying, and the naive approach of per-snapshot recovery is both expensive and sub-optimal. this paper develops generative bayesian models that fit sequential multivariate measurements arising from a low-dimensional time-varying subspace. a variational bayesian subspace filtering approach is proposed that learns the underlying subspace and its state-transition matrix. different from the plethora of deterministic counterparts, the proposed approach utilizes automatic relevance determination priors that obviate the need to tune key parameters such as rank and noise power. we also propose a forward-backward algorithm that allows the updates to be carried out at low complexity. extensive tests over traffic and electricity data demonstrate the superior imputation, outlier rejection, and temporal prediction prowess of the proposed algorithm over the state-of-the-art matrix/tensor completion algorithms."
"we present a novel inference approach which we call sample out-of-sample (or sos) inference. our motivation is to propose a method which is well suited for data-driven stress testing, in which emphasis is placed on measuring the impact of (plausible) out-of-sample scenarios on a given performance measure of interest (such as a financial loss). the methodology is inspired by empirical likelihood (el), but we optimize the empirical wasserstein distance (instead of the empirical likelihood) induced by observations. from a methodological standpoint, our analysis of the asymptotic behavior of the induced wasserstein-distance profile function shows dramatic qualitative differences relative to el. for instance, in contrast to el, which typically yields chi-squared weak convergence limits, our asymptotic distributions are often not chi-squared. also, the rates of convergence that we obtain have some dependence on the dimension in a non-trivial way but which remains controlled as the dimension increases."
"we propose in this work an original estimator of the conditional intensity of a marker-dependent counting process, that is, a counting process with covariates. we use model selection methods and provide a non asymptotic bound for the risk of our estimator on a compact set. we show that our estimator reaches automatically a convergence rate over a functional class with a given (unknown) anisotropic regularity. then, we prove a lower bound which establishes that this rate is optimal. lastly, we provide a short illustration of the way the estimator works in the context of conditional hazard estimation."
"in this paper, we give a general time-varying parameter model, where the multidimensional parameter possibly includes jumps. the quantity of interest is defined as the integrated value over time of the parameter process $\theta = t^{-1} \int_0^t \theta_t^* dt$. we provide a local parametric estimator (lpe) of $\theta$ and conditions under which we can show the central limit theorem. roughly speaking those conditions correspond to some uniform limit theory in the parametric version of the problem. the framework is restricted to the specific convergence rate $n^{1/2}$. several examples of lpe are studied: estimation of volatility, powers of volatility, volatility when incorporating trading information and time-varying ma(1)."
"this book is a collection of papers dedicated to the memory of yehuda vardi. yehuda was the chair of the department of statistics of rutgers university when he passed away unexpectedly on january 13, 2005. on october 21--22, 2005, some 150 leading scholars from many different fields, including statistics, telecommunications, biomedical engineering, bioinformatics, biostatistics and epidemiology, gathered at rutgers in a conference in his honor. this conference was on ``complex datasets and inverse problems: tomography, networks, and beyond,'' and was organized by the editors. the present collection includes research work presented at the conference, as well as contributions from yehuda's colleagues. the theme of the conference was networks and other important and emerging areas of research involving incomplete data and statistical inverse problems. networks are abundant around us: communication, computer, traffic, social and energy are just a few examples. as enormous amounts of network data are collected in this information age, the field has attracted a great amount of attention from researchers in statistics and computer engineering as well as telecommunication providers and various government agencies. however, few statistical tools have been developed for analyzing network data as they are typically governed by time-varying and mutually dependent communication protocols sitting on complicated graph-structured network topologies. many prototypical applications in these and other important technologies can be viewed as statistical inverse problems with complex, massive, high-dimensional and possibly biased/incomplete data. this unifying theme of inverse problems is particularly appropriate for a conference and volume dedicated to the memory of yehuda. indeed he made influential contributions to these fields, especially in medical tomography, biased data, statistical inverse problems, and network tomography."
"these are notes on de jong's proof of the period=index theorem over fields of transcendence degree two. they are actually about the simplified proof sketched by de jong in the last section of his paper. these notes were meant as support for my lectures at the summer school ""central simple algebras over function fields"" at the universitat konstanz between august, 26 and september, 1 2007 (other lectures on this subject were given by philippe gille, andrew kresch, max lieblich, tamas szamuely and jan van geel).   no originality is intended (except perhaps a little in the proof of the artin splitting theorem). various sources on which the material is based are indicated in the notes.   the reader should be warned that these notes have not been updated to reflect developments in the subject which occurred after the end of the summerschool."
the systematic treatment of heavy quark mass effects in dis in current cteq global analysis is summarized. applications of this treatment to the comparison between theory and experimental data on dis charm production are described. the possibility of intrinsic charm in the nucleon is studied. the issue of determining the charm mass in global analysis is discussed.
we introduce a new formalism for dealing with networks of queues. the formalism is based on the doi-peliti second quantization method for reaction diffusion systems. as a demonstration of the method's utility we compute perturbatively the different time busy-busy correlations between two servers in a jackson network.
"massive stars form in dense and massive molecular cores. the exact formation mechanism is unclear, but it is possible that some massive stars are formed by processes similar to those that produce the low-mass stars, with accretion/ejection phenomena occurring at some point of the evolution of the protostar. this picture seems to be supported by the detection of a collimated stellar wind emanating from the massive protostar iras 16547-4247. a triple radio source is associated with the protostar: a compact core and two radio lobes. the emission of the southern lobe is clearly non-thermal. such emission is interpreted as synchrotron radiation produced by relativistic electrons locally accelerated at the termination point of a thermal jet. since the ambient medium is determined by the properties of the molecular cloud in which the whole system is embedded, we can expect high densities of particles and infrared photons. because of the confirmed presence of relativistic electrons, inverse compton and relativistic bremsstrahlung interactions are unavoidable. proton-proton collision should also occur, producing an injection of neutral pions. in this paper we aim at making quantitative predictions of the spectral energy distribution of the non-thermal spots generated by massive young stellar objects, with emphasis on the particular case of iras 16547-4247. we present spectral energy distributions for the southern lobe of this source, for a variety of conditions. we show that high-energy emission might be detectable from this object in the gamma-ray domain (mev to tev). the source may also be detectable at x-rays through long exposures with current x-ray instruments."
"it is common practice in empirical work to employ cluster-robust standard errors when using the linear regression model to estimate some structural/causal effect of interest. researchers also often include a large set of regressors in their model specification in order to control for observed and unobserved confounders. in this paper we develop inference methods for linear regression models with many controls and clustering. we show that inference based on the usual cluster-robust standard errors by liang and zeger (1986) is invalid in general when the number of controls is a non-vanishing fraction of the sample size. we then propose a new clustered standard errors formula that is robust to the inclusion of many controls and allows to carry out valid inference in a variety of high-dimensional linear regression models, including fixed effects panel data models and the semiparametric partially linear model. monte carlo evidence supports our theoretical results and shows that our proposed variance estimator performs well in finite samples. the proposed method is also illustrated with an empirical application that re-visits donohue iii and levitt's (2001) study of the impact of abortion on crime."
we construct all vacuum states of $\mathcal{n}=2$ supersymmetric yang-mills quantum mechanics (for su(n) group) and discuss their origin from the su(n) real cohomology.
"the paper presents results from a computational neuroscience study conducted to test vibrotactile stimuli delivered to subject fingertips and head areas in order to evoke the somatosensory brain responses utilized in a haptic brain computer interface (hbci) paradigm. we present the preliminary and very encouraging results, with subjects conducting online hbci interfacing experiments, ranging from 40% to 90% with a very fast inter-stimulus-interval (isi) of 250ms. the presented results confirm our hypothesis that the hbci paradigm concept is valid and it allows for rapid stimuli presentation in order to achieve a satisfactory information-transfer-rate of the novel bci."
"this paper gives examples of explicit arbitrage-free term structure models with l\'evy jumps via state price density approach. by generalizing quadratic gaussian models, it is found that the probability density function of a l\'evy process is a ""natural"" scale for the process to be the state variable of a market."
"we introduce a stable, well tested python implementation of the affine-invariant ensemble sampler for markov chain monte carlo (mcmc) proposed by goodman & weare (2010). the code is open source and has already been used in several published projects in the astrophysics literature. the algorithm behind emcee has several advantages over traditional mcmc sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). one major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to $\sim n^2$ for a traditional algorithm in an n-dimensional parameter space. in this document, we describe the algorithm and the details of our implementation and api. exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple cpu cores without extra effort. the code is available online at http://dan.iel.fm/emcee under the mit license."
in the present paper we develop the general theory of renormalization of some nonequilibrium diagram technique. this technique roughly is the keldysh diagram technique. to develop our theory we have used the bogoliubov -parasiuk r - operation method. our theory can be used for the studing of the divergences in kinetic equations.
"when small rnas are loaded onto argonaute proteins they can form the rna-induced silencing complexes (riscs), which mediate rna interference. risc-formation is dependent on a shared pool of argonaute proteins and risc loading factors, and is thus susceptible to competition among small rnas for loading. we present a mathematical model that aims to understand how small rna competition for the ptr resources affects target gene repression. we discuss that small rna activity is limited by risc-formation, risc-degradation and the availability of argonautes. together, these observations explain a number of ptr saturation effects encountered experimentally. we show that different competition conditions for risc-loading result in different signatures of ptr activity determined also by the amount of risc-recycling taking place. in particular, we find that the small rnas less efficient at risc-formation, using fewer resources of the ptr pathway, can perform in the low risc-recycling range equally well as their more effective counterparts. additionally, we predict a novel signature of ptr in target expression levels. under conditions of low risc-loading efficiency and high risc-recycling, the variation in target levels increases linearly with the target transcription rate. furthermore, we show that risc-recycling determines the effect that argonaute scarcity conditions have on target expression variation. our observations taken together offer a framework of predictions which can be used in order to infer from experimental data the particular characteristics of underlying ptr activity."
"recently, considerable research efforts have been devoted to the design of methods to learn from data overcomplete dictionaries for sparse coding. however, learned dictionaries require the solution of an optimization problem for coding new data. in order to overcome this drawback, we propose an algorithm aimed at learning both a dictionary and its dual: a linear mapping directly performing the coding. by leveraging on proximal methods, our algorithm jointly minimizes the reconstruction error of the dictionary and the coding error of its dual; the sparsity of the representation is induced by an $\ell_1$-based penalty on its coefficients. the results obtained on synthetic data and real images show that the algorithm is capable of recovering the expected dictionaries. furthermore, on a benchmark dataset, we show that the image features obtained from the dual matrix yield state-of-the-art classification performance while being much less computational intensive."
"we introduce and explore a new class of stationary time series models for variance matrices based on a constructive definition exploiting inverse wishart distribution theory. the main class of models explored is a novel class of stationary, first-order autoregressive (ar) processes on the cone of positive semi-definite matrices. aspects of the theory and structure of these new models for multivariate ""volatility"" processes are described in detail and exemplified. we then develop approaches to model fitting via bayesian simulation-based computations, creating a custom filtering method that relies on an efficient innovations sampler. an example is then provided in analysis of a multivariate electroencephalogram (eeg) time series in neurological studies. we conclude by discussing potential further developments of higher-order ar models and a number of connections with prior approaches."
"we develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. this learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). we first address the counterfactual nature of the learning problem through propensity scoring. next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. these constructive bounds give rise to the counterfactual risk minimization (crm) principle. we show how crm can be used to derive a new learning method -- called policy optimizer for exponential models (poem) -- for learning stochastic linear rules for structured output prediction. we present a decomposition of the poem objective that enables efficient stochastic gradient optimization. poem is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art."
"the international trade network (itn) has received renewed multidisciplinary interest due to recent advances in network theory. however, it is still unclear whether a network approach conveys additional, nontrivial information with respect to traditional international-economics analyses that describe world trade only in terms of local (first-order) properties. in this and in a companion paper, we employ a recently proposed randomization method to assess in detail the role that local properties have in shaping higher-order patterns of the itn in all its possible representations (binary/weighted, directed/undirected, aggregated/disaggregated by commodity) and across several years. here we show that, remarkably, the properties of all binary projections of the network can be completely traced back to the degree sequence, which is therefore maximally informative. our results imply that explaining the observed degree sequence of the itn, which has not received particular attention in economic theory, should instead become one the main focuses of models of trade."
"the implementation difficulties of combining distribution matching (dm) and dematching (invdm) for probabilistic shaping (ps) with soft-decision forward error correction (fec) coding can be relaxed by reverse concatenation, for which the fec coding and decoding lies inside the shaping algorithms. ps can seemingly achieve performance close to the shannon limit, although there are practical implementation challenges that need to be carefully addressed. we propose a hierarchical dm (hidm) scheme, having fully parallelized input/output interfaces and a pipelined architecture that can efficiently perform the dm/invdm without the complex operations of previously proposed methods such as constant composition dm (ccdm). furthermore, hidm can operate at a significantly larger post-fec bit error rate (ber) for the same post-invdm ber performance, which facilitates simulations. these benefits come at the cost of a slightly larger rate loss and required signal-to-noise ratio at a given post-fec ber."
"machine learning methods are applied to finding the green's function of the anderson impurity model, a basic model system of quantum many-body condensed-matter physics. different methods of parametrizing the green's function are investigated; a representation in terms of legendre polynomials is found to be superior due to its limited number of coefficients and its applicability to state of the art methods of solution. the dependence of the errors on the size of the training set is determined. the results indicate that a machine learning approach to dynamical mean-field theory may be feasible."
we address the exact recovery of the support of a k-sparse vector with orthogonal matching pursuit (omp) and orthogonal least squares (ols) in a noiseless setting. we consider the scenario where omp/ols have selected good atoms during the first l iterations (l<k) and derive a new sufficient and worst-case necessary condition for their success in k steps. our result is based on the coherence \mu of the dictionary and relaxes tropp's well-known condition \mu<1/(2k-1) to the case where omp/ols have a partial knowledge of the support.
"the focus of this paper is on detection theory for union of subspaces (uos). to this end, generalized likelihood ratio tests (glrts) are presented for detection of signals conforming to the uos model and detection of the corresponding ""active"" subspace. one of the main contributions of this paper is bounds on the performances of these glrts in terms of geometry of subspaces under various assumptions on the observation noise. the insights obtained through geometrical interpretation of the glrts are also validated through extensive numerical experiments on both synthetic and real-world data."
"we consider the problem of governing systemic risk in a banking system model. the banking system model consists in an initial value problem for a system of stochastic differential equations whose dependent variables are the log-monetary reserves of the banks as functions of time. the banking system model considered generalizes previous models studied in [5], [4], [7] and describes an homogeneous population of banks. two distinct mechanisms are used to model the cooperation among banks and the cooperation between banks and monetary authority. these mechanisms are regulated respectively by the parameters $\alpha$ and $\gamma$. a bank fails when its log-monetary reserves go below an assigned default level. we call systemic risk or systemic event in a bounded time interval the fact that in that time interval at least a given fraction of the banks fails. the probability of systemic risk in a bounded time interval is evaluated using statistical simulation. a method to govern the probability of systemic risk in a bounded time interval is presented. the goal of the governance is to keep the probability of systemic risk in a bounded time interval between two given thresholds. the governance is based on the choice of the log-monetary reserves of a kind of ""ideal bank"" as a function of time and on the solution of an optimal control problem for the mean field approximation of the banking system model. the solution of the optimal control problem determines the parameters $\alpha$ and $\gamma$ as functions of time, that is defines the rules of the borrowing and lending activity among banks and between banks and monetary authority. some numerical examples are discussed. the systemic risk governance is tested in absence and in presence of positive and negative shocks acting on the banking system."
"cyber-physical systems come with increasingly complex architectures and failure modes, which complicates the task of obtaining accurate system reliability models. at the same time, with the emergence of the (industrial) internet-of-things, systems are more and more often being monitored via advanced sensor systems. these sensors produce large amounts of data about the components' failure behaviour, and can, therefore, be fruitfully exploited to learn reliability models automatically. this paper presents an effective algorithm for learning a prominent class of reliability models, namely fault trees, from observational data. our algorithm is evolutionary in nature; i.e., is an iterative, population-based, randomized search method among fault-tree structures that are increasingly more consistent with the observational data. we have evaluated our method on a large number of case studies, both on synthetic data, and industrial data. our experiments show that our algorithm outperforms other methods and provides near-optimal results."
"dense 3d maps from wide-angle cameras is beneficial to robotics applications such as navigation and autonomous driving. in this work, we propose a real-time dense 3d mapping method for fisheye cameras without explicit rectification and undistortion. we extend the conventional variational stereo method by constraining the correspondence search along the epipolar curve using a trajectory field induced by camera motion. we also propose a fast way of generating the trajectory field without increasing the processing time compared to conventional rectified methods. with our implementation, we were able to achieve real-time processing using modern gpus. our results show the advantages of our non-rectified dense mapping approach compared to rectified variational methods and non-rectified discrete stereo matching methods."
"let k be a field and f be a siegel modular form of weight h \geq 0 and genus g>1 over k. using f, we define an invariant of the k-isomorphism class of a principally polarized abelian variety (a,a)/k of dimension g. moreover when (a,a) is the jacobian of a smooth plane curve, we show how to associate to f a classical plane invariant. as straightforward consequences of these constructions, when g=3 and k is a subfield of the complex field, we obtain (i) a new proof of a formula of klein linking the modular form \chi_{18} to the square of the discriminant of plane quartics ; (ii) a proof that one can decide when (a,a) is a jacobian over k by looking whether the value of \chi_{18} at (a,a) is a square in k. this answers a question of j.-p. serre. finally, we study the possible generalizations of this approach for g>3."
"we introduce a deterministic dealer model which implements most of the empirical laws, such as fat tails in the price change distributions, long term memory of volatility and non-poissonian intervals. we also clarify the causality between microscopic dealers' dynamics and macroscopic market's empirical laws."
"this paper presents a new estimator of the global regularity index of a multifractional brownian motion. our estimation method is based upon a ratio statistic, which compares the realized global quadratic variation of a multifractional brownian motion at two different frequencies. we show that a logarithmic transformation of this statistic converges in probability to the minimum of the hurst function, which is, under weak assumptions, identical to the global regularity index of the path."
"each and every biological function in living organism happens as a result of protein-protein interactions.the diseases are no exception to this. identifying one or more proteins for a particular disease and then designing a suitable chemical compound (known as drug) to destroy these proteins has been an interesting topic of research in bio-informatics. in previous methods, drugs were designed using only seven chemical components and were represented as a fixed-length tree. but in reality, a drug contains many chemical groups collectively known as pharmacophore. moreover, the chemical length of the drug cannot be determined before designing the drug.in the present work, a particle swarm optimization (pso) based methodology has been proposed to find out a suitable drug for a particular disease so that the drug-protein interaction becomes stable. in the proposed algorithm, the drug is represented as a variable length tree and essential functional groups are arranged in different positions of that drug. finally, the structure of the drug is obtained and its docking energy is minimized simultaneously. also, the orientation of chemical groups in the drug is tested so that it can bind to a particular active site of a target protein and the drug fits well inside the active site of target protein. here, several inter-molecular forces have been considered for accuracy of the docking energy. results show that pso performs better than the earlier methods."
"motivated by tumor growth and spatial population genetics, we study the interplay between evolutionary and spatial dynamics at the surfaces of three-dimensional, spherical range expansions. we consider range expansion radii that grow with an arbitrary power-law in time: $r(t)=r_0(1+t/t^*)^{\theta}$, where $\theta$ is a growth exponent, $r_0$ is the initial radius, and $t^*$ is a characteristic time for the growth, to be affected by the inflating geometry. we vary the parameters $t^*$ and $\theta$ to capture a variety of possible growth regimes. guided by recent results for two-dimensional inflating range expansions, we identify key dimensionless parameters that describe the survival probability of a mutant cell with a small selective advantage arising at the population frontier. using analytical techniques, we calculate this probability for arbitrary $\theta$. we compare our results to simulations of linearly inflating expansions ($\theta=1$ spherical fisher-kolmogorov-petrovsky-piscunov waves) and treadmilling populations ($\theta=0$, with cells in the interior removed by apoptosis or a similar process). we find that mutations at linearly inflating fronts have survival probabilities enhanced by factors of 100 or more relative to mutations at treadmilling population frontiers. we also discuss the special properties of ""marginally inflating"" $(\theta=1/2)$ expansions."
"this paper proposes a biologically-inspired low-level spatiochromatic-model-based similarity method (bless) to assist full-reference image-quality estimators that originally oversimplify color perception processes. more specifically, the spatiochromatic model is based on spatial frequency, spatial orientation, and surround contrast effects. the assistant similarity method is used to complement image-quality estimators based on phase congruency, gradient magnitude, and spectral residual. the effectiveness of bless is validated using fsim, fsimc and sr-sim methods on live, multiply distorted live, and tid 2013 databases. in terms of spearman correlation, bless enhances the performance of all quality estimators in color-based degradations and the enhancement is at 100% for both feature- and spectral residual-based similarity methods. moreover, bless significantly enhances the performance of sr-sim and fsim in the full tid 2013 database."
"event counts are response variables with non-negative integer values representing the number of times that an event occurs within a fixed domain such as a time interval, a geographical area or a cell of a contingency table. analysis of counts by gaussian regression models ignores the discreteness, asymmetry and heterocedasticity and is inefficient, providing unrealistic standard errors or possibily negative predictions of the expected number of events. the poisson regression is the standard model for count data with underlying assumptions on the generating process which may be implausible in many applications. statisticians have long recognized the limitation of imposing equidispersion under the poisson regression model. a typical situation is when the conditional variance exceeds the conditional mean, in which case models allowing for overdispersion are routinely used. less reported is the case of underdispersion with fewer modelling alternatives and assessments available in the literature. one of such alternatives, the gamma-count model, is adopted here in the analysis of an agronomic experiment designed to investigate the effect of levels of defoliation on different phenological states upon the number of cotton bolls. results show improvements over the poisson model and the semiparametric quasi-poisson model in capturing the observed variability in the data. estimating rather than assuming the underlying variance process lead to important insights into the process."
"we provide a new approach, along with extensions, to results in two important papers of worsley, siegmund and coworkers closely tied to the statistical analysis of fmri (functional magnetic resonance imaging) brain data. these papers studied approximations for the exceedence probabilities of scale and rotation space random fields, the latter playing an important role in the statistical analysis of fmri data. the techniques used there came either from the euler characteristic heuristic or via tube formulae, and to a large extent were carefully attuned to the specific examples of the paper. this paper treats the same problem, but via calculations based on the so-called gaussian kinematic formula. this allows for extensions of the worsley-siegmund results to a wide class of non-gaussian cases. in addition, it allows one to obtain results for rotation space random fields in any dimension via reasonably straightforward riemannian geometric calculations. previously only the two-dimensional case could be covered, and then only via computer algebra. by adopting this more structured approach to this particular problem, a solution path for other, related problems becomes clearer."
"this paper studies a formulation of 1-bit compressed sensing (cs) problem based on the maximum likelihood estimation framework. in order to solve the problem we apply the recently proposed gradient support pursuit algorithm, with a minor modification. assuming the proposed objective function has a stable restricted hessian, the algorithm is shown to accurately solve the 1-bit cs problem. furthermore, the algorithm is compared to the state-of-the-art 1-bit cs algorithms through numerical simulations. the results suggest that the proposed method is robust to noise and at mid to low input snr regime it achieves the best reconstruction snr vs. execution time trade-off."
"in general, the performance of automatic speech recognition (asr) systems is significantly degraded due to the mismatch between training and test environments. recently, a deep-learning-based image-to-image translation technique to translate an image from a source domain to a desired domain was presented, and cycle-consistent adversarial network (cyclegan) was applied to learn a mapping for speech-to-speech conversion from a speaker to a target speaker. however, this method might not be adequate to remove corrupting noise components for robust asr because it was designed to convert speech itself. in this paper, we propose a domain adaptation method based on generative adversarial nets (gans) with disentangled representation learning to achieve robustness in asr systems. in particular, two separated encoders, context and domain encoders, are introduced to learn distinct latent variables. the latent variables allow us to convert the domain of speech according to its context and domain representation. we improved word accuracies by 6.55~15.70\% for the chime4 challenge corpus by applying a noisy-to-clean environment adaptation for robust asr. in addition, similar to the method based on the cyclegan, this method can be used for gender adaptation in gender-mismatched recognition."
"although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependence, as well. we study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing and/or dependent data. many standard approaches to noisy or missing data, such as those using the em algorithm, lead to optimization problems that are inherently nonconvex, and it is difficult to establish theoretical guarantees on practical algorithms. while our approach also involves optimizing nonconvex programs, we are able to both analyze the statistical error associated with any global optimum, and more surprisingly, to prove that a simple algorithm based on projected gradient descent will converge in polynomial time to a small neighborhood of the set of all global minimizers. on the statistical side, we provide nonasymptotic bounds that hold with high probability for the cases of noisy, missing and/or dependent data. on the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm is guaranteed to converge at a geometric rate to a near-global minimizer. we illustrate these theoretical predictions with simulations, showing close agreement with the predicted scalings."
"regularization is a powerful technique for extracting useful information from noisy data. typically, it is implemented by adding some sort of norm constraint to an objective function and then exactly optimizing the modified objective function. this procedure often leads to optimization problems that are computationally more expensive than the original problem, a fact that is clearly problematic if one is interested in large-scale applications. on the other hand, a large body of empirical work has demonstrated that heuristics, and in some cases approximation algorithms, developed to speed up computations sometimes have the side-effect of performing regularization implicitly. thus, we consider the question: what is the regularized optimization objective that an approximation algorithm is exactly optimizing?   we address this question in the context of computing approximations to the smallest nontrivial eigenvector of a graph laplacian; and we consider three random-walk-based procedures: one based on the heat kernel of the graph, one based on computing the the pagerank vector associated with the graph, and one based on a truncated lazy random walk. in each case, we provide a precise characterization of the manner in which the approximation method can be viewed as implicitly computing the exact solution to a regularized problem. interestingly, the regularization is not on the usual vector form of the optimization problem, but instead it is on a related semidefinite program."
"in this paper, we develop bayesian predictive inferential procedures for prediction of repair times of a series system, applying a minimal repair strategy, using the information contained in an independent observed hybrid censored sample of the lifetimes of the components of the system, assuming the underlying distribution of the lifetimes to be rayleigh distribution. an illustrative real data example and a simulation study are presented for the purpose of illustration and comparison of the proposed predictors."
"given a remarkable representation of the generalized pauli operators of two-qubits in terms of the points of the generalized quadrangle of order two, w(2), it is shown that specific subsets of these operators can also be associated with the points and lines of the four-dimensional projective space over the galois field with two elements - the so-called veldkamp space of w(2). an intriguing novelty is the recognition of (uni- and tri-centric) triads and specific pentads of the pauli operators in addition to the ""classical"" subsets answering to geometric hyperplanes of w(2)."
"we consider shape, size and regularity of the hulls of the chordal schramm-loewner evolution driven by a symmetric alpha-stable process. we obtain derivative estimates, show that the complements of the hulls are hoelder domains, prove that the hulls have hausdorff dimension 1, and show that the trace is right-continuous with left limits almost surely."
"we derive the probability that a randomly chosen nl-node over $s$ gets localized as a function of a variety of parameters. then, we derive the probability that the whole network of nl-nodes over $s$ gets localized. in connection with the asymptotic thresholds, we show the presence of asymptotic thresholds on the network localization probability in two different scenarios. the first refers to dense networks, which arise when the domain $s$ is bounded and the densities of the two kinds of nodes tend to grow unboundedly. the second kind of thresholds manifest themselves when the considered domain increases but the number of nodes grow in such a way that the l-node density remains constant throughout the investigated domain. in this scenario, what matters is the minimum value of the maximum transmission range averaged over the fading process, denoted as $d_{max}$, above which the network of nl-nodes almost surely gets asymptotically localized."
"optical and near-infrared spectroscopy of the newly discovered peculiar l dwarf 2mass j11263991-5003550 are presented. folkes et al. identified this source as a high proper motion l9+/-1 dwarf based on its strong h2o absorption at 1.4 micron. we find that the optical spectrum of 2mass j1126-5003 is in fact consistent with that of a normal l4.5 dwarf with notably enhanced feh absorption at 9896 a. however, its near-infrared spectrum is unusually blue, with strong h2o and weak co bands similar in character to several recently identified ``blue l dwarfs''. using 2mass j1126-5003 as a case study, and guided by trends in the condensate cloud models of burrows et al. and marley et al., we find that the observed spectral peculiarities of these sources can be adequately explained by the presence of thin and/or large-grained condensate clouds as compared to normal field l dwarfs. atypical surface gravities or metallicities alone cannot reproduce the observed peculiarities, although they may be partly responsible for the unusual condensate properties. we also rule out unresolved multiplicity as a cause for the spectral peculiarities of 2mass j1126-5003. our analysis is supported by examination of spitzer mid-infrared spectral data from cushing et al. which show that bluer l dwarfs tend to have weaker 10 micron absorption, a feature tentatively associated with silicate oxide grains. with their unique spectral properties, blue l dwarfs like 2mass j1126-5003 should prove useful in studying the formation and properties of condensates and condensate clouds in low temperature atmospheres."
"the broadband, coherent nature of narrow-linewidth fiber frequency combs is exploited to measure the full complex spectrum of a molecular gas through multi-heterodyne spectroscopy. we measure the absorption and phase shift experienced by each of 155,000 individual frequency comb lines, spaced by 100 mhz and spanning from 1495 nm to 1620 nm, after passing through a hydrogen cyanide gas. the measured phase spectrum agrees with kramers-kronig transformation of the absorption spectrum. this technique can provide a full complex spectrum rapidly, over wide bandwidths, and with hertz-level accuracy."
"this paper considers the tail asymptotics for a cumulative process $\{b(t); t \ge 0\}$ sampled at a heavy-tailed random time $t$. the main contribution of this paper is to establish several sufficient conditions for the asymptotic equality ${\sf p}(b(t) > bx) \sim {\sf p}(m(t) > bx) \sim {\sf p}(t>x)$ as $x \to \infty$, where $m(t) = \sup_{0 \le u \le t}b(u)$ and $b$ is a certain positive constant. the main results of this paper can be used to obtain the subexponential asymptotics for various queueing models in markovian environments. as an example, using the main results, we derive subexponential asymptotic formulas for the loss probability of a single-server finite-buffer queue with an on/off arrival process in a markovian environment."
"equipment sharing among people who inject drugs (pwid) is a key risk factor in infection by hepatitis c virus (hcv). both the effectiveness and cost-effectiveness of interventions aimed at reducing hcv transmission in this population (such as opioid substitution therapy, needle exchange programs or improved treatment) are difficult to evaluate using field surveys. ethical issues and complicated access to the pwid population make it difficult to gather epidemiological data. in this context, mathematical modelling of hcv transmission is a useful alternative for comparing the cost and effectiveness of various interventions. several models have been developed in the past few years. they are often based on strong hypotheses concerning the population structure. this review presents compartmental and individual-based models in order to underline their strengths and limits in the context of hcv infection among pwid. the final section discusses the main results of the papers."
"for the most popular sequential change detection rules such as cusum, ewma, and the shiryaev-roberts test, we develop integral equations and a concise numerical method to compute a number of performance metrics, including average detection delay and average time to false alarm. we pay special attention to the shiryaev-roberts procedure and evaluate its performance for various initialization strategies. regarding the randomized initialization variant proposed by pollak, known to be asymptotically optimal of order-3, we offer a means for numerically computing the quasi-stationary distribution of the shiryaev-roberts statistic that is the distribution of the initializing random variable, thus making this test applicable in practice. a significant side-product of our computational technique is the observation that deterministic initializations of the shiryaev-roberts procedure can also enjoy the same order-3 optimality property as pollak's randomized test and, after careful selection, even uniformly outperform it."
"it has been shown that local algorithms based on grey-scale images sometimes lead to asymptotically unbiased estimators for surface area and integrated mean curvature. this paper extends the results to estimators for minkowski tensors. in particular, asymptotically unbiased local algorithms for estimation of all volume and surface tensors and certain mean curvature tensors are given. this requires an extension of the known asymptotic formulas to estimators with position dependent weights."
"many synoptic surveys are observing large parts of the sky multiple times. the resulting lightcurves provide a wonderful window to the dynamic nature of the universe. however, there are many significant challenges in analyzing these light curves. these include heterogeneity of the data, irregularly sampled data, missing data, censored data, known but variable measurement errors, and most importantly, the need to classify in astronomical objects in real time using these imperfect light curves. we describe a modeling-based approach using gaussian process regression for generating critical measures representing features for the classification of such lightcurves. we demonstrate that our approach performs better by comparing it with past methods. finally, we provide future directions for use in sky-surveys that are getting even bigger by the day."
"using an integration formula recently derived by conrey, farmer and zirnbauer, we calculate the expectation value of the phase factor of the fermion determinant for the staggered lattice qcd action in one dimension. we show that the chemical potential can be absorbed into the quark masses; the theory is in the same chiral symmetry class as qcd in three dimensions at zero chemical potential. in the limit of a large number of colors and fixed number of lattice points, chiral symmetry is broken spontaneously, and our results are in agreement with expressions based on a chiral lagrangian. in this limit, the eigenvalues of the dirac operator are correlated according to random matrix theory for qcd in three dimensions. the discontinuity of the chiral condensate is due to an alternative to the banks-casher formula recently discovered for qcd in four dimensions at nonzero chemical potential. the effect of temperature on the average phase factor is discussed in a schematic random matrix model."
integral-field spectroscopy of molecular hydrogen in the optical wavelength region and complementary long-slit near-infrared spectroscopy are presented towards hh91a.the detection of some 200 h_2 lines arising from ro-vibrational levels up to v'=8 ranging between 7700a and 2.3 microns is reported. the emission arises from thermally excited gas where the bulk of the material is at 2750 k and where 1% is at 6000 k. the total column density of shocked gas is n(h_2) = 10^{18} cm^{-2}. non-thermal excitation scenarios such as uv-fluorescence do not contribute to the excitation of h_2 towards hh91a. the emission is explained in terms of a slow non-dissociative j-shock which propagates into a low-density medium which has been swept-up by previous episodes of outflows which have occurred in the evolved hh90/91 complex.
unitary evolution from pure initial states to pure final states in pi(-)p-> pi(-)pi(+)n imposes constraints on pion production amplitudes that are violated by cern data on polarized target at 17.2 gev/c. the pion creation is a non-unitary process arising from a unitary co-evolution of the pion creation process with a quantum environment. the purpose of this work is to identify the interacting degrees of freedom of the environment in a high resolution amplitude analysis of cern data on polarized target for dipion masses 580-1080 mev. the s-wave spectra |s|^2 show presence of rho0(770) while p-wave spectra |l|^2 show a dip at f0(980) mass. the observed rho0(770)-f0(980) mixing is encoded in all measured density matrix elements which also encode a level splitting of the spectra arising from the interaction of the pion creation process with the environment. the analytical form of the level splitting reveals the existence of a new quantum number characterizing the environment. we propose a model for the cpt violating interaction of the pion creation process with the environment in which non-diagonal transitions between resonant qqbar modes and pi(-)pi(+) states lead to vector-scalar mixing and a dynamic entanglement of the pi(-)pi(+) isospin states. the final states pi(-)p(+)n do not posses cpt cojugate states and the concept of cpt symmetry looses its meanining.
"a fundamental problem in applying machine learning techniques for chemical problems is to find suitable representations for molecular and crystal structures. while the structure representations based on atom connectivities are prevalent for molecules, two-dimensional descriptors are not suitable for describing molecular crystals. in this work, we introduce the sfc-m family of feature representations, which are based on morton space-filling curves, as an alternative means of representing crystal structures. latent semantic indexing (lsi) was employed in a novel setting to reduce sparsity of feature representations. the quality of the sfc-m representations were assessed by using them in combination with artificial neural networks to predict density functional theory (dft) single point, ewald summed, lattice, and many-body dispersion energies of 839 organic molecular crystal unit cells from the cambridge structural database that consist of the elements c, h, n, and o. promising initial results suggest that the sfc-m representations merit further exploration to improve its ability to predict solid-state properties of organic crystal structures"
"estimation of the parameters of an exponential distribution based on record data has been treated by samaniego and whitaker (1986) and doostparast (2009). recently, doostparast and balakrishnan (2011) obtained optimal confidence intervals as well as uniformly most powerful tests for one- and two-sided hypotheses concerning location and scale parameters based on record data from a two-parameter exponential model. in this paper, we derive optimal statistical procedures including point and interval estimation as well as most powerful tests based on record data from a two-parameter pareto model. for illustrative purpose, a data set on annual wages of a sample production-line workers in a large industrial firm is analyzed using the proposed procedures."
"while machine learning has proven to be a powerful data-driven solution to many real-life problems, its use in sensitive domains has been limited due to privacy concerns. a popular approach known as **differential privacy** offers provable privacy guarantees, but it is often observed in practice that it could substantially hamper learning accuracy. in this paper we study the learnability (whether a problem can be learned by any algorithm) under vapnik's general learning setting with differential privacy constraint, and reveal some intricate relationships between privacy, stability and learnability.   in particular, we show that a problem is privately learnable **if an only if** there is a private algorithm that asymptotically minimizes the empirical risk (aerm). in contrast, for non-private learning aerm alone is not sufficient for learnability. this result suggests that when searching for private learning algorithms, we can restrict the search to algorithms that are aerm. in light of this, we propose a conceptual procedure that always finds a universally consistent algorithm whenever the problem is learnable under privacy constraint. we also propose a generic and practical algorithm and show that under very general conditions it privately learns a wide class of learning problems. lastly, we extend some of the results to the more practical $(\epsilon,\delta)$-differential privacy and establish the existence of a phase-transition on the class of problems that are approximately privately learnable with respect to how small $\delta$ needs to be."
"this paper presents a new methodology to compute first-order greeks for barrier options under the framework of path-dependent payoff functions with european, lookback, or asian type and with time-dependent trigger levels. in particular, we develop chain rules for wiener path integrals between two curves that arise in the computation of first-order greeks for barrier options. we also illustrate the effectiveness of our method through numerical examples."
"most of parameters used to describe states and dynamics of financial market depend on proportions of the appropriate variables rather than on their actual values. therefore, projective geometry seems to be the correct language to describe the theater of financial activities. we suppose that the object of interest of agents, called here baskets, form a vector space over the reals. a portfolio is defined as an equivalence class of baskets containing assets in the same proportions. therefore portfolios form a projective space. cross ratios, being invariants of projective maps, form key structures in the proposed model. quotation with respect to an asset x (i.e. in units of x) are given by linear maps. among various types of metrics that have financial interpretation, the min-max metrics on the space of quotations can be introduced. this metrics has an interesting interpretation in terms of rates of return. it can be generalized so that to incorporate a new numerical parameter (called temperature) that describes agent's lack of knowledge about the state of the market. in a dual way, a metrics on the space of market quotation is defined. in addition, one can define an interesting metric structure on the space of portfolios/quotation that is invariant with respect to hyperbolic (lorentz) symmetries of the space of portfolios. the introduced formalism opens new interesting and possibly fruitful fields of research."
"to model recurrent interaction events in continuous time, an extension of the stochastic block model is proposed where every individual belongs to a latent group and interactions between two individuals follow a conditional inhomogeneous poisson process with intensity driven by the individuals' latent groups. the model is shown to be identifiable and its estimation is based on a semiparametric variational expectation-maximization algorithm. two versions of the method are developed, using either a nonparametric histogram approach (with an adaptive choice of the partition size) or kernel intensity estimators. the number of latent groups can be selected by an integrated classification likelihood criterion. finally, we demonstrate the performance of our procedure on synthetic experiments, analyse two datasets to illustrate the utility of our approach and comment on competing methods."
"the instability of the financial system as experienced in recent years and in previous periods is often linked to credit defaults, i.e., to the failure of obligors to make promised payments. given the large number of credit contracts, this problem is amenable to be treated with approaches developed in statistical physics. we introduce the idea of ensemble averaging and thereby uncover generic features of credit risk. we then show that the often advertised concept of diversification, i.e., reducing the risk by distributing it, is deeply flawed when it comes to credit risk. the risk of extreme losses remain due to the ever present correlations, implying a substantial and persistent intrinsic danger to the financial system."
"this paper presents the r package mcs which implements the model confidence set (mcs) procedure recently developed by hansen et al. (2011). the hansen's procedure consists on a sequence of tests which permits to construct a set of 'superior' models, where the null hypothesis of equal predictive ability (epa) is not rejected at a certain confidence level. the epa statistic tests is calculated for an arbitrary loss function, meaning that we could test models on various aspects, for example punctual forecasts. the relevance of the package is shown using an example which aims at illustrating in details the use of the functions provided by the package. the example compares the ability of different models belonging to the arch family to predict large financial losses. we also discuss the implementation of the arch--type models and their maximum likelihood estimation using the popular r package rugarch developed by ghalanos (2014)."
"this paper derives a new semi closed-form approximation formula for pricing an up-and-out barrier option under a certain type of stochastic volatility model including sabr model by applying a rigorous asymptotic expansion method developed by kato, takahashi and yamada (2012). we also demonstrate the validity of our approximation method through numerical examples."
"i propose a cheap-talk model in which the sender can use private messages and only cares about persuading a subset of her audience. for example, a candidate only needs to persuade a majority of the electorate in order to win an election. i find that senders can gain credibility by speaking truthfully to some receivers while lying to others. in general settings, the model admits information transmission in equilibrium for some prior beliefs. the sender can approximate her preferred outcome when the fraction of the audience she needs to persuade is sufficiently small. i characterize the sender-optimal equilibrium and the benefit of not having to persuade your whole audience in separable environments. i also analyze different applications and verify that the results are robust to some perturbations of the model, including non-transparent motives as in crawford and sobel (1982), and full commitment as in kamenica and gentzkow (2011)."
"in this paper we show the existence of two principal eigenvalues associated to general non-convex fully nonlinear elliptic operators with neumann boundary conditions in a bounded $c^2$ domain. we study these objects and we establish some of their basic properties. finally, lipschitz regularity, uniqueness and existence results for the solution of the neumann problem are given."
"we present a mathematical analysis of the early detection of ebola virus. the propagation of the virus is analysed by using a susceptible, infected, recovered (sir) model. in order to provide useful predictions about the potential transmission of the virus, we analyse and simulate the sir model with vital dynamics, by adding demographic effects and an induced death rate. then, we compute the equilibria of the model. the numerical simulations confirm the theoretical analysis. our study describes the 2015 detection of ebola virus in guinea, the parameters of the model being identified from the world health organization data. finally, we consider an optimal control problem of the propagation of the ebola virus, minimizing the number of infected individuals while taking into account the cost of vaccination."
"our previous point-contact andreev reflection studies of the heavy-fermion superconductor cecoin$_5$ using au tips have shown two clear features: reduced andreev signal and asymmetric background conductance [1]. to explore their physical origins, we have extended our measurements to point-contact junctions between single crystalline heavy-fermion metals and superconducting nb tips. differential conductance spectra are taken on junctions with three heavy-fermion metals, cecoin$_5$, cerhin$_5$, and ybal$_3$, each with different electron mass. in contrast with au/cecoin$_5$ junctions, andreev signal is not reduced and no dependence on effective mass is observed. a possible explanation based on a two-fluid picture for heavy fermions is proposed. [1] w. k. park et al., phys. rev. b 72 052509 (2005); w. k. park et al., proc. spie-int. soc. opt. eng. 5932 59321q (2005); w. k. park et al., physica c (in press) (cond-mat/0606535)."
"gene innovation is a key mechanism on the evolution and phenotypic diversity of life forms. there is a need for tools able to study gene innovation across an increasingly large number of genomic sequences to maximally capitalise our understanding of biological systems. here we present comparative-phylostratigraphy, an open-source software suite that enables to time the emergence of new genes across evolutionary time and to correlate patterns of gene emergence with species traits simultaneously across whole genomes from multiple species. such a comparative strategy is a new powerful tool for starting to dissect the relationship between gene innovation and phenotypic diversity. we describe and showcase our method by analysing recently published ant genomes. this new methodology identified significant bouts of new gene evolution in ant clades, that are associated with shifts in life-history traits. our method allows easy integration of new genomic data as it becomes available, and thus will be a valuable analytical tool for evolutionary biologists interested in explaining the evolution of diversity of life at the level of the genes."
"in this paper, over-the-air experiments with external and internal interferences were performed using chalmers millimeter-wave multiple-input-multiple-output testbed mate. the resulting sinr for both interference experiments are compared and discussed."
"we establish a formula for the classes of certain tori in the grothendieck ring of varieties, in terms of its lambda-structure. more explicitly, we will see that if l* is the torus of invertible elements in the n-dimensional separable k-algebra l, then the class of l* can be expressed as an alternating sum of the images of the spectrum of l under the lambda-operations, multiplied by powers of the lefschetz class. this formula is suggested from the cohomology of the torus, illustrating a heuristic method that can be used in other situations. to prove the formula will require some rather explicit calculations in the grothendieck ring. to be able to make these we introduce a homomorphism from the burnside ring of the absolute galois group of k, to the grothendieck ring of varieties over k. in the process we obtain some information about the structure of the subring generated by zero-dimensional varieties."
"pain remains a major concern in patients suffering from metastatic cancer to the bone and more knowledge of the condition, as well as novel treatment avenues, are called for. neuropeptide y (npy) is a highly conserved peptide that appears to play a central role in nociceptive signaling in inflammatory and neuropathic pain. however, little is known about the peptide in cancer-induced bone pain. here, we evaluate the role of spinal npy in the mrmt-1 rat model of cancer-induced bone pain. our studies revealed an up-regulation of npy-immunoreactivity in the dorsal horn of cancer-bearing rats 17 days after inoculation, which could be a compensatory antinociceptive response. consistent with this interpretation, intrathecal administration of npy to rats with cancer-induced bone pain caused a reduction in nociceptive behaviors that lasted up to 150 min. this effect was diminished by both y1 (bibo3304) and y2 (biie0246) receptor antagonists, indicating that both receptors participate in mediating the antinociceptive effect of npy. y1 and y2 receptor binding in the spinal cord was unchanged in the cancer state as compared to sham-operated rats, consistent with the notion that increased npy results in a net antinociceptive effect in the mrmt-1 model. in conclusion, the data indicate that npy is involved in the spinal nociceptive signaling of cancer-induced bone pain and could be a new therapeutic target for patients with this condition."
"ccd was born in bell laboratories in 1969 and has been widely used in various fields. its ultra-low noise and high quantum efficiency make it work well in particle physics, high energy physics, nuclear physics and astrophysics. nowadays, more and more ccd cameras have been developed for medical diagnosis, scientific experiments, aerospace, military exploration and other fields. for the wide range of ccd cameras, a non-vacuum-cooling compact (nvcc) scientific ccd camera has been developed, including fpga-based low noise clock and bias driver circuit, data acquisition circuit, stm32-based temperature control design. at the same time, the readout noise of the imaging system is studied emphatically. the scheme to generate the ccd clock and the bias driving circuit through ultralow noise ldos is proposed. the camera was tested in a variety of environments, and the test results show that the system can run at a maximum rate of 5m pixels/s and readout noise is as low as 9.29e^- when the ccd readout speed is 500k pixels/s. finally, a series of stability tests were carried out on the camera system."
"we consider the problem of $n$-class classification ($n\geq 2$), where the classifier can choose to abstain from making predictions at a given cost, say, a factor $\alpha$ of the cost of misclassification. designing consistent algorithms for such $n$-class classification problems with a `reject option' is the main goal of this paper, thereby extending and generalizing previously known results for $n=2$. we show that the crammer-singer surrogate and the one vs all hinge loss, albeit with a different predictor than the standard argmax, yield consistent algorithms for this problem when $\alpha=\frac{1}{2}$. more interestingly, we design a new convex surrogate that is also consistent for this problem when $\alpha=\frac{1}{2}$ and operates on a much lower dimensional space ($\log(n)$ as opposed to $n$). we also generalize all three surrogates to be consistent for any $\alpha\in[0, \frac{1}{2}]$."
"we introduce a bayesian solution for the problem in forensic speaker recognition, where there may be very little background material for estimating score calibration parameters. we work within the bayesian paradigm of evidence reporting and develop a principled probabilistic treatment of the problem, which results in a bayesian likelihood-ratio as the vehicle for reporting weight of evidence. we show in contrast, that reporting a likelihood-ratio distribution does not solve this problem. our solution is experimentally exercised on a simulated forensic scenario, using nist sre'12 scores, which demonstrates a clear advantage for the proposed method compared to the traditional plugin calibration recipe."
"the paper is devoted to the relationship between psychophysics and physics of mind. the basic trends in psychophysics development are briefly discussed with special attention focused on teghtsoonian's hypotheses. these hypotheses pose the concept of the universality of inner psychophysics and enable to speak about psychological space as an individual object with its own properties. turning to the two-component description of human behavior (i. lubashevsky, physics of the human mind, springer, 2017) the notion of mental space is formulated and human perception of external stimuli is treated as the emergence of the corresponding images in the mental space. on one hand, these images are caused by external stimuli and their magnitude bears the information about the intensity of the corresponding stimuli. on the other hand, the individual structure of such images as well as their subsistence after emergence is determined only by the properties of mental space on its own. finally, the mental operations of image comparison and their scaling are defined in a way allowing for the bounded capacity of human cognition. as demonstrated, the developed theory of stimulus perception is able to explain the basic regularities of psychophysics, e.g., (i) the regression and range effects leading to the overestimation of weak stimuli and the underestimation of strong stimuli, (ii) scalar variability (weber's and ekman' laws), and (\textit{iii}) the sequential (memory) effects. as the final result, a solution to the fechner-stevens dilemma is proposed. this solution posits that fechner's logarithmic law is not a consequences of weber's law but stems from the interplay of uncertainty in evaluating stimulus intensities and the multi-step scaling required to overcome the stimulus incommensurability."
"a quantitative method is described for comparing chess openings. test openings and baseline openings are run through chess engines under controlled conditions and compared to evaluate the effectiveness of the test openings. the results are intuitively appealing and in some cases they agree with expert opinion. the specific contribution of this work is the development of an objective measure that may be used for the evaluation and refutation of chess openings, a process that had been left to thought experiments and subjective conjectures and thereby to a large variety of opinion and a great deal of debate."
"this paper considers an estimation of semiparametric functional (varying)-coefficient quantile regression with spatial data. a general robust framework is developed that treats quantile regression for spatial data in a natural semiparametric way. the local m-estimators of the unknown functional-coefficient functions are proposed by using local linear approximation, and their asymptotic distributions are then established under weak spatial mixing conditions allowing the data processes to be either stationary or nonstationary with spatial trends. application to a soil data set is demonstrated with interesting findings that go beyond traditional analysis."
"we develop algorithms for sampling from a probability distribution on a submanifold embedded in rn. applications are given to the evaluation of algorithms in 'topological statistics'; to goodness of fit tests in exponential families and to neyman's smooth test. this article is partially expository, giving an introduction to the tools of geometric measure theory."
"chronic fatigue syndrome is a protracted illness condition (lasting even years) appearing with strong flu symptoms and systemic defiances by the immune system. here, by means of statistical mechanics techniques, we study the most widely accepted picture for its genesis, namely a persistent acute mononucleosis infection, and we show how such infection may drive the immune system toward an out-of-equilibrium metastable state displaying chronic activation of both humoral and cellular responses (a state of full inflammation without a direct ""causes-effect"" reason). by exploiting a bridge with a neural scenario, we mirror killer lymphocytes $t_k$ and $b$ cells to neurons and helper lymphocytes $t_{h_1},t_{h_2}$ to synapses, hence showing that the immune system may experience the pavlov conditional reflex phenomenon: if the exposition to a stimulus (ebv antigens) lasts for too long, strong internal correlations among $b,t_k,t_h$ may develop ultimately resulting in a persistent activation even though the stimulus itself is removed. these outcomes are corroborated by several experimental findings."
"decisions in the cell that lead to its ultimate fate are important for cellular functions such as proliferation, growth, differentiation, development and death. understanding this decision process is imperative for advancements in the treatment of diseases such as cancer. it is clear that underlying gene regulatory networks and surrounding environments of the cells are crucial for function. the self-repressor is a very abundant gene regulatory motif, and is often believed to have only one cell fate. in this study, we elucidate the effects of microenvironments mimicking the epigenetic effects on cell fates through the introduction of inducers capable of binding to a self-repressing gene product (protein), thus regulating the associated gene. this alters the effective regulatory binding speed of the self-repressor regulatory protein to its destination dna without changing the gene itself. the steady state observations and real time monitoring of the self-repressor expression dynamics reveal the emergence of the two cell fates, the simulations are consistent with the experimental findings. we provide physical and quantitative explanations for the origin of the two phenotypic cell fates. we find that two cell fates, rather than a single fate, and their associated switching dynamics emerge from a change in effective gene regulation strengths. the switching time scale is quantified. our results reveal a new mechanism for the emergence of multiple cell fates. this provides an origin for the heterogeneity often observed among cell states, while illustrating the influence of microenvironments on cell fates and their decision-making processes without genetic changes"
"in this paper, a 1d convolutional neural network is designed for classification tasks of leaves with centroid contour distance curve (ccdc) as the single feature. with this classifier, simple feature as ccdc shows more discriminating power than people thought previously. the same architecture can also be applied for classifying 1 dimensional time series with little changes. experiments on some benchmark datasets shows this architecture can provide classification accuracies that are higher than some existing methods. code for the paper is available at https://github.com/dykuang/leaf project."
"a central problem in analog wireless sensor networks is to design the gain or phase-shifts of the sensor nodes (i.e. the relaying configuration) in order to achieve an accurate estimation of some parameter of interest at a fusion center, or more generally, at each node by employing a distributed parameter estimation scheme. in this paper, by using an over-parametrization of the original design problem, we devise a cyclic optimization approach that can handle tuning both gains and phase-shifts of the sensor nodes, even in intricate scenarios involving sensor selection or discrete phase-shifts. each iteration of the proposed design framework consists of a combination of the gram-schmidt process and power method-like iterations, and as a result, enjoys a low computational cost. along with formulating the design problem for a fusion center, we further present a consensus-based framework for decentralized estimation of deterministic parameters in a distributed network, which results in a similar sensor gain design problem. the numerical results confirm the computational advantage of the suggested approach in comparison with the state-of-the-art methods---an advantage that becomes more pronounced when the sensor network grows large."
"the maximal rate for non-square complex orthogonal designs (cods) with $n$ transmit antennas is ${1/2}+\frac{1}{n}$ if $n$ is even and ${1/2}+\frac{1}{n+1}$ if $n$ is odd, which are close to 1/2 for large values of $n.$ a class of maximal rate non-square cods have been constructed by liang (ieee trans. inform. theory, 2003) and lu et. al. (ieee trans. inform. theory, 2005) have shown that the decoding delay of the codes given by liang, can be reduced by 50% when number of transmit antennas is a multiple of 4. adams et. al. (ieee trans. inform. theory, 2007) have shown that the designs of liang are of minimal-delay for $n$ equal to 1 and 3 modulo 4 and that of lu et.al. are of minimal delay when $n$ is a multiple of $4.$ however, these minimal delays are large compared to the delays of the rate 1/2 non-square cods constructed by tarokh et al (ieee trans. inform. theory, 1999) from rate-1 real orthogonal designs (rods). in this paper, we construct a class of rate-1/2 non-square cods for any $n$ with the decoding delay equal to 50% of that of the delay of the rate-1/2 codes given by tarokh et al. this is achieved by giving first a general construction of rate-1 square real orthogonal designs (rods) which includes as special cases the well known constructions of adams, lax and phillips and geramita and pullman, and then making use of it to obtain the desired rate-1/2 non-square cod. for the case of 9 transmit antennas, our rate-1/2 cod is shown to be of minimal-delay. the proposed construction results in designs with zero entries which may have high peak-to-average power ratio (papr) and it is shown that by appropriate postmultiplication, a design with no zero entries can be obtained with no change in the code parameters."
"price stability has often been cited as a key reason that cryptocurrencies have not gained widespread adoption as a medium of exchange and continue to prove incapable of powering the economy of decentralized applications (dapps) efficiently. exeum proposes a novel method to provide price stable digital tokens whose values are pegged to real world assets, serving as a bridge between the real world and the decentralized economy.   pegged tokens issued by exeum - for example, usde refers to a stable token issued by the system whose value is pegged to usd - are backed by virtual assets in a virtual asset exchange where users can deposit the base token of the system and take long or short positions. guaranteeing the stability of the pegged tokens boils down to the problem of maintaining the peg of the virtual assets to real world assets, and the main mechanism used by exeum is controlling the swap rate of assets. if the swap rate is fully controlled by the system, arbitrageurs can be incentivized enough to restore a broken peg; exeum distributes statistical arbitrage trading software to decentralize this type of market making activity. the last major component of the system is a central bank equivalent that determines the long term interest rate of the base token, pays interest on the deposit by inflating the supply if necessary, and removes the need for stability fees on pegged tokens, improving their usability.   to the best of our knowledge, exeum is the first to propose a truly decentralized method for developing a stablecoin that enables 1:1 value conversion between the base token and pegged assets, completely removing the mismatch between supply and demand. in this paper, we will also discuss its applications, such as improving staking based dapp token models, price stable gas fees, pegging to an index of dapp tokens, and performing cross-chain asset transfer of legacy crypto assets."
"we consider different types of (local) products $f_1 f_2$ in fourier lebesgue spaces. furthermore, we prove the existence of such products for other distributions satisfying appropriate wave-front properties. we also consider semi-linear equations of the form $$ \qquad p(x,d)f = g(x,j_k f), $$ with appropriate polynomials $p $ and $g$. if the solution locally belongs to appropriate weighted fourier lebesgue space ${\mathscr f}l^q_{(\omega)} (\rr d)$ and $p$ is non-characteristic at $(x_0,\xi_0),$ then we prove that $(x_0,\xi_0)\not \in wf_{{\mathscr f}l^q_{(\widetilde {\omega})}} (f)$, where $\widetilde{\omega}$ depends on $\omega$, $p$ and $g$."
"we have developed a new model to describe fret efficiency (e fret) between freely-diffusing membrane probes in phase-separated bilayers (finite phase-separation fret, or fp-fret), that in principle applies to any system where phase domain dimensions are larger than ~ro . here we use monte carlo techniques to simulate e fret for a range of probe partitioning behaviors and domain sizes, and then fit the simulated data to the fp-fret model to recover simulation parameters. we find that fp-fret can determine domain size to within 5% of simulated values for domain diameters up to ~ 5ro, and to within 15% for diameters up to ~ 20ro . we also investigated the performance of the model in cases where specific model assumptions are not valid."
"the current standard for confidence interval construction in the context of a possibly misspecified model is to use an interval based on the sandwich estimate of variance. these intervals provide asymptotically correct coverage, but small-sample coverage is known to be poor. by eliminating a plug-in assumption, we derive a pivot-based method for confidence interval construction under possibly misspecified models. when compared against confidence intervals generated by the sandwich estimate of variance, this method provides more accurate coverage of the pseudo-true parameter at small sample sizes. this is shown in the results of several simulation studies. asymptotic results show that our pivot-based intervals have large sample efficiency equal to that of intervals based on the sandwich estimate of variance."
"chromatin immunoprecipitation-sequencing (chip-seq) experiments have now become routine in biology for the detection of protein binding sites. in this paper, we present a markov random field model for the joint analysis of multiple chip-seq experiments. the proposed model naturally accounts for spatial dependencies in the data, by assuming first order markov properties, and for the large proportion of zero counts, by using zero-inflated mixture distributions. in contrast to all other available implementations, the model allows for the joint modelling of multiple experiments, by incorporating key aspects of the experimental design. in particular, the model uses the information about replicates and about the different antibodies used in the experiments. an extensive simulation study shows a lower false non-discovery rate for the proposed method, compared to existing methods, at the same false discovery rate. finally, we present an analysis on real data for the detection of histone modifications of two chromatin modifiers from eight chip-seq experiments, including technical replicates with different ip efficiencies."
"fibered holomorphic dynamics are skew-product transformations over an irrational rotation, whose fibers are holomorphic functions. in this paper we study such a dynamics on a neighborhood of an invariant curve. we obtain some results analogous to the results in the non fibered case."
"we perform detrending moving average analysis (dma) and detrended fluctuation analysis (dfa) of the wti crude oil futures prices (1983-2012) to investigate its efficiency. we further put forward a strict statistical test in the spirit of bootstrapping to verify the weak-form market efficiency hypothesis by employing the dma (or dfa) exponent as the statistic. we verify the weak-form efficiency of the crude oil futures market when the whole period is considered. when we break the whole series into three sub-series separated by the outbreaks of the gulf war and the iraq war, our statistical tests uncover that only the gulf war has the impact of reducing the efficiency of the crude oil market. if we split the whole time series into two sub-series based on the signing date of the north american free trade agreement, we find that the market is inefficient in the sub-periods during which the gulf war broke out. we also perform the same analysis on short time series in moving windows and find that the market is inefficient only when some turbulent events occur, such as the oil price crash in 1985, the gulf war, and the oil price crash in 2008. our analysis may offer a new understanding of the efficiency of the crude oil futures market and shed new lights on the investigation of the efficiency in other financial markets."
"in this paper we present a review of the existing typologies of internet service users. we zoom in on social networking services including blogs and crowdsourcing websites. based on the results of the analysis of the considered typologies obtained by means of fca we developed a new user typology of a certain class of internet services, namely a collaboration innovation platform. cluster analysis of data extracted from the collaboration platform witology was used to divide more than 500 participants into six groups based on three activity indicators: idea generation, commenting, and evaluation (assigning marks) the obtained groups and their percentages appear to follow the ""90 - 9 - 1"" rule."
"we investigate the equations of anisotropic incompressible viscous fluids in $\r^3$, rotating around an inhomogeneous vector $b(t, x_1, x_2)$. we prove the global existence of strong solutions in suitable anisotropic sobolev spaces for small initial data, as well as uniformlocal existence result with respect to the rossby number in the same functional spaces under the additional assumption that $b=b(t,x_1)$ or $b=b(t,x_2)$. we also obtain the propagation of the isotropic sobolev regularity using a new refined product law."
"in this article we prove first of all the nonexistence of holomorphic submersions other than covering maps between compact quotients of complex unit balls, with a proof that works equally well in a more general equivariant setting. for a non-equidimensional surjective holomorphic map between compact ball quotients, our method applies to show that the set of critical values must be nonempty and of codimension 1. in the equivariant setting the line of arguments extend to holomorphic mappings of maximal rank into the complex projective space or the complex euclidean space, yielding in the latter case a lower estimate on the dimension of the singular locus of certain holomorphic maps defined by integrating holomorphic 1-forms. in another direction, we extend the nonexistence statement on holomorphic submersions to the case of ball quotients of finite volume, provided that the target complex unit ball is of dimension m>=2, giving in particular a new proof that a local biholomorphism between noncompact m-ball quotients of finite volume must be a covering map whenever m>=2. finally, combining our results with hermitian metric rigidity, we show that any holomorphic submersion from a bounded symmetric domain into a complex unit ball equivariant with respect to a lattice must factor through a canonical projection to yield an automorphism of the complex unit ball, provided that either the lattice is cocompact or the ball is of dimension at least 2."
"we show how to reduce the problem of symplectically embedding one 4-dimensional rational ellipsoid into another to a problem of embedding disjoint unions of balls into appropriate blow ups of \c p^2. for example, the problem of embedding the ellipsoid e(1,k) into a ball b is equivalent to that of embedding k disjoint equal balls into \c p^2, and so can be solved by the work of gromov, mcduff--polterovich and biran. (here k is the ratio of the area of the major axis to that of the minor axis.) as a consequence we show that the ball may be fully filled by the ellipsoid e(1,k) for k=1,4 and all k\ge 9, thus answering a question raised by hofer."
"high-dimensional data pose challenges in statistical learning and modeling. sometimes the predictors can be naturally grouped where pursuing the between-group sparsity is desired. collinearity may occur in real-world high-dimensional applications where the popular $l_1$ technique suffers from both selection inconsistency and prediction inaccuracy. moreover, the problems of interest often go beyond gaussian models. to meet these challenges, nonconvex penalized generalized linear models with grouped predictors are investigated and a simple-to-implement algorithm is proposed for computation. a rigorous theoretical result guarantees its convergence and provides tight preliminary scaling. this framework allows for grouped predictors and nonconvex penalties, including the discrete $l_0$ and the `$l_0+l_2$' type penalties. penalty design and parameter tuning for nonconvex penalties are examined. applications of super-resolution spectrum estimation in signal processing and cancer classification with joint gene selection in bioinformatics show the performance improvement by nonconvex penalized estimation."
"the aim of the present work is to investigate the performances of a specific bayesian control chart used to compare two processes. the chart monitors the ratio of the percentiles of a key characteristic associated with the processes. the variability of such a characteristic is modeled via the weibull distribution and a practical bayesian approach to deal with weibull data is adopted. the percentiles of the two monitored processes are assumed to be independent random variables. the weibull distributions of the key characteristic of both processes are assumed to have the same and stable shape parameter. this is usually experienced in practice because the weibull shape parameter is related to the main involved factor of variability. however, if a change of the shape parameters of the processes is suspected, the involved distributions can be used to monitor their stability. we first tested the effects of the number of the training data on the responsiveness of the chart. then we tested the robustness of the chart in spite of very poor prior information. to this end, the prior values were changed to reflect a 50% shift in both directions from the original values of the shape parameter and the percentiles of the two monitored processes. finally, various combinations of shifts were considered for the sampling distributions after the phase i, with the purpose of estimating the diagnostic ability of the charts to signal an out-of-control state. the traditional approach based on the average run length, empirically computed via a monte carlo simulation, was adopted."
"in order to compute the log-likelihood for high dimensional spatial gaussian models, it is necessary to compute the determinant of the large, sparse, symmetric positive definite precision matrix, q. traditional methods for evaluating the log-likelihood for very large models may fail due to the massive memory requirements. we present a novel approach for evaluating such likelihoods when the matrix-vector product, qv, is fast to compute. in this approach we utilise matrix functions, krylov subspaces, and probing vectors to construct an iterative method for computing the log-likelihood."
"in recent years there has been explosive growth in the number of neuroimaging studies performed using functional magnetic resonance imaging (fmri). the field that has grown around the acquisition and analysis of fmri data is intrinsically interdisciplinary in nature and involves contributions from researchers in neuroscience, psychology, physics and statistics, among others. a standard fmri study gives rise to massive amounts of noisy data with a complicated spatio-temporal correlation structure. statistics plays a crucial role in understanding the nature of the data and obtaining relevant results that can be used and interpreted by neuroscientists. in this paper we discuss the analysis of fmri data, from the initial acquisition of the raw data to its use in locating brain activity, making inference about brain connectivity and predictions about psychological or disease states. along the way, we illustrate interesting and important issues where statistics already plays a crucial role. we also seek to illustrate areas where statistics has perhaps been underutilized and will have an increased role in the future."
"let $g$ be a finite group or a compact connected lie group and let $bg$ be its classifying space. let $\mathcal{l}bg:=map(s^1,bg)$ be the free loop space of $bg$ i.e. the space of continuous maps from the circle $s^1$ to $bg$. the purpose of this paper is to study the singular homology $h_*(\mathcal lbg)$ of this loop space. we prove that when taken with coefficients in a field the homology of $\mathcal lbg$ is a homological conformal field theory. as a byproduct of our main theorem, we get a batalin-vilkovisky algebra structure on the cohomology $h^*(\mathcal lbg)$. we also prove an algebraic version of this result by showing that the hochschild cohomology $hh^*(s_* (g),s_*(g))$ of the singular chains of $g$ is a batalin-vilkovisky algebra."
"as humanity is becoming increasingly confronted by earth's finite biophysical limits, there is increasing interest in questions about the stability and equitability of a zero-growth capitalist economy, most notably: if one maintains a positive interest rate for loans, can a zero-growth economy be stable? this question has been explored on a few different macroeconomic models, and both `yes' and `no' answers have been obtained. however, economies can become unstable whether or not there is ongoing underlying growth in productivity with which to sustain growth in output. here we attempt, for the first time, to assess via a model the relative stability of growth versus no-growth scenarios. the model employed draws from keen's model of the minsky financial instability hypothesis. the analysis focuses on dynamics as opposed to equilibrium, and scenarios of growth and no-growth of output (gdp) are obtained by tweaking a productivity growth input parameter. we confirm that, with or without growth, there can be both stable and unstable scenarios. to maintain stability, firms must not change their debt levels or target debt levels too quickly. further, according to the model, the wages share is higher for zero-growth scenarios, although there are more frequent substantial drops in employment."
"perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. here we propose an information-theoretic formalization of bounded rational decision-making where decision-makers trade off expected utility and information processing costs. such bounded rational decision-makers can be thought of as thermodynamic machines that undergo physical state changes when they compute. their behavior is governed by a free energy functional that trades off changes in internal energy-as a proxy for utility-and entropic changes representing computational costs induced by changing states. as a result, the bounded rational decision-making problem can be rephrased in terms of well-known concepts from statistical physics. in the limit when computational costs are ignored, the maximum expected utility principle is recovered. we discuss the relation to satisficing decision-making procedures as well as links to existing theoretical frameworks and human decision-making experiments that describe deviations from expected utility theory. since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to axiomatically derive and interpret the thermodynamic free energy as a model of bounded rational decision-making."
"in weak gravitational lensing, the image distortion caused by shear measures the projected tidal gravitational field of the deflecting mass distribution. to lowest order, the shear is proportional to the mean image ellipticity. if the image sizes are not small compared to the scale over which the shear varies, higher-order distortions occur, called flexion. for ordinary weak lensing, the observable quantity is not the shear, but the reduced shear, owing to the mass-sheet degeneracy. likewise, the flexion itself is unobservable. rather, higher-order image distortions measure the reduced flexion, i.e., derivatives of the reduced shear. we derive the corresponding lens equation in terms of the reduced flexion and calculate the resulting relation between brightness moments of source and image. assuming an isotropic distribution of source orientations, estimates for the reduced shear and flexion are obtained; these are then tested with simulations. in particular, the presence of flexion affects the determination of the reduced shear. the results of these simulations yield the amount of bias of the estimators, as a function of the shear and flexion. we point out and quantify a fundamental limitation of the flexion formalism, in terms of the product of reduced flexion and source size. if this product increases above the derived threshold, multiple images of the source are formed locally, and the formalism breaks down. finally, we show how a general (reduced) flexion field can be decomposed into its four components: two of them are due to a shear field, carrying an e- and b-mode in general. the other two components do not correspond to a shear field; they can also be split up into corresponding e- and b-modes."
"a comprehensive research framework for a comparative analysis of candidate network architectures and protocols in the clean-slate design of next-generation optical access is proposed. the proposed research framework consists of a comparative analysis framework based on multivariate non-inferiority testing and a notion of equivalent circuit rate taking into account user-perceived performances, and a virtual test bed providing a complete experimental platform for the comparative analysis. the capability of the research framework is demonstrated through numerical results from the study of the elasticity of hybrid tdm/wdm-pon based on tunable transceivers."
"it is shown that beck and mackey electromagnetic model of dark energy in superconductors can account for the non-classical inertial properties of superconductors, which have been conjectured by the author to explain the cooper pair's mass excess reported by cabrera and tate. a new einstein-planck regime for gravitation in condensed matter is proposed as a natural scale to host the gravitoelectrodynamic properties of superconductors."
"we calculate the lyapunov exponent for the non-hermitian zakharov-shabat eigenvalue problem corresponding to the attractive non-linear schroedinger equation with a gaussian random pulse as initial value function. using an extension of the thouless formula to non-hermitian random operators, we calculate the corresponding average density of states. we analyze two cases, one with circularly symmetric complex gaussian pulses and the other with real gaussian pulses. we discuss the implications in the context of the information transmission through non-linear optical fibers."
"in this work, we present a general procedure, which is able to generate new exact solitonic models in 1+1 dimensions, from a known one, consisting of two coupled scalar fields. an interesting consequence of the method, is that of the appearing of nontrivial extensions, where the deformed systems presents other bps solitons than that appearing in the original model. finally we take a particular example, in order to check the above mentioned features."
"differential privacy formalises privacy-preserving mechanisms that provide access to a database. we pose the question of whether bayesian inference itself can be used directly to provide private access to data, with no modification. the answer is affirmative: under certain conditions on the prior, sampling from the posterior distribution can be used to achieve a desired level of privacy and utility. to do so, we generalise differential privacy to arbitrary dataset metrics, outcome spaces and distribution families. this allows us to also deal with non-i.i.d or non-tabular datasets. we prove bounds on the sensitivity of the posterior to the data, which gives a measure of robustness. we also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. finally, we provide bounds on the utility and on the distinguishability of datasets. the latter are complemented by a novel use of le cam's method to obtain lower bounds. all our general results hold for arbitrary database metrics, including those for the common definition of differential privacy. for specific choices of the metric, we give a number of examples satisfying our assumptions."
"population diversity is essential for avoiding premature convergence in genetic algorithms (gas) and for the effective use of crossover. yet the dynamics of how diversity emerges in populations are not well understood. we use rigorous runtime analysis to gain insight into population dynamics and ga performance for the ($\mu$+1) ga and the $\text{jump}_k$ test function. we show that the interplay of crossover and mutation may serve as a catalyst leading to a sudden burst of diversity. this leads to improvements of the expected optimisation time of order $\omega(n/\log n)$ compared to mutation-only algorithms like (1+1) ea. moreover, increasing the mutation rate by an arbitrarily small constant factor can facilitate the generation of diversity, leading to speedups of order $\omega(n)$. we also compare seven commonly used diversity mechanisms and evaluate their impact on runtime bounds for the ($\mu$+1) ga. all previous results in this context only hold for unrealistically low crossover probability $p_c=o(k/n)$, while we give analyses for the setting of constant $p_c < 1$ in all but one case.   for the typical case of constant $k > 2$ and constant $p_c$, we can compare the resulting expected runtimes for different diversity mechanisms assuming an optimal choice of $\mu$: $o(n^{k-1})$ for duplicate elimination/minim., $o(n^2\log n)$ for maximising the convex hull, $o(n\log n)$ for deterministic crowding (assuming $p_c = k/n$), $o(n\log n)$ for maximising hamming distance, $o(n\log n)$ for fitness sharing, $o(n\log n)$ for single-receiver island model.   this proves a sizeable advantage of all variants of the ($\mu$+1) ga compared to (1+1) ea, which requires time $\theta(n^k)$. experiments complement our theoretical findings and further highlight the benefits of crossover and diversity on $\text{jump}_k$."
"understanding how the time-complexity of evolutionary algorithms (eas) depend on their parameter settings and characteristics of fitness landscapes is a fundamental problem in evolutionary computation. most rigorous results were derived using a handful of key analytic techniques, including drift analysis. however, since few of these techniques apply effortlessly to population-based eas, most time-complexity results concern simplified eas, such as the (1+1) ea.   this paper describes the level-based theorem, a new technique tailored to population-based processes. it applies to any non-elitist process where offspring are sampled independently from a distribution depending only on the current population. given conditions on this distribution, our technique provides upper bounds on the expected time until the process reaches a target state.   we demonstrate the technique on several pseudo-boolean functions, the sorting problem, and approximation of optimal solutions in combinatorial optimisation. the conditions of the theorem are often straightforward to verify, even for genetic algorithms and estimation of distribution algorithms which were considered highly non-trivial to analyse. finally, we prove that the theorem is nearly optimal for the processes considered. given the information the theorem requires about the process, a much tighter bound cannot be proved."
"in section 1, we present a number of classical results concerning the generalized gamma convolution (:ggc) variables, their wiener-gamma representations, and relation with the dirichlet processes.to a ggc variable, one may associate a unique thorin measure. let $g$ a positive r.v. and $\gamma_t(g)$ (resp. $\gamma_t(1/g))$ the generalized gamma convolution with thorin measure $t$-times the law of $g$ (resp. the law of $1/g$). in section 2, we compare the laws of $\gamma_t(g)$ and $\gamma_t(1/g)$.in section 3, we present some old and some new examples of ggc variables, among which the lengths of excursions of bessel processes straddling an independent exponential time."
"let $(x,y)\in\mathcal{x}\times \mathcal{y}$ be a random couple with unknown distribution $p$. let $\gg$ be a class of measurable functions and $\ell$ a loss function. the problem of statistical learning deals with the estimation of the bayes: $$g^*=\arg\min_{g\in\gg}\e_p \ell(g(x),y). $$ in this paper, we study this problem when we deal with a contaminated sample $(z_1,y_1),..., (z_n,y_n)$ of i.i.d. indirect observations. each input $z_i$, $i=1,...,n$ is distributed from a density $af$, where $a$ is a known compact linear operator and $f$ is the density of the direct input $x$. we derive fast rates of convergence for empirical risk minimizers based on regularization methods, such as deconvolution kernel density estimators or spectral cut-off. these results are comparable to the existing fast rates in koltchinskii for the direct case. it gives some insights into the effect of indirect measurements in the presence of fast rates of convergence."
"if the baryon asymmetry of the universe is produced by leptogenesis, cp violation is required in the lepton sector. in the seesaw extension of the standard model with three hierarchical right-handed neutrinos, we show that the baryon asymmetry is insensitive to the pmns phases: thermal leptogenesis can work for any value of the observable phases. this result was well-known when there are no flavour effects in leptogenesis; we show that it remains true when flavour effects are included."
"a signal with discrete frequency components, has a zero bispectrum if no linear combination of the frequencies equals one of the frequency components. we introduce fractional bispectrum in which for such signals the fractional bispectrum is nonzero. it is shown that fractional bispectrum has the same property as bispectrum for gaussian signals: the fractional bispectrum of a zero mean gaussian signal is zero; therefore it can be used to eliminate or reduce the gaussian noise."
"accurate prediction of inter-residue contacts of a protein is important to calcu- lating its tertiary structure. analysis of co-evolutionary events among residues has been proved effective to inferring inter-residue contacts. the markov ran- dom field (mrf) technique, although being widely used for contact prediction, suffers from the following dilemma: the actual likelihood function of mrf is accurate but time-consuming to calculate, in contrast, approximations to the actual likelihood, say pseudo-likelihood, are efficient to calculate but inaccu- rate. thus, how to achieve both accuracy and efficiency simultaneously remains a challenge. in this study, we present such an approach (called clmdca) for contact prediction. unlike plmdca using pseudo-likelihood, i.e., the product of conditional probability of individual residues, our approach uses composite- likelihood, i.e., the product of conditional probability of all residue pairs. com- posite likelihood has been theoretically proved as a better approximation to the actual likelihood function than pseudo-likelihood. meanwhile, composite likelihood is still efficient to maximize, thus ensuring the efficiency of clmdca. we present comprehensive experiments on popular benchmark datasets, includ- ing psicov dataset and casp-11 dataset, to show that: i) clmdca alone outperforms the existing mrf-based approaches in prediction accuracy. ii) when equipped with deep learning technique for refinement, the prediction ac- curacy of clmdca was further significantly improved, suggesting the suitability of clmdca for subsequent refinement procedure. we further present successful application of the predicted contacts to accurately build tertiary structures for proteins in the psicov dataset.   accessibility: the software clmdca and a server are publicly accessible through http://protein.ict.ac.cn/clmdca/."
"in this work, we aim to explore the potential of machine learning methods to the problem of beehive sound recognition. a major contribution of this work is the creation and release of annotations for a selection of beehive recordings. by experimenting with both support vector machines and convolutional neural networks, we explore important aspects to be considered in the development of beehive sound recognition systems using machine learning approaches."
"a dna palindrome is a segment of double-stranded dna sequence with inver- sion symmetry which may form secondary structures conferring significant biolog- ical functions ranging from rna transcription to dna replication. to test if the clusters of dna palindromes distribute randomly is an interesting bioinformatic problem, where the occurrence rate of the dna palindromes is a key estimator for setting up a test. the most commonly used statistics for estimating the occur- rence rate for scan statistics is the average rate. however, in our simulation, the average rate may double the null occurrence rate of dna palindromes due to hot spot regions of 3000 bp's in a herpes virus genome. here, we propose a formula to estimate the occurrence rate through an analytic derivation under a markov assumption on dna sequence. our simulation study shows that the performance of this method has improved the accuracy and robustness against hot spots, as compared to the commonly used average rate. in addition, we derived analytical formula for the moment-generating functions of various statistics under a markov model, enabling further calculations of p-values."
"many networks are complex dynamical systems, where both attributes of nodes and topology of the network (link structure) can change with time. we propose a model of co-evolving networks where both node at- tributes and network structure evolve under mutual influence. specifically, we consider a mixed membership stochastic blockmodel, where the probability of observing a link between two nodes depends on their current membership vectors, while those membership vectors themselves evolve in the presence of a link between the nodes. thus, the network is shaped by the interaction of stochastic processes describing the nodes, while the processes themselves are influenced by the changing network structure. we derive an efficient variational inference procedure for our model, and validate the model on both synthetic and real-world data."
"we develop a recursion for hidden markov model of any order h, which allows us to obtain the posterior distribution of the latent state at every occasion, given the previous h states and the observed data. with respect to the well-known baum-welch recursions, the proposed recursion has the advantage of being more direct to use and, in particular, of not requiring dummy renormalizations to avoid numerical problems. we also show how this recursion may be expressed in matrix notation, so as to allow for an efficient implementation, and how it may be used to obtain the manifest distribution of the observed data and for parameter estimation within the expectation-maximization algorithm. the approach is illustrated by an application to financial data which is focused on the study of the dynamics of the volatility level of log-returns."
"we consider regression scenarios where it is natural to impose an order constraint on the coefficients. we propose an order-constrained version of l1-regularized regression for this problem, and show how to solve it efficiently using the well-known pool adjacent violators algorithm as its proximal operator. the main application of this idea is time-lagged regression, where we predict an outcome at time t from features at the previous k time points. in this setting it is natural to assume that the coefficients decay as we move farther away from t, and hence the order constraint is reasonable. potential applications include financial time series and prediction of dynamic patient out- comes based on clinical measurements. we illustrate this idea on real and simulated data."
"panel studies typically suffer from attrition, which reduces sample size and can result in biased inferences. it is impossible to know whether or not the attrition causes bias from the observed panel data alone. refreshment samples - new, randomly sampled respondents given the questionnaire at the same time as a subsequent wave of the panel - offer information that can be used to diagnose and adjust for bias due to attrition. we review and bolster the case for the use of refreshment samples in panel studies. we include examples of both a fully bayesian approach for analyzing the concatenated panel and refreshment data, and a multiple imputation approach for analyzing only the original panel. for the latter, we document a positive bias in the usual multiple imputation variance estimator. we present models appropriate for three waves and two refreshment samples, including nonterminal attrition. we illustrate the three-wave analysis using the 2007-2008 associated press-yahoo! news election poll."
"we show that the so-called hidden potential symmetries considered in a recent paper [gandarias m., physica a, 2008, v.387, 2234-2242] are ordinary potential symmetries that can be obtained using the method introduced by bluman and collaborators. in fact, these are simplest potential symmetries associated with potential systems which are constructed with single conservation laws having no constant characteristics. furthermore we classify the conservation laws for classes of porous medium equations and then using the corresponding conserved (potential) systems we search for potential symmetries. this is the approach one needs to adopt in order to determine the complete list of potential symmetries. the provenance of potential symmetries is explained for the porous medium equations by using potential equivalence transformations. point and potential equivalence transformations are also applied to deriving new results on potential symmetries and corresponding invariant solutions from known ones. in particular, in this way the potential systems, potential conservation laws and potential symmetries of linearizable equations from the classes of differential equations under consideration are exhaustively described. infinite series of infinite-dimensional algebras of potential symmetries are constructed for such equations."
"this thesis studies two problems in modern statistics. first, we study selective inference, or inference for hypothesis that are chosen after looking at the data. the motiving application is inference for regression coefficients selected by the lasso. we present the condition-on-selection method that allows for valid selective inference, and study its application to the lasso, and several other selection algorithms.   in the second part, we consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. we present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning. in previous work, authors have considered structure learning of gaussian graphical models and structure learning of discrete models. our approach is a natural generalization of these two lines of work to the mixed case. the penalization scheme involves a novel symmetric use of the group-lasso norm and follows naturally from a particular parametrization of the model. we provide conditions under which our estimator is model selection consistent in the high-dimensional regime."
"this paper presents analytical solutions to the problem of how to calculate sensible var (value-at-risk) and es (expected shortfall) contributions in the creditrisk+ methodology. via the es contributions, es itself can be exactly computed in finitely many steps. the methods are illustrated by numerical examples."
"we show that the bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. we prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. further, we discuss the role of boosting in unsupervised learning."
"we present a novel methodology to determine the fundamental value of firms in the social-networking sector based on two ingredients: (i) revenues and profits are inherently linked to its user basis through a direct channel that has no equivalent in other sectors; (ii) the growth of the number of users can be calibrated with standard logistic growth models and allows for reliable extrapolations of the size of the business at long time horizons. we illustrate the methodology with a detailed analysis of facebook, one of the biggest of the social-media giants. there is a clear signature of a change of regime that occurred in 2010 on the growth of the number of users, from a pure exponential behavior (a paradigm for unlimited growth) to a logistic function with asymptotic plateau (a paradigm for growth in competition). we consider three different scenarios, a base case, a high growth and an extreme growth scenario. using a discount factor of 5%, a profit margin of 29% and 3.5 usd of revenues per user per year yields a value of facebook of 15.3 billion usd in the base case scenario, 20.2 billion usd in the high growth scenario and 32.9 billion usd in the extreme growth scenario. according to our methodology, this would imply that facebook would need to increase its profit per user before the ipo by a factor of 3 to 6 in the base case scenario, 2.5 to 5 in the high growth scenario and 1.5 to 3 in the extreme growth scenario in order to meet the current, widespread, high expectations. to prove the wider applicability of our methodology, the analysis is repeated on groupon, the well-known deal-of-the-day website which is expected to go public in november 2011. the results are in line with the facebook analysis. customer growth will plateau. by not taking this fundamental property of the growth process into consideration, estimates of its ipo are wildly overpriced."
"cooling is the main process leading to the condensation of gas in the dark matter potential wells and consequently to star and structure formation. in a metal-free environment, the main available coolants are h, he, h$_2$ and hd; once the gas is enriched with metals, these also become important in defining the cooling properties of the gas. we discuss the implementation in gadget-2 of molecular and metal cooling at temperatures lower that $\rm10^4 k$, following the time dependent properties of the gas and pollution from stellar evolution. we have checked the validity of our scheme comparing the results of some test runs with previous calculations of cosmic abundance evolution and structure formation, finding excellent agreement. we have also investigated the relevance of molecule and metal cooling in some specific cases, finding that inclusion of hd cooling results in a higher clumping factor of the gas at high redshifts, while metal cooling at low temperatures can have a significant impact on the formation and evolution of cold objects."
"in this paper an approach for finding a sparse incomplete cholesky factor through an incomplete orthogonal factorization with givens rotations is discussed and applied to gaussian markov random fields (gmrfs). the incomplete cholesky factor obtained from the incomplete orthogonal factorization is usually sparser than the commonly used cholesky factor obtained through the standard cholesky factorization. on the computational side, this approach can provide a sparser cholesky factor, which gives a computationally more efficient representation of gmrfs. on the theoretical side, this approach is stable and robust and always returns a sparse cholesky factor. since this approach applies both to square matrices and to rectangle matrices, it works well not only on precision matrices for gmrfs but also when the gmrfs are conditioned on a subset of the variables or on observed data. some common structures for precision matrices are tested in order to illustrate the usefulness of the approach. one drawback to this approach is that the incomplete orthogonal factorization is usually slower than the standard cholesky factorization implemented in standard libraries and currently it can be slower to build the sparse cholesky factor."
"several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space. the resulting eigenvectors encode the embedding coordinates for the training samples only, and so the embedding of novel data samples requires further costly computation. in this paper, we present a method for the out-of-sample extension of graph embeddings using deep neural networks (dnn) to parametrically approximate these nonlinear maps. compared with traditional nonparametric out-of-sample extension methods, we demonstrate that the dnns can generalize with equal or better fidelity and require orders of magnitude less computation at test time. moreover, we find that unsupervised pretraining of the dnns improves optimization for larger network sizes, thus removing sensitivity to model selection."
"the paper pursues two connected goals. firstly, we establish the li-yau-hamilton estimate for the heat equation on a manifold $m$ with nonempty boundary. results of this kind are typically used to prove monotonicity formulas related to geometric flows. secondly, we establish bounds for a solution $\nabla(t)$ of the yang-mills heat equation in a vector bundle over $m$. the li-yau-hamilton estimate is utilized in the proofs. our results imply that the curvature of $\nabla(t)$ does not blow up if the dimension of $m$ is less than 4 or if the initial energy of $\nabla(t)$ is sufficiently small."
"in this paper, the study of bivariate generalised beta type i and ii distributions is extended to the complex matrix variate case, for which the corresponding density functions are found. in addition, for complex bimatrix variate beta type i distributions, several basic properties, including the joint eigenvalue density and the maximum eigenvalue distribution, are studied."
"many biological phenomena undergo developmental changes in time and space. functional mapping, which is aimed at mapping genes that affect developmental patterns, is instrumental for studying the genetic architecture of biological changes. often biological processes are mediated by a network of developmental and physiological components and, therefore, are better described by multiple phenotypes. in this article, we develop a multivariate model for functional mapping that can detect and characterize quantitative trait loci (qtls) that simultaneously control multiple dynamic traits. because the true genotypes of qtls are unknown, the measurements for the multiple dynamic traits are modeled using a mixture distribution. the functional means of the multiple dynamic traits are estimated using the nonparametric regression method, which avoids any parametric assumption on the functional means. we propose the profile likelihood method to estimate the mixture model. a likelihood ratio test is exploited to test for the existence of pleiotropic effects on distinct but developmentally correlated traits. a simulation study is implemented to illustrate the finite sample performance of our proposed method. we also demonstrate our method by identifying qtls that simultaneously control three dynamic traits of soybeans. the three dynamic traits are the time-course biomass of the leaf, the stem, and the root of the whole soybean. the genetic linkage map is constructed with 950 microsatellite markers. the new model can aid in our comprehension of the genetic control mechanisms of complex dynamic traits over time."
"collaborative filtering (cf) aims to predict users' ratings on items according to historical user-item preference data. in many real-world applications, preference data are usually sparse, which would make models overfit and fail to give accurate predictions. recently, several research works show that by transferring knowledge from some manually selected source domains, the data sparseness problem could be mitigated. however for most cases, parts of source domain data are not consistent with the observations in the target domain, which may misguide the target domain model building. in this paper, we propose a novel criterion based on empirical prediction error and its variance to better capture the consistency across domains in cf settings. consequently, we embed this criterion into a boosting framework to perform selective knowledge transfer. comparing to several state-of-the-art methods, we show that our proposed selective transfer learning framework can significantly improve the accuracy of rating prediction tasks on several real-world recommendation tasks."
"consider an investor trading dynamically to maximize expected utility from terminal wealth. our aim is to study the dependence between her risk aversion and the distribution of the optimal terminal payoff.   economic intuition suggests that high risk aversion leads to a rather concentrated distribution, whereas lower risk aversion results in a higher average payoff at the expense of a more widespread distribution.   dybvig and wang [j. econ. theory, 2011, to appear] find that this idea can indeed be turned into a rigorous mathematical statement in one-period models. more specifically, they show that lower risk aversion leads to a payoff which is larger in terms of second order stochastic dominance.   in the present study, we extend their results to (weakly) complete continuous-time models. we also complement an ad-hoc counterexample of dybvig and wang, by showing that these results are ""fragile"", in the sense that they fail in essentially any model, if the latter is perturbed on a set of arbitrarily small probability. on the other hand, we establish that they hold for power investors in models with (conditionally) independent increments."
"in this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix m, and in particular the observed samples consist only of ones and no zeros. this problem is motivated by modern applications such as recommender systems and social networks where only ""likes"" or ""friendships"" are observed. the problem of learning from only positive and unlabeled examples, called pu (positive-unlabeled) learning, has been studied in the context of binary classification. we consider the pu matrix completion problem, where an underlying real-valued matrix m is first quantized to generate one-bit observations and then a subset of positive entries is revealed. under the assumption that m has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) m parameterizes a distribution that generates a binary matrix, 2) m is thresholded to obtain a binary matrix. for the first case, we propose a ""shifted matrix completion"" method that recovers m using only a subset of indices corresponding to ones, while for the second case, we propose a ""biased matrix completion"" method that recovers the (thresholded) binary matrix. both methods yield strong error bounds --- if m is n by n, the frobenius error is bounded as o(1/((1-rho)n), where 1-rho denotes the fraction of ones observed. this implies a sample complexity of o(n\log n) ones to achieve a small error, when m is dense and n is large. we extend our methods and guarantees to the inductive matrix completion problem, where rows and columns of m have associated features. we provide efficient and scalable optimization procedures for both the methods and demonstrate the effectiveness of the proposed methods for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks."
"in this paper we propose and demonstrate the potential for unifying models and algorithms for the steady state and transient simulation of single-phase and three-phase power systems. at present, disparate algorithms and models are used for the different analyses, which can lead to inconsistencies such as the transient analysis as time approaches infinity not matching the steady state analysis of the same conditions. using our equivalent circuit formulation of the power system, we propose a methodology for forming physics-based models that can facilitate transient, balanced power flow, and three-phase power flow in one simulation environment. the approach is demonstrated on a three-phase induction motor. existing industry tools are used to validate the model and simulation results for the different analyses."
"we improve the inequality used in pronzato [2003. removing non-optimal support points in d-optimum design algorithms. statist. probab. lett. 63, 223-228] to remove points from the design space during the search for a $d$-optimum design. let $\xi$ be any design on a compact space $\mathcal{x} \subset \mathbb{r}^m$ with a nonsingular information matrix, and let $m+\epsilon$ be the maximum of the variance function $d(\xi,\mathbf{x})$ over all $\mathbf{x} \in \mathcal{x}$. we prove that any support point $\mathbf{x}_{*}$ of a $d$-optimum design on $\mathcal{x}$ must satisfy the inequality $d(\xi,\mathbf{x}_{*}) \geq m(1+\epsilon/2-\sqrt{\epsilon(4+\epsilon-4/m)}/2)$. we show that this new lower bound on $d(\xi,\mathbf{x}_{*})$ is, in a sense, the best possible, and how it can be used to accelerate algorithms for $d$-optimum design."
"the measurement of the scale of the baryon acoustic oscillations (bao) in the galaxy power spectrum as a function of redshift is a promising method to constrain the equation-of-state parameter of the dark energy w. to measure the scale of the bao precisely, a substantial volume of space must be surveyed. we test whether light-cone effects are important and whether the scaling relations used to compensate for an incorrect reference cosmology are in this case sufficiently accurate. we investigate the degeneracies in the cosmological parameters and the benefits of using the two-dimensional anisotropic power spectrum. finally, we estimate the uncertainty with which w can be measured by proposed surveys at redshifts of about z=3 and z=1, respectively.   in the simulated survey we find that light-cone effects are small and that the simple scaling relations used to correct for the cosmological distortion work fairly well even for large survey volumes. the analysis of the two-dimensional anisotropic power spectra enables an independent determination to be made of the apparent scale of the bao, perpendicular and parallel to the line of sight. this is essential for two-parameter w-models, such as the redshift-dependent dark energy model w=w_0+(1-a)w_a. using planck priors for the matter and baryon density and delta(h_0)=5% for the hubble constant, we estimate that the bao measurements of future surveys around z=3 and z=1 will be able to constrain, independently of other cosmological probes, a constant w to ~ 12% and ~ 11% (68% c.l.), respectively."
"this work proposes new inference methods for a regression coefficient of interest in a (heterogeneous) quantile regression model. we consider a high-dimensional model where the number of regressors potentially exceeds the sample size but a subset of them suffice to construct a reasonable approximation to the conditional quantile function. the proposed methods are (explicitly or implicitly) based on orthogonal score functions that protect against moderate model selection mistakes, which are often inevitable in the approximately sparse model considered in the present paper. we establish the uniform validity of the proposed confidence regions for the quantile regression coefficient. importantly, these methods directly apply to more than one variable and a continuum of quantile indices. in addition, the performance of the proposed methods is illustrated through monte-carlo experiments and an empirical example, dealing with risk factors in childhood malnutrition."
"this paper is the fifth in a series exploring the physical consequences of the solidity of glass-forming liquids. paper iv proposed a model where the density field is described by a time-dependent ginzburg-landau equation of the nonconserved type with rates in $k$ space of the form $\gamma_0+dk^2$. the model assumes that $d\gg\gamma_0a^2$ where $a$ is the average intermolecular distance; this inequality expresses a long-wavelength dominance of the dynamics which implies that the hamiltonian (free energy) to a good approximation may be taken to be ultralocal. in the present paper we argue that this is the simplest model consistent with the following three experimental facts: 1) viscous liquids approaching the glass transition do not develop long-range order; 2) the glass has lower compressibility than the liquid; 3) the alpha process involves several decades of relaxation times shorter than the mean relaxation time. the paper proceeds to list six further experimental facts characterizing equilibrium viscous liquid dynamics and shows that these are readily understood in terms of the model; some are direct consequences, others are quite natural when viewed in light of the model."
"neutron- and proton-induced fission cross-sections of separated isotopes of tungsten (182w, 183w, 184w, and 186w) and 181ta relative to 209bi have been measured in the incident nucleon energy region 50 - 200 mev using fission chambers based on thin-film breakdown counters (tfbc) using quasi-monoenergetic neutrons from the 7li(p,n) reaction and at the proton beams of the svedberg laboratory (tsl), uppsala university (uppsala, sweden). the results are compared with predictions by the cem03.01 event generator, as well as with the recent data for nuclei in the lead-bismuth region. the effect of ""compound nucleus"" in the intermediate energy region is discussed, displaying in exponential dependence of nucleon-induced fission cross-sections on the parameter z^2/a of the composite system (projectile+target nucleus), and in other characteristics of the fission process for which parameter z^2/a plays a role similar to the one of the usual liquid-drop parameter z^2/a of compound nuclei."
"we study a generalized framework for structured sparsity. it extends the well-known methods of lasso and group lasso by incorporating additional constraints on the variables as part of a convex optimization problem. this framework provides a straightforward way of favouring prescribed sparsity patterns, such as orderings, contiguous regions and overlapping groups, among others. existing optimization methods are limited to specific constraint sets and tend to not scale well with sample size and dimensionality. we propose a novel first order proximal method, which builds upon results on fixed points and successive approximations. the algorithm can be applied to a general class of conic and norm constraints sets and relies on a proximity operator subproblem which can be computed explicitly. experiments on different regression problems demonstrate the efficiency of the optimization algorithm and its scalability with the size of the problem. they also demonstrate state of the art statistical performance, which improves over lasso and structomp."
"time series prediction covers a vast field of every-day statistical applications in medical, environmental and economic domains. in this paper we develop nonparametric prediction strategies based on the combination of a set of 'experts' and show the universal consistency of these strategies under a minimum of conditions. we perform an in-depth analysis of real-world data sets and show that these nonparametric strategies are more flexible, faster and generally outperform arma methods in terms of normalized cumulative prediction error."
"community detection in networks has drawn much attention in diverse fields, especially social sciences. given its significance, there has been a large body of literature with approaches from many fields. here we present a statistical framework that is representative, extensible, and that yields an estimator with good properties. our proposed approach considers a stochastic blockmodel based on a logistic regression formulation with node correction terms. we follow a bayesian approach that explicitly captures the community behavior via prior specification. we further adopt a data augmentation strategy with latent polya-gamma variables to obtain posterior samples. we conduct inference based on a principled, canonically mapped centroid estimator that formally addresses label non-identifiability and captures representative community assignments. we demonstrate the proposed model and estimation on real-world as well as simulated benchmark networks and show that the proposed model and estimator are more flexible, representative, and yield smaller error rates when compared to the map estimator from classical degree-corrected stochastic blockmodels."
"most scientific institutions acknowledge the importance of opening the so-called 'ivory tower' of academic research through popularization, industrial collaboration or teaching. however, little is known about the actual openness of scientific institutions and how their proclaimed priorities translate into concrete measures. this paper gives an idea of some actual practices by studying three key points: the proportion of researchers who are active in wider dissemination, the academic productivity of these scientists, and the institutional recognition of their wider dissemination activities in terms of their careers. we analyze extensive data about the academic production, career recognition and teaching or public/industrial outreach of several thousand of scientists, from many disciplines, from france's centre national de la recherche scientifique. we find that, contrary to what is often suggested, scientists active in wider dissemination are also more active academically. however, their dissemination activities have almost no impact (positive or negative) on their careers."
"neural responses are highly variable, and some portion of this variability arises from fluctuations in modulatory factors that alter their gain, such as adaptation, attention, arousal, expected or actual reward, emotion, and local metabolic resource availability. regardless of their origin, fluctuations in these signals can confound or bias the inferences that one derives from spiking responses. recent work demonstrates that for sensory neurons, these effects can be captured by a modulated poisson model, whose rate is the product of a stimulus-driven response function and an unknown modulatory signal. here, we extend this model, by incorporating explicit modulatory elements that are known (specifically, spike-history dependence, as in previous models), and by constraining the remaining latent modulatory signals to be smooth in time. we develop inference procedures for fitting the entire model, including hyperparameters, via evidence optimization, and apply these to simulated data, and to responses of ferret auditory midbrain and cortical neurons to complex sounds. we show that integrating out the latent modulators yields better (or more readily-interpretable) receptive field estimates than a standard poisson model. conversely, integrating out the stimulus dependence yields estimates of the slowly-varying latent modulators."
"we consider a simple one-dimensional time-dependent model for bone regeneration in the presence of a bio-resorbable polymer scaffold. within the framework of the model, we optimize the effective mechanical stiffness of the polymer scaffold together with the regenerated bone matrix. the result of the optimization procedure is a scaffold porosity distribution which maximizes the stiffness of the scaffold-bone system over the regeneration time, such that the propensity for mechanical failure is reduced."
"lymphedema, a disfiguring condition characterized by the asymmetrical swelling of the limbs, is suspected to be caused by dysfunctions in the lymphatic system. lymphangions, the spontaneously contracting units of the lymphatic system, are sensitive to luminal wall shear stress. in this study, the response of the lymphangions to dynamically varying wall shear stress is characterized in isolated rat thoracic ducts in relation to their shear sensitivity. the critical shear stress above which the thoracic duct shows substantial inhibition of contraction was found to be significantly negatively correlated to the diameter of the lymphangion. the entrainment of the lymphangion to an applied oscillatory shear stress was found to be significantly dependent on the difference between the applied frequency and intrinsic frequency of contraction of the lymphangion. the strength of entrainment was also positively correlated to the applied shear stress when this shear was below the critical shear stress. the results suggest an adaptation of the lymphangion contractility to the existing oscillatory mechanical shear stress as a function of its intrinsic contractility and shear sensitivity. these adaptations might be crucial to ensure synchronized contraction of adjacent lymphangions through mechanosensitive means and might help explain the lymphatic dysfunctions that result from impaired mechanosensitivity."
"diverse classes of proteins function through large-scale conformational changes; sophisticated enhanced sampling methods have been proposed to generate these macromolecular transition paths. as such paths are curves in a high-dimensional space, they have been difficult to compare quantitatively, a prerequisite to, for instance, assess the quality of different sampling algorithms. the path similarity analysis (psa) approach alleviates these difficulties by utilizing the full information in 3n-dimensional trajectories in configuration space. psa employs the hausdorff or fr\'echet path metrics---adopted from computational geometry---enabling us to quantify path (dis)similarity, while the new concept of a hausdorff-pair map permits the extraction of atomic-scale determinants responsible for path differences. combined with clustering techniques, psa facilitates the comparison of many paths, including collections of transition ensembles. we use the closed-to-open transition of the enzyme adenylate kinase (adk)---a commonly used testbed for the assessment enhanced sampling algorithms---to examine multiple microsecond equilibrium molecular dynamics (md) transitions of adk in its substrate-free form alongside transition ensembles from the md-based dynamic importance sampling (dims-md) and targeted md (tmd) methods, and a geometrical targeting algorithm (froda). a hausdorff pairs analysis of these ensembles revealed, for instance, that differences in dims-md and froda paths were mediated by a set of conserved salt bridges whose charge-charge interactions are fully modeled in dims-md but not in froda. we also demonstrate how existing trajectory analysis methods relying on pre-defined collective variables, such as native contacts or geometric quantities, can be used synergistically with psa, as well as the application of psa to more complex systems such as membrane transporter proteins."
the electromagnetic form factors of the exotic baryons are calculated in the framework of the relativistic quark model at small and intermediate momentum transfer. the charge radii of the e+++ baryons are determined.
"this paper is concerned with regular flows of incompressible weakly viscoelastic fluids which obey a differential constitutive law of oldroyd type.   we study the newtonian limit for weakly viscoelastic fluid flows in $\r^n$ or $\t^n$ for $n=2, 3$, when the weissenberg number (relaxation time measuring the elasticity effect in the fluid) tends to zero. more precisely, we prove that the velocity field and the extra-stress tensor converge in their existence spaces (we examine the sobolev-$h^s$ theory and the besov-$b^{s,1}_2$ theory to reach the critical case $s= n/2$) to the corresponding newtonian quantities.   these convergence results are established in the case of ""ill-prepared""' data.we deduce, in the two-dimensional case, a new result concerning the global existence of weakly viscoelastic fluids flow. our approach makes use of essentially two ingredients : the stability of the null solution of the viscoelastic fluids flow and the damping effect,on the difference between the extra-stress tensor and the tensor of rate of deformation, induced by the constitutive law of the fluid."
in this paper we prove an existence and uniqueness result for a monge-amp\`{e}re type equation in the cegrell class $\mathcal{e}_{\chi}$.
"in this article we use the mean-variance model in order to measure the current market state. in our study we take the approach of detecting the overall alignment of portfolios in the spin picture. the projection to the ground-states enables us to use physical observables in order to describe the current state of the explored market. the defined magnetization of portfolios shows cursor effects, which we use to detect turmoils."
"when speaking in presence of background noise, humans reflexively change their way of speaking in order to improve the intelligibility of their speech. this reflex is known as lombard effect. collecting speech in lombard conditions is usually hard and costly. for this reason, speech enhancement systems are generally trained and evaluated on speech recorded in quiet to which noise is artificially added. since these systems are often used in situations where lombard speech occurs, in this work we perform an analysis of the impact that lombard effect has on audio, visual and audio-visual speech enhancement, focusing on deep-learning-based systems, since they represent the current state of the art in the field.   we conduct several experiments using an audio-visual lombard speech corpus consisting of utterances spoken by 54 different talkers. the results show that training deep-learning-based models with lombard speech is beneficial in terms of both estimated speech quality and estimated speech intelligibility at low signal to noise ratios, where the visual modality can play an important role in acoustically challenging situations. we also find that a performance difference between genders exists due to the distinct lombard speech exhibited by males and females, and we analyse it in relation with acoustic and visual features. furthermore, listening tests conducted with audio-visual stimuli show that the speech quality of the signals processed with systems trained using lombard speech is statistically significantly better than the one obtained using systems trained with non-lombard speech at a signal to noise ratio of -5 db. regarding speech intelligibility, we find a general tendency of the benefit in training the systems with lombard speech."
"the advent of accessible ancient dna technology now allows the direct ascertainment of allele frequencies in ancestral populations, thereby enabling the use of allele frequency time series to detect and estimate natural selection. such direct observations of allele frequency dynamics are expected to be more powerful than inferences made using patterns of linked neutral variation obtained from modern individuals. we develop a bayesian method to make use of allele frequency time series data and infer the parameters of general diploid selection, along with allele age, in non-equilibrium populations. we introduce a novel path augmentation approach, in which we use markov chain monte carlo to integrate over the space of allele frequency trajectories consistent with the observed data. using simulations, we show that this approach has good power to estimate selection coefficients and allele age. moreover, when applying our approach to data on horse coat color, we find that ignoring a relevant demographic history can significantly bias the results of inference. our approach is made available in a c++ software package."
"in this article, we formulate a planar limited version of the b-side in homological mirror symmetry that formularizes chern-simons-type topological open string field theory using homotopy associative algebra ($a_{\infty}$ algebra). this formulation is based on the works by dijkgraaf and vafa. we show that our formularization includes gravity/gauge theory correspondence which originates in the ads/cft duality of dijkgraaf-vafa theory."
"in this paper we examine the implications of the statistical large sample theory for the computational complexity of bayesian and quasi-bayesian estimation carried out using metropolis random walks. our analysis is motivated by the laplace-bernstein-von mises central limit theorem, which states that in large samples the posterior or quasi-posterior approaches a normal density. using the conditions required for the central limit theorem to hold, we establish polynomial bounds on the computational complexity of general metropolis random walks methods in large samples. our analysis covers cases where the underlying log-likelihood or extremum criterion function is possibly non-concave, discontinuous, and with increasing parameter dimension. however, the central limit theorem restricts the deviations from continuity and log-concavity of the log-likelihood or extremum criterion function in a very specific manner.   under minimal assumptions required for the central limit theorem to hold under the increasing parameter dimension, we show that the metropolis algorithm is theoretically efficient even for the canonical gaussian walk which is studied in detail. specifically, we show that the running time of the algorithm in large samples is bounded in probability by a polynomial in the parameter dimension $d$, and, in particular, is of stochastic order $d^2$ in the leading cases after the burn-in period. we then give applications to exponential families, curved exponential families, and z-estimation of increasing dimension."
"many practical applications such as gene expression analysis, multi-task learning, image recognition, signal processing, and medical data analysis pursue a sparse solution for the feature selection purpose and particularly favor the nonzeros \emph{evenly} distributed in different groups. the exclusive sparsity norm has been widely used to serve to this purpose. however, it still lacks systematical studies for exclusive sparsity norm optimization. this paper offers two main contributions from the optimization perspective: 1) we provide several efficient algorithms to solve exclusive sparsity norm minimization with either smooth loss or hinge loss (non-smooth loss). all algorithms achieve the optimal convergence rate $o(1/k^2)$ ($k$ is the iteration number). to the best of our knowledge, this is the first time to guarantee such convergence rate for the general exclusive sparsity norm minimization; 2) when the group information is unavailable to define the exclusive sparsity norm, we propose to use the random grouping scheme to construct groups and prove that if the number of groups is appropriately chosen, the nonzeros (true features) would be grouped in the ideal way with high probability. empirical studies validate the efficiency of proposed algorithms, and the effectiveness of random grouping scheme on the proposed exclusive svm formulation."
"large scale optimization problems are ubiquitous in machine learning and data analysis and there is a plethora of algorithms for solving such problems. many of these algorithms employ sub-sampling, as a way to either speed up the computations and/or to implicitly implement a form of statistical regularization. in this paper, we consider second-order iterative optimization algorithms and we provide bounds on the convergence of the variants of newton's method that incorporate uniform sub-sampling as a means to estimate the gradient and/or hessian. our bounds are non-asymptotic and quantitative. our algorithms are global and are guaranteed to converge from any initial iterate.   using random matrix concentration inequalities, one can sub-sample the hessian to preserve the curvature information. our first algorithm incorporates hessian sub-sampling while using the full gradient. we also give additional convergence results for when the sub-sampled hessian is regularized by modifying its spectrum or ridge-type regularization. next, in addition to hessian sub-sampling, we also consider sub-sampling the gradient as a way to further reduce the computational complexity per iteration. we use approximate matrix multiplication results from randomized numerical linear algebra to obtain the proper sampling strategy. in all these algorithms, computing the update boils down to solving a large scale linear system, which can be computationally expensive. as a remedy, for all of our algorithms, we also give global convergence results for the case of inexact updates where such linear system is solved only approximately.   this paper has a more advanced companion paper, [42], in which we demonstrate that, by doing a finer-grained analysis, we can get problem-independent bounds for local convergence of these algorithms and explore trade-offs to improve upon the basic results of the present paper."
"we show that the mobius function mu(n) is strongly asymptotically orthogonal to any polynomial nilsequence n -> f(g(n)l). here, g is a simply-connected nilpotent lie group with a discrete and cocompact subgroup l (so g/l is a nilmanifold), g : z -> g is a polynomial sequence and f: g/l -> r is a lipschitz function. more precisely, we show that the inner product of mu(n) with f(g(n)l) over {1,...,n} is bounded by 1/log^a n, for all a > 0. in particular, this implies the mobius and nilsequence conjecture mn(s) from our earlier paper ""linear equations in primes"" for every positive integer s. this is one of two major ingredients in our programme, outlined in that paper, to establish a large number of cases of the generalised hardy-littlewood conjecture, which predicts how often a collection \psi_1,...,\psi_t : z^d -> z of linear forms all take prime values. the proof is a relatively quick application of the results in our recent companion paper on the distribution of polynomial orbits on nilmanifolds.   we give some applications of our main theorem. we show, for example, that the mobius function is uncorrelated with any bracket polynomial. we also obtain a result about the distribution of nilsequences n -> a^nxl as n ranges only over the primes."
"we study the adaptive minimax estimation of non-linear integral functionals of a density and extend the results obtained for linear and quadratic functionals to general functionals. the typical rate optimal non-adaptive minimax estimators of ""smooth"" non-linear functionals are higher order u-statistics. since lepski's method requires tight control of tails of such estimators, we bypass such calculations by a modification of lepski's method which is applicable in such situations. as a necessary ingredient, we also provide a method to control higher order moments of minimax estimator of cubic integral functionals. following a standard constrained risk inequality method, we also show the optimality of our adaptation rates."
"motivated by several recent experimental observations that vitamin-d could interact with antigen presenting cells (apcs) and t-lymphocyte cells (t-cells) to promote and to regulate different stages of immune response, we developed a coarse grained kinetic model in an attempt to quantify the role of vitamin-d in immunomodulatory responses. our kinetic model, developed using the ideas of chemical network theory, leads to a system of nine coupled equations that we solve both by direct and by stochastic (gillespie) methods. both the analyses consistently provide detail information on the dependence of immune response to the variation of critical rate parameters. we find that although vitamin-d plays a negligible role in the initial immune response, it exerts a profound influence in the long term, especially in helping the system to achieve a new, stable steady state. the study explores the role of vitamin-d in preserving an observed bistability in the phase diagram (spanned by system parameters) of immune regulation, thus allowing the response to tolerate a wide range of pathogenic stimulation which could help in resisting autoimmune diseases. we also study how vitamin-d affects the time dependent population of dendritic cells that connect between innate and adaptive immune responses. variations in dose dependent response in anti-inflammatory and pro-inflammatory t-cell populations to vitamin-d correlate well with recent experimental results. our kinetic model allows for an estimation of the range of optimum level of vitamin-d required for smooth functioning of the immune system and for control of both hyper-regulation and inflammation. most importantly, the present study reveals that an overdose or toxic level of vitamin-d or any steroid analogue could give rise to too large a tolerant response, leading to an inefficacy in adaptive immune function."
"learning optimal dictionaries for sparse coding has exposed characteristic sparse features of many natural signals. however, universal guarantees of the stability of such features in the presence of noise are lacking. here, we provide very general conditions guaranteeing when dictionaries yielding the sparsest encodings are unique and stable with respect to measurement or modeling error. we demonstrate that some or all original dictionary elements are recoverable from noisy data even if the dictionary fails to satisfy the spark condition, its size is overestimated, or only a polynomial number of distinct sparse supports appear in the data. importantly, we derive these guarantees without requiring any constraints on the recovered dictionary beyond a natural upper bound on its size. our results also yield an effective procedure sufficient to affirm if a proposed solution to the dictionary learning problem is unique within bounds commensurate with the noise. we suggest applications to data analysis, engineering, and neuroscience and close with some remaining challenges left open by our work."
we study the interplays between paracontact geometry and the theory of bi-legendrian manifolds. we interpret the bi-legendrian connection of a bi-legendrian manifold m as the paracontact connection of a canonical paracontact structure induced on m and then we discuss many consequences of this result both for bi-legendrian and for paracontact manifolds. finally new classes of examples of paracontact manifolds are presented.
"suppose each site independently and randomly chooses some sites around it, and it is weakly (strongly) connected with them (if there choose each other). what is the probability that the weak (strong) connected cluster is infinite? we investigate a percolation model for this problem, which is a generalization of site percolation. we give a relation between the probability of the number of chosen sites around a site and the size of clusters. we also see the expected number of infinite clusters, and the exponential tail decay of the radius and the size of a cluster."
"two-inertia systems are prone to resonance vibrations that degrade their control performances. these unwanted vibrations can be effectively suppressed by control methods based on a disturbance observer (dob). vibration suppression control methods using the information of both the motor and load sides have been widely researched in recent years. methods that exploit the spring deflection or torsional force of two-inertia systems have delivered promising performances. however, few conventional methods have exploited the relative position information, and the discussion of position control is currently insufficient. focusing on the relative position, this study proposes a new resonance ratio control (rrc) based on the relative acceleration and state feedback. the structure of the proposed rrc is derived theoretically and the proposed method is experimentally validated."
"in 'a closed-form solution for options with stochastic volatility with applications to bond and currency options', heston proposes a stochastic volatility (sv) model with constant interest rate and derives a semi-explicit valuation formula. heston also describes, in general terms, how the model could be extended to incorporate stochastic interest rates (sir). this paper is devoted to the construction of an extension of heston's sv model with a particular stochastic bond model which, just increasing in one the number of parameters, allows to incorporate sir and to derive a semi-explicit formula for option pricing."
"we report unexpected evidence of critical fluctuations of the electric potential of the heart during atrial fibrillation in humans. scale invariance and long range correlations are found, which we show cannot be accounted for solely with the property of excitability, since disorder emerges by the formation of chaotic patterns in excitable media. to shed light on the data, we discuss the hypothesis that, in fact, fibrillation appears through a phase transition, which we compare on phenomenological grounds to a quenched-in disorder magnetic transition. we infer that, during propagation of pulses, random pinning might occur due to random modulation of the gap junction channels."
"the kernel least mean squares (klms) algorithm is a computationally efficient nonlinear adaptive filtering method that ""kernelizes"" the celebrated (linear) least mean squares algorithm. we demonstrate that the least mean squares algorithm is closely related to the kalman filtering, and thus, the klms can be interpreted as an approximate bayesian filtering method. this allows us to systematically develop extensions of the klms by modifying the underlying state-space and observation models. the resulting extensions introduce many desirable properties such as ""forgetting"", and the ability to learn from discrete data, while retaining the computational simplicity and time complexity of the original algorithm."
"in clinical practice, physicians make a series of treatment decisions over the course of a patient's disease based on his/her baseline and evolving characteristics. a dynamic treatment regime is a set of sequential decision rules that operationalizes this process. each rule corresponds to a decision point and dictates the next treatment action based on the accrued information. using existing data, a key goal is estimating the optimal regime, that, if followed by the patient population, would yield the most favorable outcome on average. q- and a-learning are two main approaches for this purpose. we provide a detailed account of these methods, study their performance, and illustrate them using data from a depression study."
"a new theorem on conditions for convergence to consensus of a multiagent time-dependent time-discrete dynamical system is presented. the theorem is build up on the notion of averaging maps. we compare this theorem to results by moreau (ieee transactions on automatic control, vol. 50, no. 2, 2005) about set-valued lyapunov theory and convergence under switching communication topologies. we give examples that point out differences of approaches including examples where moreau's theorem is not applicable but ours is. further on, we give examples that demonstrate that the theory of convergence to consensus is still not complete."
"currently we routinely develop a complex neuronal network to explain observed but often paradoxical phenomena based upon biological recordings. here we present a general approach to demonstrate how to mathematically tackle such a complex neuronal network so that we can fully understand the underlying mechanism. using an oxytocin network developed earlier as an example, we show how we can reduce a complex model with many variables to a tractable model with two variables, while retaining all key qualitative features of the model. the approach enables us to uncover how emergent synchronous bursting could arise from a neuronal network which embodies all known biological features. surprisingly, the discovered mechanisms for bursting are similar to those found in other systems reported in the literature, and illustrate a generic way to exhibit emergent and multi-time scale spikes: at the membrane potential level and the firing rate level."
"we investigate the cooper-pair propagation and the proximity effect in graphene under conditions in which the distance l between superconducting electrodes is much larger than the width w of the contacts. in the case of undoped graphene, supercurrents may exist with a spatial decay proportional to w^2/l^3. this changes upon doping into a 1/l^2 behavior, opening the possibility to observe a supercurrent over length scales above 1 micron at suitable doping levels. we also show that there is in general a crossover temperature t ~ v_f/k_b l that marks the onset of the strong decay of the supercurrent, and that corresponds to the scale below which the cooper pairs are not disrupted by thermal effects during their propagation."
"in high throughput settings we inspect a great many candidate variables (e.g., genes) searching for associations with a primary variable (e.g., a phenotype). high throughput hypothesis testing can be made difficult by the presence of systemic effects and other latent variables. it is well known that those variables alter the level of tests and induce correlations between tests. they also change the relative ordering of significance levels among hypotheses. poor rankings lead to wasteful and ineffective follow-up studies. the problem becomes acute for latent variables that are correlated with the primary variable. we propose a two-stage analysis to counter the effects of latent variables on the ranking of hypotheses. our method, called leapp, statistically isolates the latent variables from the primary one. in simulations, it gives better ordering of hypotheses than competing methods such as sva and eigenstrat. for an illustration, we turn to data from the agemap study relating gene expression to age for 16 tissues in the mouse. leapp generates rankings with greater consistency across tissues than the rankings attained by the other methods."
"we show that the decay of a soliton into vortices provides a mechanism for measuring the initial phase difference between two merging bose-einstein condensates. at very low temperatures, the mechanism is resonant, operating only when the clouds start in anti-phase. but at higher temperatures, phase fluctuations trigger vortex production over a wide range of initial relative phase, as observed in recent experiments at mit. choosing the merge time to maximize the number of vortices created makes the interferometer highly sensitive to spatially varying phase patterns and hence atomic movement."
"this paper focuses on recovering an unknown vector $\beta$ from the noisy data $y=x\beta +\sigma\xi$, where $x$ is a known $n\times p$-matrix, $\xi $ is a standard white gaussian noise, and $\sigma$ is an unknown noise level. in order to estimate $\beta$, a spectral regularization method is used, and our goal is to choose its regularization parameter with the help of the data $y$. in this paper, we deal solely with regularization methods based on the so-called ordered smoothers and provide some oracle inequalities in the case, where the noise level is unknown."
"we consider the l1-regularized markowitz model, where a l1-penalty term is added to the objective function of the classical mean-variance one to stabilize the solution process, promoting sparsity in the solution. the l1-penalty term can also be interpreted in terms of short sales, on which several financial markets have posed restrictions. the choice of the regularization parameter plays a key role to obtain optimal portfolios that meet the financial requirements. we propose an updating rule for the regularization parameter in bregman iteration to control both the sparsity and the number of short positions. we show that the modified scheme preserves the properties of the original one. numerical tests are reported, which show the effectiveness of the approach."
"a coupling between a light scalar field and neutrinos has been widely discussed as a mechanism for linking (time varying) neutrino masses and the present energy density and equation of state of dark energy. however, it has been pointed out that the viability of this scenario in the non-relativistic neutrino regime is threatened by the strong growth of hydrodynamic perturbations associated with a negative adiabatic sound speed squared. in this paper we revisit the stability issue in the framework of linear perturbation theory in a model independent way. the criterion for the stability of a model is translated into a constraint on the scalar-neutrino coupling, which depends on the ratio of the energy densities in neutrinos and cold dark matter. we illustrate our results by providing meaningful examples both for stable and unstable models."
"a large number of effects related to the phenomenon of bose-einstein condensation (bec) can be understood in terms of lowest order mean field theory, whereby the entire system is assumed to be condensed, with thermal and quantum fluctuations completely ignored. such a treatment leads to the gross-pitaevskii equation (gpe) used extensively throughout this book. although this theory works remarkably well for a broad range of experimental parameters, a more complete treatment is required for understanding various experiments, including experiments with solitons and vortices. such treatments should include the dynamical coupling of the condensate to the thermal cloud, the effect of dimensionality, the role of quantum fluctuations, and should also describe the critical regime, including the process of condensate formation. the aim of this chapter is to give a brief but insightful overview of various recent theories, which extend beyond the gpe. to keep the discussion brief, only the main notions and conclusions will be presented. this chapter generalizes the presentation of chapter 1, by explicitly maintaining fluctuations around the condensate order parameter. while the theoretical arguments outlined here are generic, the emphasis is on approaches suitable for describing single weakly-interacting atomic bose gases in harmonic traps. interesting effects arising when condensates are trapped in double-well potentials and optical lattices, as well as the cases of spinor condensates, and atomic-molecular coupling, along with the modified or alternative theories needed to describe them, will not be covered here."
"the rapid development of wireless sensor networks (wsns) is the significant incentive to contribute in the vulnerable applications such as cognitive radio (cr). this paper proposes a stackelberg game approach to enhance the wsn-based cr security against the spectrum sensing data falsification (ssdf) attack and conserve the consequent lost power consumption. the attack aims to corrupt the spectrum decision by imposing interference power to the delivered reports from the sensor nodes (sns) to the fusion center (fc) to make a protection level below a specific threshold. the proposed model utilizes the intelligent stackelberg game features along with the matched filter (mf) to maximize the number of protected reports sent by the sns to the fc leading to accurate decision of the spectrum status. furthermore, the tdma protocol is utilized to resolve the complexity of employing mf for the spectrum detection to avoid the collision between the delivered reports. the proposed model aims to enhance the number of correctly received reports at the fc, and hence manage the lost energy of reports retransmission due to the malicious attack effect. moreover, the model can conserve the lost power of the failure communication attempts due to the ssdf attack impact. simulation results indicate the improved performance of the proposed protection model along with the mf over the six different environments against the ssdf attack as compared to two defense schemes, namely, random and equal weight defense strategies."
"consider a linear regression model. fan and li (2001) describe the smoothly clipped absolute deviation (scad) point estimator of the regression parameter vector. to gain insight into the properties of this estimator, they consider an orthonormal design matrix and focus on the estimation of a specified component of this vector. they show that the scad point estimator has three attractive properties. we answer the question: to what extent can an interval estimator, centred on the scad estimator, have similar attractive properties?"
"in this letter we revisit the problem of optimal design of quantum tomographic experiments. in contrast to previous approaches where an optimal set of measurements is decided in advance of the experiment, we allow for measurements to be adaptively and efficiently re-optimised depending on data collected so far. we develop an adaptive statistical framework based on bayesian inference and shannon's information, and demonstrate a ten-fold reduction in the total number of measurements required as compared to non-adaptive methods, including mutually unbiased bases."
"we analyze empirical data from the internet auction site aukro.cz. the time series of activity shows truncated fractal structure on scales from about 1 minute to about 1 day. the distribution of waiting times as well as the distribution of number of auctions within fixed interval is a power law, with exponents $1.5$ and $3$, respectively. possible implications for the modeling of stock-market fluctuations are briefly discussed."
"lower urinary tract symptoms (luts) can indicate the presence of urinary tract infection (uti), a condition that if it becomes chronic requires expensive and time consuming care as well as leading to reduced quality of life. detecting the presence and gravity of an infection from the earliest symptoms is then highly valuable. typically, white blood cell count (wbc) measured in a sample of urine is used to assess uti. we consider clinical data from 1341 patients at their first visit in which uti (i.e. wbc$\geq 1$) is diagnosed. in addition, for each patient, a clinical profile of 34 symptoms was recorded. in this paper we propose a bayesian nonparametric regression model based on the dirichlet process (dp) prior aimed at providing the clinicians with a meaningful clustering of the patients based on both the wbc (response variable) and possible patterns within the symptoms profiles (covariates). this is achieved by assuming a probability model for the symptoms as well as for the response variable. to identify the symptoms most associated to uti, we specify a spike and slab base measure for the regression coefficients: this induces dependence of symptoms selection on cluster assignment. posterior inference is performed through markov chain monte carlo methods."
"the coherent hadron production analogous to cherenkov radiation of photons gives rise to the ring-like events. being projected on the ring diameter they produce the two-bump structure recently observed for the away-side jets at rhic. the position of the peaks and their height determine such properties of the hadronic medium as its nuclear refractive index, the parton density, the free path length and the energy loss of cherenkov gluons. cherenkov gluons may be responsible for the asymmetry of dilepton mass spectra near rho-meson observed in experiment. beside comparatively low energy gluons observed at rhic, there could be high energy gluons at lhc, related to the high energy region of positive real part of the forward scattering amplitude and possessing different characteristics. this would allow to scan (x, q^2)-plane determining the parton densities in its various regions."
"we study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner. in this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. as a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure."
"the hidden markov model (hmm) is a generative model that treats sequential data under the assumption that each observation is conditioned on the state of a discrete hidden variable that evolves in time as a markov chain. in this paper, we derive a novel algorithm to cluster hmms through their probability distributions. we propose a hierarchical em algorithm that i) clusters a given collection of hmms into groups of hmms that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ""cluster center"", i.e., a novel hmm that is representative for the group. we present several empirical studies that illustrate the benefits of the proposed algorithm."
"we study the robustness of active learning (al) algorithms against prior misspecification: whether an algorithm achieves similar performance using a perturbed prior as compared to using the true prior. in both the average and worst cases of the maximum coverage setting, we prove that all $\alpha$-approximate algorithms are robust (i.e., near $\alpha$-approximate) if the utility is lipschitz continuous in the prior. we further show that robustness may not be achieved if the utility is non-lipschitz. this suggests we should use a lipschitz utility for al if robustness is required. for the minimum cost setting, we can also obtain a robustness result for approximate al algorithms. our results imply that many commonly used al algorithms are robust against perturbed priors. we then propose the use of a mixture prior to alleviate the problem of prior misspecification. we analyze the robustness of the uniform mixture prior and show experimentally that it performs reasonably well in practice."
"we propose a new procedure for the risk measurement of large portfolios. it employs the following objects as the building blocks: - coherent risk measures introduced by artzner, delbaen, eber, and heath; - factor risk measures introduced in this paper, which assess the risks driven by particular factors like the price of oil, s&p500 index, or the credit spread; - risk contributions and factor risk contributions, which provide a coherent alternative to the sensitivity coefficients.   we also propose two particular classes of coherent risk measures called alpha v@r and beta v@r, for which all the objects described above admit an extremely simple empirical estimation procedure. this procedure uses no model assumptions on the structure of the price evolution.   moreover, we consider the problem of the risk management on a firm's level. it is shown that if the risk limits are imposed on the risk contributions of the desks to the overall risk of the firm (rather than on their outstanding risks) and the desks are allowed to trade these limits within a firm, then the desks automatically find the globally optimal portfolio."
we construct local normal forms of pseudo-riemannian projectively equivalent 2-dimensional metrics.
"bayesian models offer great flexibility for clustering applications---bayesian nonparametrics can be used for modeling infinite mixtures, and hierarchical bayesian models can be utilized for sharing clusters across multiple data sets. for the most part, such flexibility is lacking in classical clustering methods such as k-means. in this paper, we revisit the k-means clustering algorithm from a bayesian nonparametric viewpoint. inspired by the asymptotic connection between k-means and mixtures of gaussians, we show that a gibbs sampling algorithm for the dirichlet process mixture approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like clustering objective that includes a penalty for the number of clusters. we generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical dirichlet process. we also discuss further extensions that highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph."
"markov chain monte carlo (mcmc) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. this challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. in this work, we extend the recently introduced bidirectional monte carlo technique to evaluate mcmc-based posterior inference algorithms. by running annealed importance sampling (ais) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized kl divergence between the true posterior distribution and the distribution of approximate samples. we present bounding divergences with reverse annealing (bread), a protocol for validating the relevance of simulated data experiments to real datasets, and integrate it into two probabilistic programming languages: webppl and stan. as an example of how bread can be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in both webppl and stan."
"fragmentation functions are determined for the pion, kaon, and proton by analyzing charged-hadron production data in electron-positron annihilation. it is important that uncertainties of the determined fragmentation functions are estimated in this analysis. analysis results indicate that gluon and light-quark functions have large uncertainties especially at small q^2. we find that next-to-leading-order (nlo) uncertainties are significantly reduced in comparison with leading-order (lo) ones in the pion and kaon. the fragmentation functions are very different in various analysis groups. however, all the recent functions are roughly within the estimated uncertainties, which indicates that they are consistent with each other. we provide a code for calculating the fragmentation functions and their uncertainties at a given kinematical point of z and q^2 by a user."
"the minos long baseline experiment has been collecting neutrino beam data since march 2005 and has accumulated 3 x 10^{20} protons-on-target (pot) to date. minos uses fermilab's numi neutrino beam which is measured by two steel-scintillator tracking calorimeters, one at fermilab and the other 735 km downstream, in northern minnesota. by observing the oscillatory structure in the neutrino energy spectrum, minos can precisely measure the neutrino oscillation parameters in the atmospheric sector. from analysis of the first year of data, corresponding to 1.27 x 10^{20} pot, these parameters were determined to be |\delta m^2_{32}|=2.74^{+0.44}_{-0.26} x 10^{-3} ev^2 and sin^2(2\theta_{23})>0.87 (68% c.l.). minos is able to measure the neutrino velocity by comparing the arrival times of the neutrino beam in its two detectors. using a total of 473 far detector events, (v-c)/c = (5.1 +/- 2.9) x 10^{-5} (68% c.l.) was measured. in addition, we report recent progress in the analysis of neutral current events and give an outline of experimental goals for the future."
"subsystem codes are the most versatile class of quantum error-correcting codes known to date that combine the best features of all known passive and active error-control schemes. the subsystem code is a subspace of the quantum state space that is decomposed into a tensor product of two vector spaces: the subsystem and the co-subsystem. a generic method to derive subsystem codes from existing subsystem codes is given that allows one to trade the dimensions of subsystem and co-subsystem while maintaining or improving the minimum distance. as a consequence, it is shown that all pure mds subsystem codes are derived from mds stabilizer codes. the existence of numerous families of mds subsystem codes is established. propagation rules are derived that allow one to obtain longer and shorter subsystem codes from given subsystem codes. furthermore, propagation rules are derived that allow one to construct a new subsystem code by combining two given subsystem codes."
"we study dirac-harmonic maps from degenerating spin surfaces with uniformly bounded energy and show the so-called generalized energy identity in the case that the domain converges to a spin surface with only neveu-schwarz type nodes. we find condition that is both necessary and sufficient for the $w^{1,2} \times l^{4}$ modulo bubbles compactness of a sequence of such maps."
"in this note we study kloosterman sums twisted by a multiplicative characters modulo a prime power. we show, by an elementary calculation, that these sums become equidistributed on the real line with respect to a suitable measure."
"we obtain the rate of growth of long strange segments and the rate of decay of infinite horizon ruin probabilities for a class of infinite moving average processes with exponentially light tails. the rates are computed explicitly. we show that the rates are very similar to those of an i.i.d. process as long as the moving average coefficients decay fast enough. if they do not, then the rates are significantly different. this demonstrates the change in the length of memory in a moving average process associated with certain changes in the rate of decay of the coefficients."
"the time evolution of a spin-1/2 particle under the influence of a locally applied external magnetic field, and interacting with anisotropic spin environment in thermal equilibrium at temperature $t$ is studied. the exact analytical form of the reduced density matrix of the central spin is calculated explicitly for finite number of bath spins. the case of an infinite number of environmental spins is investigated using the convergence of the rescaled bath operators to normal gaussian random variables. in this limit, we derive the analytical form of the components of the bloch vector for antiferromagnetic interactions within the bath, and we investigate the short-time and long-time behavior of reduced dynamics. the effect of the external magnetic field, the anisotropy and the temperature of the bath on the decoherence of the central spin is discussed."
